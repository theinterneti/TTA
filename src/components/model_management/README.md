# Model Management Component

A comprehensive AI model management system for the TTA (Therapeutic Text Adventure) platform, providing unified access to multiple AI model providers with intelligent selection, performance monitoring, and fallback mechanisms.

## Features

### 🌐 Multi-Provider Support
- **OpenRouter**: Cloud-based models with free tier filtering
- **Ollama**: Containerized local model deployment
- **Local Models**: Direct Hugging Face Transformers integration
- **LM Studio**: Integration with LM Studio local server
- **Custom APIs**: Support for OpenAI, Anthropic, and other compatible APIs

### 🧠 Intelligent Model Selection
- Task-specific model recommendations
- Hardware-aware model selection
- Cost optimization with free model preference
- Therapeutic safety scoring
- Performance-based ranking

### 📊 Performance Monitoring
- Real-time performance metrics
- Response time tracking
- Token usage statistics
- Quality score monitoring
- Resource utilization tracking

### 🔄 Fallback Mechanisms
- Automatic failover on model errors
- Provider health monitoring
- Retry logic with exponential backoff
- Cross-provider fallback support

### 🛡️ Therapeutic Safety
- Content safety scoring
- Crisis detection capabilities
- Therapeutic appropriateness validation
- Safe model recommendations

## Quick Start

### Installation

The model management system is included in the main TTA installation:

```bash
# Install TTA with all dependencies
pip install -e .

# Or install with minimal dependencies
pip install -e .[minimal]

# For GPU support
pip install -e .[gpu]
```

### Basic Usage

```python
from src.components.model_management import ModelManagementComponent, TaskType

# Initialize component
config = {
    "model_management": {
        "enabled": True,
        "default_provider": "openrouter",
        "providers": {
            "openrouter": {
                "enabled": True,
                "api_key": "your-openrouter-key",
                "free_models_only": True
            }
        }
    }
}

component = ModelManagementComponent(config)

# Start the system
await component.start()

# Generate text with automatic model selection
response = await component.generate_text(
    "Tell me a therapeutic story about overcoming anxiety",
    task_type=TaskType.THERAPEUTIC_NARRATIVE
)

print(f"Generated by {response.model_id}: {response.text}")

# Stop the system
await component.stop()
```

## Configuration

### Environment Variables

```bash
# Required for OpenRouter
export OPENROUTER_API_KEY="your-openrouter-api-key"

# Optional for custom APIs
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
```

### Configuration File

Add to your `tta_config.yaml`:

```yaml
model_management:
  enabled: true
  default_provider: "openrouter"
  
  providers:
    openrouter:
      enabled: true
      api_key: "${OPENROUTER_API_KEY}"
      free_models_only: true
      preferred_models:
        - "meta-llama/llama-3.1-8b-instruct:free"
        - "microsoft/phi-3-mini-128k-instruct:free"
    
    ollama:
      enabled: true
      base_url: "http://localhost:11434"
      docker_enabled: true
      auto_pull_models: true
    
    local:
      enabled: false  # Resource intensive
      models_cache_dir: "./models"
      max_concurrent_models: 2
  
  selection_strategy:
    algorithm: "performance_based"
    prefer_free_models: true
    therapeutic_safety_threshold: 7.0
  
  fallback_config:
    enabled: true
    max_retries: 3
    fallback_strategy: "performance_based"
```

## API Endpoints

The system provides REST API endpoints for integration:

```python
from fastapi import FastAPI
from src.components.model_management.api import router

app = FastAPI()
app.include_router(router)

# Available endpoints:
# POST /api/v1/models/generate - Generate text
# GET /api/v1/models/available - List available models
# GET /api/v1/models/recommendations - Get model recommendations
# POST /api/v1/models/test - Test model connectivity
# GET /api/v1/models/status - System status
# GET /api/v1/models/performance - Performance metrics
```

## Providers

### OpenRouter Provider

Access to 100+ cloud-based models with cost optimization:

```python
# Free models only
config = {
    "openrouter": {
        "enabled": True,
        "api_key": "your-key",
        "free_models_only": True,
        "cost_limit_per_request": 0.01
    }
}
```

### Ollama Provider

Containerized local model deployment:

```python
# Automatic Docker management
config = {
    "ollama": {
        "enabled": True,
        "base_url": "http://localhost:11434",
        "docker_enabled": True,
        "gpu_enabled": True,
        "auto_pull_models": True,
        "preferred_models": ["llama3.1:8b", "qwen2.5:7b"]
    }
}
```

### Local Provider

Direct Hugging Face integration:

```python
# Hardware-optimized local models
config = {
    "local": {
        "enabled": True,
        "models_cache_dir": "./models",
        "auto_quantization": True,
        "gpu_memory_fraction": 0.8,
        "max_concurrent_models": 2
    }
}
```

## Task Types

The system supports various task-specific optimizations:

- `GENERAL_CHAT`: General conversation
- `THERAPEUTIC_NARRATIVE`: Therapeutic storytelling
- `CREATIVE_WRITING`: Creative content generation
- `INSTRUCTION_FOLLOWING`: Task completion
- `CODE_GENERATION`: Programming assistance
- `ANALYSIS`: Text analysis and reasoning

## Performance Monitoring

### Metrics Collection

```python
# Get model performance
performance = await component.performance_monitor.get_model_performance(
    model_id="llama-3.1-8b",
    timeframe_hours=24
)

print(f"Average response time: {performance['average_response_time_ms']}ms")
print(f"Success rate: {performance['success_rate']*100}%")
print(f"Total requests: {performance['total_requests']}")
```

### System Monitoring

```python
# Get system-wide metrics
system_perf = await component.performance_monitor.get_system_performance()

print(f"Active models: {system_perf['active_models']}")
print(f"Total requests: {system_perf['total_requests']}")
print(f"Average quality score: {system_perf['average_quality_score']}")
```

## Hardware Detection

Automatic hardware detection and model recommendations:

```python
# Get hardware-appropriate model recommendations
recommendations = await component.get_model_recommendations(
    TaskType.THERAPEUTIC_NARRATIVE
)

# System automatically considers:
# - Available RAM and GPU memory
# - CPU capabilities
# - Storage space
# - Network bandwidth
```

## Fallback Handling

Automatic failover and recovery:

```python
# Configure fallback behavior
config = {
    "fallback_config": {
        "enabled": True,
        "max_retries": 3,
        "exclude_failed_models_minutes": 30,
        "fallback_strategy": "performance_based",
        "prefer_different_provider": True
    }
}

# Automatic fallback on failures:
# 1. Model fails -> try different model from same provider
# 2. Provider fails -> try different provider
# 3. All fail -> return error with details
```

## Testing

Run the comprehensive test suite:

```bash
# Run all tests
pytest tests/test_model_management.py -v

# Run specific test categories
pytest tests/test_model_management.py::TestModelManagementComponent -v
pytest tests/test_model_management.py::TestHardwareDetector -v
pytest tests/test_model_management.py::TestIntegration -v

# Run with coverage
pytest tests/test_model_management.py --cov=src.components.model_management
```

## Examples

See the `examples/model_management_demo.py` script for comprehensive usage examples:

```bash
# Run the demo (requires OPENROUTER_API_KEY)
python examples/model_management_demo.py
```

## Architecture

```
ModelManagementComponent
├── Providers/
│   ├── OpenRouterProvider    # Cloud models via OpenRouter
│   ├── OllamaProvider       # Containerized local models
│   ├── LocalModelProvider   # Direct HuggingFace integration
│   ├── LMStudioProvider     # LM Studio integration
│   └── CustomAPIProvider    # OpenAI/Anthropic/etc APIs
├── Services/
│   ├── HardwareDetector     # System resource detection
│   ├── ModelSelector        # Intelligent model selection
│   ├── PerformanceMonitor   # Metrics and monitoring
│   └── FallbackHandler      # Error handling and recovery
└── API/
    └── FastAPI endpoints     # REST API interface
```

## Contributing

1. Follow the existing code structure and patterns
2. Add comprehensive tests for new features
3. Update documentation and examples
4. Ensure therapeutic safety considerations
5. Test across different hardware configurations

## License

Part of the TTA (Therapeutic Text Adventure) platform.
