name: Performance Regression Tracking

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run performance tests daily at 4 AM UTC
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      baseline_ref:
        description: 'Git ref to use as baseline (default: main)'
        required: false
        default: 'main'
      test_suite:
        description: 'Performance test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - api
          - frontend
          - database
          - build

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'
  # Performance thresholds (percentage increase allowed)
  API_RESPONSE_TIME_THRESHOLD: 10
  BUILD_TIME_THRESHOLD: 15
  TEST_EXECUTION_THRESHOLD: 20
  MEMORY_USAGE_THRESHOLD: 25

jobs:
  # Collect baseline performance metrics
  baseline-metrics:
    name: Collect Baseline Metrics
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    outputs:
      baseline_data: ${{ steps.collect.outputs.data }}
    steps:
      - name: Checkout baseline code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          version: "0.8.17"

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-baseline-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-baseline-
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: uv sync --all-groups

      - name: Collect baseline performance metrics
        id: collect
        run: |
          echo "Collecting baseline performance metrics..."

          # Create metrics directory
          mkdir -p performance-metrics/baseline

          # Measure test execution time
          start_time=$(date +%s)
          uv run pytest tests/ -q --tb=no --maxfail=1 || true
          end_time=$(date +%s)
          test_duration=$((end_time - start_time))

          # Measure build time
          start_time=$(date +%s)
          uv sync --all-groups > /dev/null 2>&1
          end_time=$(date +%s)
          build_duration=$((end_time - start_time))

          # Create baseline metrics JSON
          cat > performance-metrics/baseline/metrics.json <<EOF
          {
            "test_execution_time": $test_duration,
            "build_time": $build_duration,
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.event.pull_request.base.sha }}"
          }
          EOF

          # Output for next job
          echo "data=$(cat performance-metrics/baseline/metrics.json | jq -c .)" >> $GITHUB_OUTPUT

      - name: Upload baseline metrics
        uses: actions/upload-artifact@v4
        with:
          name: baseline-metrics
          path: performance-metrics/baseline/
          retention-days: 7

  # Collect current performance metrics
  current-metrics:
    name: Collect Current Metrics
    runs-on: ubuntu-latest
    outputs:
      current_data: ${{ steps.collect.outputs.data }}
    steps:
      - name: Checkout current code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          version: "0.8.17"

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-current-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-current-
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: uv sync --all-groups

      - name: Collect current performance metrics
        id: collect
        run: |
          echo "Collecting current performance metrics..."

          # Create metrics directory
          mkdir -p performance-metrics/current

          # Measure test execution time
          start_time=$(date +%s)
          uv run pytest tests/ -q --tb=no --maxfail=1 || true
          end_time=$(date +%s)
          test_duration=$((end_time - start_time))

          # Measure build time
          start_time=$(date +%s)
          uv sync --all-groups > /dev/null 2>&1
          end_time=$(date +%s)
          build_duration=$((end_time - start_time))

          # Create current metrics JSON
          cat > performance-metrics/current/metrics.json <<EOF
          {
            "test_execution_time": $test_duration,
            "build_time": $build_duration,
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}"
          }
          EOF

          # Output for next job
          echo "data=$(cat performance-metrics/current/metrics.json | jq -c .)" >> $GITHUB_OUTPUT

      - name: Upload current metrics
        uses: actions/upload-artifact@v4
        with:
          name: current-metrics
          path: performance-metrics/current/
          retention-days: 7

  # API performance testing
  api-performance:
    name: API Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'api' || github.event.inputs.test_suite == ''
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install uv
        uses: astral-sh/setup-uv@v1
        with:
          version: "0.8.17"

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-api-perf-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-api-perf-
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: uv sync --all-groups

      - name: Run API performance tests
        run: |
          echo "Running API performance tests..."
          mkdir -p performance-metrics/api

          # Run performance tests with timing
          uv run pytest tests/api/ -v --benchmark-only \
            --benchmark-json=performance-metrics/api/benchmark.json || true
        env:
          REDIS_URL: redis://localhost:6379

      - name: Upload API performance metrics
        uses: actions/upload-artifact@v4
        with:
          name: api-performance-metrics
          path: performance-metrics/api/
          retention-days: 30

  # Frontend performance testing
  frontend-performance:
    name: Frontend Performance Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'frontend' || github.event.inputs.test_suite == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: src/player_experience/frontend/package-lock.json

      - name: Install dependencies
        working-directory: src/player_experience/frontend
        run: npm ci

      - name: Build frontend with timing
        working-directory: src/player_experience/frontend
        run: |
          echo "Building frontend and measuring performance..."
          mkdir -p ../../../performance-metrics/frontend

          start_time=$(date +%s)
          npm run build
          end_time=$(date +%s)
          build_duration=$((end_time - start_time))

          # Get bundle size
          bundle_size=$(du -sb dist | cut -f1)

          # Create metrics JSON
          cat > ../../../performance-metrics/frontend/metrics.json <<EOF
          {
            "build_time": $build_duration,
            "bundle_size": $bundle_size,
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}"
          }
          EOF

      - name: Upload frontend performance metrics
        uses: actions/upload-artifact@v4
        with:
          name: frontend-performance-metrics
          path: performance-metrics/frontend/
          retention-days: 30

  # Compare and report performance regression
  performance-comparison:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [baseline-metrics, current-metrics]
    if: github.event_name == 'pull_request'
    steps:
      - name: Download baseline metrics
        uses: actions/download-artifact@v4
        with:
          name: baseline-metrics
          path: baseline/

      - name: Download current metrics
        uses: actions/download-artifact@v4
        with:
          name: current-metrics
          path: current/

      - name: Analyze performance regression
        id: analyze
        run: |
          echo "Analyzing performance regression..."

          # Read metrics
          baseline_test_time=$(jq -r '.test_execution_time' baseline/metrics.json)
          baseline_build_time=$(jq -r '.build_time' baseline/metrics.json)
          current_test_time=$(jq -r '.test_execution_time' current/metrics.json)
          current_build_time=$(jq -r '.build_time' current/metrics.json)

          # Calculate percentage changes
          test_change=$(awk "BEGIN {printf \"%.2f\", (($current_test_time - $baseline_test_time) / $baseline_test_time) * 100}")
          build_change=$(awk "BEGIN {printf \"%.2f\", (($current_build_time - $baseline_build_time) / $baseline_build_time) * 100}")

          # Check thresholds
          regression_found=false

          if (( $(echo "$test_change > ${{ env.TEST_EXECUTION_THRESHOLD }}" | bc -l) )); then
            echo "⚠️ Test execution time regression detected: ${test_change}%"
            regression_found=true
          fi

          if (( $(echo "$build_change > ${{ env.BUILD_TIME_THRESHOLD }}" | bc -l) )); then
            echo "⚠️ Build time regression detected: ${build_change}%"
            regression_found=true
          fi

          # Create comparison report
          cat > performance-comparison.md <<EOF
          ## 📊 Performance Regression Analysis

          ### Test Execution Time
          - **Baseline**: ${baseline_test_time}s
          - **Current**: ${current_test_time}s
          - **Change**: ${test_change}%
          - **Threshold**: ${{ env.TEST_EXECUTION_THRESHOLD }}%
          - **Status**: $(if (( $(echo "$test_change > ${{ env.TEST_EXECUTION_THRESHOLD }}" | bc -l) )); then echo "❌ REGRESSION"; else echo "✅ PASS"; fi)

          ### Build Time
          - **Baseline**: ${baseline_build_time}s
          - **Current**: ${current_build_time}s
          - **Change**: ${build_change}%
          - **Threshold**: ${{ env.BUILD_TIME_THRESHOLD }}%
          - **Status**: $(if (( $(echo "$build_change > ${{ env.BUILD_TIME_THRESHOLD }}" | bc -l) )); then echo "❌ REGRESSION"; else echo "✅ PASS"; fi)

          ### Summary
          $(if [ "$regression_found" = true ]; then
            echo "⚠️ **Performance regression detected!** Please review the changes and optimize if necessary."
          else
            echo "✅ **No significant performance regression detected.**"
          fi)

          ---
          *Performance thresholds: Test execution: ${{ env.TEST_EXECUTION_THRESHOLD }}%, Build time: ${{ env.BUILD_TIME_THRESHOLD }}%*
          EOF

          # Output for PR comment
          echo "regression_found=$regression_found" >> $GITHUB_OUTPUT
          echo "report<<EOF" >> $GITHUB_OUTPUT
          cat performance-comparison.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Comment PR with performance analysis
        uses: actions/github-script@v7
        with:
          script: |
            const report = `${{ steps.analyze.outputs.report }}`;

            // Find existing performance comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Regression Analysis')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: report
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
            }

      - name: Fail if regression detected
        if: steps.analyze.outputs.regression_found == 'true'
        run: |
          echo "❌ Performance regression detected!"
          echo "Please review the performance analysis and optimize if necessary."
          exit 1

  # Store historical performance data
  store-metrics:
    name: Store Performance Metrics
    runs-on: ubuntu-latest
    needs: [current-metrics, api-performance, frontend-performance]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all metrics
        uses: actions/download-artifact@v4
        with:
          path: all-metrics/

      - name: Store metrics in repository
        run: |
          echo "Storing performance metrics..."

          # Create performance history directory
          mkdir -p .performance-history

          # Create timestamped metrics file
          timestamp=$(date +%Y%m%d-%H%M%S)
          commit_sha="${{ github.sha }}"

          # Combine all metrics
          cat > .performance-history/${timestamp}-${commit_sha:0:7}.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "$commit_sha",
            "branch": "${{ github.ref_name }}",
            "metrics": $(find all-metrics -name "*.json" -exec cat {} \; | jq -s 'add')
          }
          EOF

          echo "✅ Performance metrics stored"

      - name: Upload performance history
        uses: actions/upload-artifact@v4
        with:
          name: performance-history
          path: .performance-history/
          retention-days: 90
