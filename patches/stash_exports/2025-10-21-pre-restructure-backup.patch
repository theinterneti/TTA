diff --git a/config/tta_config.yaml b/config/tta_config.yaml
index 1f1a43fc0..0c44219c1 100644
--- a/config/tta_config.yaml
+++ b/config/tta_config.yaml
@@ -94,7 +94,7 @@ agent_orchestration:
       auth_required: true
       connection_timeout: 60.0
       cors_origins:
-      - '*'
+        - "*"
       enabled: true
       heartbeat_interval: 30.0
       max_connections: 1000
@@ -168,7 +168,7 @@ deeply:
       key: 42
 docker:
   compose_profiles:
-  - default
+    - default
   enabled: true
   ensure_consistent_env_vars: true
   ensure_consistent_extensions: true
@@ -225,8 +225,8 @@ model_management:
       max_concurrent_models: 2
       models_cache_dir: ./models
       preferred_models:
-      - microsoft/DialoGPT-medium
-      - gpt2
+        - microsoft/DialoGPT-medium
+        - gpt2
     ollama:
       auto_pull_models: true
       base_url: http://localhost:11434
@@ -235,9 +235,9 @@ model_management:
       enabled: true
       gpu_enabled: true
       preferred_models:
-      - llama3.1:8b
-      - qwen2.5:7b
-      - phi3:mini
+        - llama3.1:8b
+        - qwen2.5:7b
+        - phi3:mini
     openrouter:
       api_key: ${OPENROUTER_API_KEY}
       base_url: https://openrouter.ai/api/v1
@@ -245,9 +245,9 @@ model_management:
       enabled: true
       free_models_only: false
       preferred_models:
-      - meta-llama/llama-3.1-8b-instruct:free
-      - microsoft/phi-3-mini-128k-instruct:free
-      - qwen/qwen-2-7b-instruct:free
+        - meta-llama/llama-3.1-8b-instruct:free
+        - microsoft/phi-3-mini-128k-instruct:free
+        - qwen/qwen-2-7b-instruct:free
   selection_strategy:
     algorithm: performance_based
     max_latency_ms: 5000
@@ -265,8 +265,8 @@ new:
 player_experience:
   api:
     cors_origins:
-    - http://localhost:3000
-    - https://localhost:3000
+      - http://localhost:3000
+      - https://localhost:3000
     host: 0.0.0.0
     port: 8080
   character_limits:
diff --git a/docs/ISSUE-048-CODE-SNIPPETS.md b/docs/ISSUE-048-CODE-SNIPPETS.md
index 43c911508..c12d6e0ed 100644
--- a/docs/ISSUE-048-CODE-SNIPPETS.md
+++ b/docs/ISSUE-048-CODE-SNIPPETS.md
@@ -224,4 +224,3 @@ For issues during implementation:
 2. Review the Analysis: `docs/ISSUE-048-SESSION-PERSISTENCE-ANALYSIS.md`
 3. Check E2E test output for specific errors
 4. Review browser console for JavaScript errors
-
diff --git a/docs/ISSUE-048-COMPLETION-SUMMARY.md b/docs/ISSUE-048-COMPLETION-SUMMARY.md
index 00a36e72a..7d8aeee89 100644
--- a/docs/ISSUE-048-COMPLETION-SUMMARY.md
+++ b/docs/ISSUE-048-COMPLETION-SUMMARY.md
@@ -1,8 +1,8 @@
 # Issue #48 Completion Summary: Frontend Session Persistence
 
-**Status**: âœ… RESOLVED & CLOSED  
-**Date Completed**: 2025-10-17  
-**Priority**: CRITICAL  
+**Status**: âœ… RESOLVED & CLOSED
+**Date Completed**: 2025-10-17
+**Priority**: CRITICAL
 **Component**: Player Experience (Backend/Frontend)
 
 ---
@@ -169,4 +169,3 @@ All issues have been fixed, tested, and deployed to the staging environment.
 Issue #48 has been successfully resolved through systematic investigation, root cause analysis, and targeted implementation. The session persistence feature is now working correctly in the staging environment, improving user experience and enabling seamless session restoration across page refreshes.
 
 The remaining failing test (logout functionality) has been tracked as a separate issue (#51) and is ready for investigation and implementation.
-
diff --git a/docs/ISSUE-048-FINAL-REPORT.md b/docs/ISSUE-048-FINAL-REPORT.md
index 6aeb972af..7ef309d3f 100644
--- a/docs/ISSUE-048-FINAL-REPORT.md
+++ b/docs/ISSUE-048-FINAL-REPORT.md
@@ -1,8 +1,8 @@
 # Issue #48 Final Report: Frontend Session Persistence
 
-**Status**: âœ… RESOLVED & CLOSED  
-**Date Completed**: 2025-10-17  
-**Total Effort**: ~12-15 hours (investigation + implementation + cleanup)  
+**Status**: âœ… RESOLVED & CLOSED
+**Date Completed**: 2025-10-17
+**Total Effort**: ~12-15 hours (investigation + implementation + cleanup)
 **Test Pass Rate**: 90% (9/10 tests passing)
 
 ---
@@ -193,4 +193,3 @@ Issue #48 has been successfully resolved. The session persistence feature is now
 The remaining failing test (logout functionality) has been tracked as Issue #51 and is ready for investigation.
 
 **Status**: âœ… COMPLETE & PRODUCTION-READY FOR STAGING
-
diff --git a/docs/ISSUE-048-IMPLEMENTATION-GUIDE.md b/docs/ISSUE-048-IMPLEMENTATION-GUIDE.md
index 5f4e78659..16c4b0dfa 100644
--- a/docs/ISSUE-048-IMPLEMENTATION-GUIDE.md
+++ b/docs/ISSUE-048-IMPLEMENTATION-GUIDE.md
@@ -18,12 +18,12 @@ Fix session persistence so users remain authenticated after page refresh.
 ```typescript
 setToken(accessToken: string, expiresIn: number = 3600): void {
   const expiresAt = Date.now() + (expiresIn * 1000);
-  
+
   this.tokenData = {
     accessToken,
     expiresAt,
   };
-  
+
   // PERSIST TO LOCALSTORAGE (NEW)
   try {
     localStorage.setItem('tta_token', JSON.stringify({
@@ -33,7 +33,7 @@ setToken(accessToken: string, expiresIn: number = 3600): void {
   } catch (error) {
     console.warn('Failed to persist token to localStorage:', error);
   }
-  
+
   this.scheduleTokenRefresh(expiresAt);
 }
 ```
@@ -78,7 +78,7 @@ async function restoreAuthentication(): Promise<boolean> {
   try {
     // RESTORE TOKEN FROM STORAGE FIRST (NEW)
     secureStorage.restoreFromStorage();
-    
+
     // Check retry limit...
     if (authRetryCount >= MAX_AUTH_RETRIES) {
       // ... existing code
@@ -104,7 +104,7 @@ async function restoreAuthentication(): Promise<boolean> {
   state.user = action.payload.user;
   state.token = action.payload.token;
   state.sessionId = action.payload.sessionId;
-  
+
   // PERSIST AUTH STATE (NEW)
   try {
     localStorage.setItem('tta_auth_state', JSON.stringify({
@@ -122,7 +122,7 @@ async function restoreAuthentication(): Promise<boolean> {
 ```typescript
 .addCase(logout.fulfilled, (state) => {
   // ... existing code
-  
+
   // CLEAR PERSISTED STATE (NEW)
   localStorage.removeItem('tta_auth_state');
 })
@@ -201,4 +201,3 @@ async function restoreAuthentication(): Promise<boolean> {
 5. Deploy to staging
 6. Validate in staging environment
 7. Deploy to production
-
diff --git a/docs/ISSUE-048-INVESTIGATION-SUMMARY.md b/docs/ISSUE-048-INVESTIGATION-SUMMARY.md
index 255c08202..e30096ec7 100644
--- a/docs/ISSUE-048-INVESTIGATION-SUMMARY.md
+++ b/docs/ISSUE-048-INVESTIGATION-SUMMARY.md
@@ -1,10 +1,10 @@
 # Issue #48: Frontend Session Persistence - Investigation Summary
 
-**GitHub Issue**: #48  
-**Title**: [BUG] Frontend Session Persistence: State Not Restored After Page Refresh  
-**Impact**: 2 failing E2E tests, users lose login state after page refresh  
-**Investigation Date**: 2025-10-16  
-**Status**: âœ… Investigation Complete - Ready for Implementation  
+**GitHub Issue**: #48
+**Title**: [BUG] Frontend Session Persistence: State Not Restored After Page Refresh
+**Impact**: 2 failing E2E tests, users lose login state after page refresh
+**Investigation Date**: 2025-10-16
+**Status**: âœ… Investigation Complete - Ready for Implementation
 
 ---
 
@@ -51,7 +51,7 @@ Steps:
   1. Login successfully âœ…
   2. Refresh page âœ…
   3. User remains authenticated âŒ (FAILS HERE)
-  
+
 Reason: Token lost from memory after refresh
 ```
 
@@ -62,7 +62,7 @@ Steps:
   1. Login and navigate âœ…
   2. Navigate between pages âœ…
   3. Session remains active âŒ (FAILS HERE)
-  
+
 Reason: Session data lost during navigation
 ```
 
@@ -166,4 +166,3 @@ Reason: Session data lost during navigation
 4. â­ï¸ Run E2E tests
 5. â­ï¸ Deploy to staging
 6. â­ï¸ Validate in staging environment
-
diff --git a/docs/ISSUE-048-SESSION-PERSISTENCE-ANALYSIS.md b/docs/ISSUE-048-SESSION-PERSISTENCE-ANALYSIS.md
index 081db3d4a..e6ef040a6 100644
--- a/docs/ISSUE-048-SESSION-PERSISTENCE-ANALYSIS.md
+++ b/docs/ISSUE-048-SESSION-PERSISTENCE-ANALYSIS.md
@@ -1,9 +1,9 @@
 # Issue #48: Frontend Session Persistence Investigation - Comprehensive Analysis
 
-**Status**: Investigation Complete  
-**Issue**: Session state not restored after page refresh (2 failing E2E tests)  
-**Severity**: High (blocks user experience)  
-**Estimated Fix Time**: 4-7 hours  
+**Status**: Investigation Complete
+**Issue**: Session state not restored after page refresh (2 failing E2E tests)
+**Severity**: High (blocks user experience)
+**Estimated Fix Time**: 4-7 hours
 
 ---
 
@@ -47,14 +47,14 @@ Session restoration (sessionRestoration.ts, line 110) tries:
 
 ### **Login Flow (Works)**
 ```
-User Login â†’ Backend returns token â†’ secureStorage.setToken() â†’ 
+User Login â†’ Backend returns token â†’ secureStorage.setToken() â†’
 sessionManager.setSession() â†’ Redux updated â†’ User authenticated âœ…
 ```
 
 ### **Page Refresh Flow (Fails)**
 ```
-Page Refresh â†’ JavaScript memory cleared â†’ Token lost âŒ â†’ 
-sessionRestoration.restoreAuthentication() â†’ No token found âŒ â†’ 
+Page Refresh â†’ JavaScript memory cleared â†’ Token lost âŒ â†’
+sessionRestoration.restoreAuthentication() â†’ No token found âŒ â†’
 Backend session check fails âŒ â†’ User redirected to login âŒ
 ```
 
@@ -130,11 +130,11 @@ Backend session check fails âŒ â†’ User redirected to login âŒ
 
 ## ðŸ“ˆ Expected Outcomes
 
-âœ… Session persists after page refresh  
-âœ… Session maintained across navigation  
-âœ… 2 failing E2E tests pass  
-âœ… User experience improved  
-âœ… No security regressions  
+âœ… Session persists after page refresh
+âœ… Session maintained across navigation
+âœ… 2 failing E2E tests pass
+âœ… User experience improved
+âœ… No security regressions
 
 ---
 
@@ -145,4 +145,3 @@ Backend session check fails âŒ â†’ User redirected to login âŒ
 3. Update session restoration logic
 4. Run E2E tests to verify fix
 5. Deploy to staging for validation
-
diff --git a/docs/STAGING-ENVIRONMENT-NEXT-STEPS.md b/docs/STAGING-ENVIRONMENT-NEXT-STEPS.md
index e4f599129..6cf099fe7 100644
--- a/docs/STAGING-ENVIRONMENT-NEXT-STEPS.md
+++ b/docs/STAGING-ENVIRONMENT-NEXT-STEPS.md
@@ -1,7 +1,7 @@
 # Staging Environment: Next Steps & Recommendations
 
-**Date**: 2025-10-17  
-**Current Status**: 9/10 E2E tests passing (90%)  
+**Date**: 2025-10-17
+**Current Status**: 9/10 E2E tests passing (90%)
 **Last Updated**: After Issue #48 resolution
 
 ---
@@ -27,9 +27,9 @@
 ### ðŸ”´ CRITICAL (Immediate - Next 1-2 days)
 
 #### 1. Issue #51: Logout Functionality
-**Status**: Created, ready for investigation  
-**Impact**: User session management completeness  
-**Effort**: 3-5 hours  
+**Status**: Created, ready for investigation
+**Impact**: User session management completeness
+**Effort**: 3-5 hours
 **Acceptance Criteria**:
 - Session cookie cleared on logout
 - Session data removed from Redis
@@ -55,9 +55,9 @@
 ### ðŸŸ¡ HIGH (Next 3-5 days)
 
 #### 2. Refresh Token Implementation
-**Status**: Not started  
-**Impact**: Session security and token rotation  
-**Effort**: 4-6 hours  
+**Status**: Not started
+**Impact**: Session security and token rotation
+**Effort**: 4-6 hours
 **Current State**: Refresh tokens are empty strings in responses
 
 **Recommended Approach**:
@@ -69,9 +69,9 @@
 6. Add E2E tests for token refresh
 
 #### 3. Session Timeout Handling
-**Status**: Not started  
-**Impact**: Security and user experience  
-**Effort**: 2-3 hours  
+**Status**: Not started
+**Impact**: Security and user experience
+**Effort**: 2-3 hours
 **Current State**: Sessions have 24-hour TTL but no timeout handling
 
 **Recommended Approach**:
@@ -85,18 +85,18 @@
 ### ðŸŸ¢ MEDIUM (Next 1-2 weeks)
 
 #### 4. Multi-Device Session Management
-**Status**: Not started  
-**Impact**: User experience across devices  
+**Status**: Not started
+**Impact**: User experience across devices
 **Effort**: 5-7 hours
 
 #### 5. Session Activity Tracking
-**Status**: Not started  
-**Impact**: Security monitoring and analytics  
+**Status**: Not started
+**Impact**: Security monitoring and analytics
 **Effort**: 3-4 hours
 
 #### 6. Concurrent Session Limits
-**Status**: Not started  
-**Impact**: Security and resource management  
+**Status**: Not started
+**Impact**: Security and resource management
 **Effort**: 4-5 hours
 
 ---
@@ -138,7 +138,7 @@
 ## Infrastructure Improvements
 
 ### 1. Redis Configuration
-**Current**: Single Redis instance with 24-hour session TTL  
+**Current**: Single Redis instance with 24-hour session TTL
 **Recommended**:
 - Add Redis persistence (RDB/AOF)
 - Implement Redis replication for HA
@@ -146,7 +146,7 @@
 - Document Redis backup strategy
 
 ### 2. Logging & Monitoring
-**Current**: Debug logging at debug level  
+**Current**: Debug logging at debug level
 **Recommended**:
 - Implement structured logging aggregation
 - Add session lifecycle metrics
@@ -154,7 +154,7 @@
 - Set up alerts for session errors
 
 ### 3. Security Hardening
-**Current**: Basic session security  
+**Current**: Basic session security
 **Recommended**:
 - Implement CSRF protection
 - Add rate limiting for auth endpoints
@@ -243,4 +243,3 @@
 The staging environment is 90% ready for production deployment. The primary blocker is the logout functionality (Issue #51), which should be addressed immediately. After that, refresh token implementation and session timeout handling are the next priorities.
 
 With focused effort on the recommended priority order, the system should be production-ready within 2-3 weeks.
-
diff --git a/docs/architecture/agentic-primitives-analysis.md b/docs/architecture/agentic-primitives-analysis.md
index c4ced76f0..c0905fabb 100644
--- a/docs/architecture/agentic-primitives-analysis.md
+++ b/docs/architecture/agentic-primitives-analysis.md
@@ -1,6 +1,6 @@
 # Agentic Primitives & Context Engineering Analysis for TTA
 
-**Date:** 2025-10-20  
+**Date:** 2025-10-20
 **Reference:** [GitHub Blog: How to build reliable AI workflows with agentic primitives and context engineering](https://github.blog/ai-and-ml/github-copilot/how-to-build-reliable-ai-workflows-with-agentic-primitives-and-context-engineering/)
 
 ## Executive Summary
@@ -492,4 +492,3 @@ TTA's existing architecture already implements many agentic primitives effective
 By focusing on these areas, TTA can build more reliable, observable, and sophisticated therapeutic AI workflows while maintaining our commitment to appropriate complexity and therapeutic safety.
 
 The recommended roadmap prioritizes reliability and observability first, aligning with our therapeutic focus and component maturity workflow. Implementation should proceed incrementally, building on existing patterns rather than introducing unnecessary complexity.
-
diff --git a/docs/architecture/agentic-primitives-implementation-plan.md b/docs/architecture/agentic-primitives-implementation-plan.md
index 565f3ed26..a0c0322c2 100644
--- a/docs/architecture/agentic-primitives-implementation-plan.md
+++ b/docs/architecture/agentic-primitives-implementation-plan.md
@@ -1,7 +1,7 @@
 # Agentic Primitives Implementation Plan
 
-**Date:** 2025-10-20  
-**Status:** Planning  
+**Date:** 2025-10-20
+**Status:** Planning
 **Priority:** High
 
 ## Overview
@@ -41,12 +41,12 @@ class ContextWindow:
     current_tokens: int
     messages: list[dict[str, Any]]
     metadata: dict[str, Any]
-    
+
     @property
     def utilization(self) -> float:
         """Return context window utilization (0.0 to 1.0)."""
         return self.current_tokens / self.max_tokens
-    
+
     @property
     def remaining_tokens(self) -> int:
         """Return remaining token capacity."""
@@ -61,14 +61,14 @@ class TokenCounter(Protocol):
 class ContextWindowManager:
     """
     Manages LLM context windows with automatic pruning and optimization.
-    
+
     Features:
     - Token counting and tracking
     - Automatic context pruning
     - Context summarization
     - Multi-scale context management (immediate, session, historical)
     """
-    
+
     def __init__(
         self,
         max_tokens: int = 8000,
@@ -80,37 +80,37 @@ class ContextWindowManager:
         self.token_counter = token_counter or self._default_token_counter()
         self.pruning_strategy = pruning_strategy
         self.pruning_threshold = pruning_threshold
-    
+
     def create_window(self, initial_messages: list[dict] | None = None) -> ContextWindow:
         """Create a new context window."""
         messages = initial_messages or []
         current_tokens = sum(self.token_counter.count_tokens(str(m)) for m in messages)
-        
+
         return ContextWindow(
             max_tokens=self.max_tokens,
             current_tokens=current_tokens,
             messages=messages,
             metadata={}
         )
-    
+
     def add_message(
-        self, 
-        window: ContextWindow, 
+        self,
+        window: ContextWindow,
         message: dict[str, Any],
         auto_prune: bool = True
     ) -> ContextWindow:
         """Add a message to the context window, pruning if necessary."""
         message_tokens = self.token_counter.count_tokens(str(message))
-        
+
         # Check if pruning needed
         if auto_prune and (window.current_tokens + message_tokens) / window.max_tokens > self.pruning_threshold:
             window = self._prune_context(window, message_tokens)
-        
+
         window.messages.append(message)
         window.current_tokens += message_tokens
-        
+
         return window
-    
+
     def _prune_context(self, window: ContextWindow, needed_tokens: int) -> ContextWindow:
         """Prune context based on configured strategy."""
         if self.pruning_strategy == ContextPruningStrategy.RECENCY:
@@ -121,40 +121,40 @@ class ContextWindowManager:
             return self._prune_hybrid(window, needed_tokens)
         elif self.pruning_strategy == ContextPruningStrategy.SUMMARIZE:
             return self._prune_with_summarization(window, needed_tokens)
-        
+
         return window
-    
+
     def _prune_by_recency(self, window: ContextWindow, needed_tokens: int) -> ContextWindow:
         """Keep most recent messages, remove oldest."""
         # Implementation: Remove oldest messages until we have space
         pass
-    
+
     def _prune_by_relevance(self, window: ContextWindow, needed_tokens: int) -> ContextWindow:
         """Keep most relevant messages based on semantic similarity."""
         # Implementation: Score messages by relevance, keep highest scoring
         pass
-    
+
     def _prune_hybrid(self, window: ContextWindow, needed_tokens: int) -> ContextWindow:
         """Combine recency and relevance for pruning decisions."""
         # Implementation: Weight recency and relevance scores
         pass
-    
+
     def _prune_with_summarization(self, window: ContextWindow, needed_tokens: int) -> ContextWindow:
         """Summarize older context to save tokens."""
         # Implementation: Summarize messages beyond certain age
         pass
-    
+
     def _default_token_counter(self) -> TokenCounter:
         """Default token counter using tiktoken."""
         import tiktoken
-        
+
         class TiktokenCounter:
             def __init__(self):
                 self.encoding = tiktoken.get_encoding("cl100k_base")
-            
+
             def count_tokens(self, text: str) -> int:
                 return len(self.encoding.encode(text))
-        
+
         return TiktokenCounter()
 ```
 
@@ -173,28 +173,28 @@ class UnifiedAgentOrchestrator:
             pruning_strategy=ContextPruningStrategy.HYBRID,
             pruning_threshold=0.8
         )
-    
+
     async def _build_narrative_prompt(self, state: OrchestrationState) -> str:
         """Build narrative prompt with context window management."""
         # Create context window
         window = self.context_manager.create_window()
-        
+
         # Add system message
         window = self.context_manager.add_message(window, {
             "role": "system",
             "content": "You are a therapeutic narrative generator..."
         })
-        
+
         # Add conversation history (with automatic pruning)
         for msg in state.therapeutic_context.get("history", []):
             window = self.context_manager.add_message(window, msg)
-        
+
         # Add current context
         window = self.context_manager.add_message(window, {
             "role": "user",
             "content": state.user_input
         })
-        
+
         # Build final prompt from window
         return self._format_messages(window.messages)
 ```
@@ -215,7 +215,7 @@ def test_context_window_creation():
     """Test creating a context window."""
     manager = ContextWindowManager(max_tokens=1000)
     window = manager.create_window()
-    
+
     assert window.max_tokens == 1000
     assert window.current_tokens == 0
     assert window.utilization == 0.0
@@ -225,10 +225,10 @@ def test_add_message_without_pruning():
     """Test adding messages below pruning threshold."""
     manager = ContextWindowManager(max_tokens=1000, pruning_threshold=0.8)
     window = manager.create_window()
-    
+
     # Add small message
     window = manager.add_message(window, {"role": "user", "content": "Hello"})
-    
+
     assert len(window.messages) == 1
     assert window.current_tokens > 0
 
@@ -241,14 +241,14 @@ def test_automatic_pruning():
         pruning_threshold=0.5
     )
     window = manager.create_window()
-    
+
     # Add messages until pruning triggers
     for i in range(10):
         window = manager.add_message(window, {
             "role": "user",
             "content": f"Message {i}" * 10
         })
-    
+
     # Should have pruned some messages
     assert window.utilization <= 1.0
     assert len(window.messages) < 10
@@ -312,7 +312,7 @@ class RecoveryStrategy:
 class ErrorRecoveryFramework:
     """
     Centralized error recovery framework for agent workflows.
-    
+
     Features:
     - Error classification
     - Severity assessment
@@ -320,15 +320,15 @@ class ErrorRecoveryFramework:
     - Fallback handling
     - Error metrics and logging
     """
-    
+
     def __init__(self):
         self.strategies: dict[ErrorCategory, RecoveryStrategy] = {}
         self.error_counts: dict[ErrorCategory, int] = {}
-    
+
     def register_strategy(self, strategy: RecoveryStrategy) -> None:
         """Register a recovery strategy for an error category."""
         self.strategies[strategy.category] = strategy
-    
+
     def classify_error(self, error: Exception) -> tuple[ErrorCategory, ErrorSeverity]:
         """Classify an error into category and severity."""
         # LLM errors
@@ -336,21 +336,21 @@ class ErrorRecoveryFramework:
             return ErrorCategory.LLM_ERROR, ErrorSeverity.MEDIUM
         if "timeout" in str(error).lower():
             return ErrorCategory.TIMEOUT_ERROR, ErrorSeverity.MEDIUM
-        
+
         # Validation errors
         if "safety" in str(error).lower() or "validation" in str(error).lower():
             return ErrorCategory.VALIDATION_ERROR, ErrorSeverity.HIGH
-        
+
         # State errors
         if "state" in str(error).lower() or "redis" in str(error).lower():
             return ErrorCategory.STATE_ERROR, ErrorSeverity.HIGH
-        
+
         # Tool errors
         if "tool" in str(error).lower():
             return ErrorCategory.TOOL_ERROR, ErrorSeverity.MEDIUM
-        
+
         return ErrorCategory.UNKNOWN, ErrorSeverity.MEDIUM
-    
+
     async def handle_error(
         self,
         error: Exception,
@@ -361,7 +361,7 @@ class ErrorRecoveryFramework:
         """Handle an error with appropriate recovery strategy."""
         # Classify error
         category, severity = self.classify_error(error)
-        
+
         # Create error context
         context = ErrorContext(
             error=error,
@@ -371,16 +371,16 @@ class ErrorRecoveryFramework:
             workflow_id=workflow_id,
             metadata=metadata
         )
-        
+
         # Track error
         self.error_counts[category] = self.error_counts.get(category, 0) + 1
-        
+
         # Get recovery strategy
         strategy = self.strategies.get(category)
         if not strategy:
             # No strategy registered, use default
             return await self._default_recovery(context)
-        
+
         # Attempt recovery
         try:
             result = await strategy.handler(context)
@@ -390,13 +390,13 @@ class ErrorRecoveryFramework:
             if strategy.fallback:
                 return await strategy.fallback(context)
             raise recovery_error
-    
+
     async def _default_recovery(self, context: ErrorContext) -> Any:
         """Default recovery strategy when no specific strategy registered."""
         if context.severity == ErrorSeverity.CRITICAL:
             # Critical errors should not be recovered automatically
             raise context.error
-        
+
         # Log and return None for non-critical errors
         import logging
         logger = logging.getLogger(__name__)
@@ -421,7 +421,7 @@ class UnifiedAgentOrchestrator:
         # ... existing init ...
         self.error_recovery = ErrorRecoveryFramework()
         self._register_recovery_strategies()
-    
+
     def _register_recovery_strategies(self):
         """Register error recovery strategies."""
         # LLM error recovery
@@ -432,7 +432,7 @@ class UnifiedAgentOrchestrator:
             max_retries=3,
             fallback=self._llm_fallback
         ))
-        
+
         # Validation error recovery
         self.error_recovery.register_strategy(RecoveryStrategy(
             name="validation_fallback",
@@ -441,11 +441,11 @@ class UnifiedAgentOrchestrator:
             max_retries=1,
             fallback=self._validation_fallback
         ))
-    
+
     async def _recover_llm_error(self, context: ErrorContext) -> Any:
         """Recover from LLM errors with retry and backoff."""
         import asyncio
-        
+
         for attempt in range(3):
             await asyncio.sleep(2 ** attempt)  # Exponential backoff
             try:
@@ -455,9 +455,9 @@ class UnifiedAgentOrchestrator:
             except Exception:
                 if attempt == 2:
                     raise
-        
+
         return None
-    
+
     async def _llm_fallback(self, context: ErrorContext) -> Any:
         """Fallback for LLM errors - use cached response or template."""
         # Return a safe, generic response
@@ -502,17 +502,17 @@ class ToolExecutionTrace:
     started_at: datetime
     ended_at: datetime | None = None
     duration_ms: float | None = None
-    
+
     # Input/Output
     input_params: dict[str, Any] = field(default_factory=dict)
     output_result: Any | None = None
     error: str | None = None
-    
+
     # Context
     agent_id: str | None = None
     workflow_id: str | None = None
     session_id: str | None = None
-    
+
     # Metrics
     token_usage: dict[str, int] = field(default_factory=dict)
     metadata: dict[str, Any] = field(default_factory=dict)
@@ -521,18 +521,18 @@ class ToolExecutionTrace:
 class ToolObservabilityCollector:
     """
     Collects and manages tool execution observability data.
-    
+
     Features:
     - Execution tracing
     - Performance metrics
     - Error tracking
     - Result validation logging
     """
-    
+
     def __init__(self):
         self.traces: dict[str, ToolExecutionTrace] = {}
         self.metrics: dict[str, list[float]] = {}
-    
+
     def start_execution(
         self,
         tool_name: str,
@@ -542,7 +542,7 @@ class ToolObservabilityCollector:
     ) -> str:
         """Start tracking a tool execution."""
         import uuid
-        
+
         execution_id = str(uuid.uuid4())
         trace = ToolExecutionTrace(
             tool_name=tool_name,
@@ -553,10 +553,10 @@ class ToolObservabilityCollector:
             agent_id=agent_id,
             workflow_id=workflow_id
         )
-        
+
         self.traces[execution_id] = trace
         return execution_id
-    
+
     def end_execution(
         self,
         execution_id: str,
@@ -568,24 +568,24 @@ class ToolObservabilityCollector:
         trace = self.traces.get(execution_id)
         if not trace:
             return
-        
+
         trace.ended_at = datetime.utcnow()
         trace.duration_ms = (trace.ended_at - trace.started_at).total_seconds() * 1000
         trace.status = status
         trace.output_result = output_result
         trace.error = error
-        
+
         # Update metrics
         if trace.tool_name not in self.metrics:
             self.metrics[trace.tool_name] = []
         self.metrics[trace.tool_name].append(trace.duration_ms)
-    
+
     def get_tool_metrics(self, tool_name: str) -> dict[str, Any]:
         """Get performance metrics for a tool."""
         durations = self.metrics.get(tool_name, [])
         if not durations:
             return {}
-        
+
         return {
             "tool_name": tool_name,
             "execution_count": len(durations),
@@ -615,6 +615,5 @@ class ToolObservabilityCollector:
 
 ---
 
-**Status:** Ready for implementation  
+**Status:** Ready for implementation
 **Next Review:** After Phase 1 completion
-
diff --git a/docs/architecture/agentic-primitives-quick-reference.md b/docs/architecture/agentic-primitives-quick-reference.md
index b3225f27f..a0de94871 100644
--- a/docs/architecture/agentic-primitives-quick-reference.md
+++ b/docs/architecture/agentic-primitives-quick-reference.md
@@ -1,6 +1,6 @@
 # Agentic Primitives Quick Reference
 
-**For:** Developers implementing agentic primitives in TTA  
+**For:** Developers implementing agentic primitives in TTA
 **Last Updated:** 2025-10-20
 
 ## Overview
@@ -54,27 +54,27 @@ class UnifiedAgentOrchestrator:
             max_tokens=8000,
             pruning_strategy=ContextPruningStrategy.HYBRID
         )
-    
+
     async def _build_prompt(self, state: OrchestrationState) -> str:
         # Create managed context window
         window = self.context_manager.create_window()
-        
+
         # Add system prompt
         window = self.context_manager.add_message(window, {
             "role": "system",
             "content": self._get_system_prompt()
         })
-        
+
         # Add conversation history (auto-pruned)
         for msg in state.therapeutic_context.get("history", []):
             window = self.context_manager.add_message(window, msg)
-        
+
         # Add current input
         window = self.context_manager.add_message(window, {
             "role": "user",
             "content": state.user_input
         })
-        
+
         return self._format_messages(window.messages)
 ```
 
@@ -90,20 +90,20 @@ def custom_therapeutic_pruning(
     """Custom pruning that preserves therapeutic context."""
     # Always keep system message
     system_msgs = [m for m in window.messages if m.get("role") == "system"]
-    
+
     # Keep recent therapeutic insights
     therapeutic_msgs = [
-        m for m in window.messages 
+        m for m in window.messages
         if "therapeutic_insight" in m.get("metadata", {})
     ]
-    
+
     # Keep most recent conversation
     recent_msgs = window.messages[-10:]
-    
+
     # Combine and deduplicate
     preserved = system_msgs + therapeutic_msgs + recent_msgs
     # ... (implementation details)
-    
+
     return window
 ```
 
@@ -149,11 +149,11 @@ except Exception as e:
 async def recover_llm_error(context: ErrorContext) -> Any:
     """Recover from LLM errors with exponential backoff."""
     import asyncio
-    
+
     for attempt in range(3):
         # Exponential backoff
         await asyncio.sleep(2 ** attempt)
-        
+
         try:
             # Retry with same parameters
             return await retry_llm_call(context.metadata["params"])
@@ -163,7 +163,7 @@ async def recover_llm_error(context: ErrorContext) -> Any:
                 raise
             # Log and continue
             logger.warning(f"Retry {attempt + 1} failed: {e}")
-    
+
     return None
 
 async def llm_fallback(context: ErrorContext) -> Any:
@@ -182,7 +182,7 @@ async def llm_fallback(context: ErrorContext) -> Any:
 class NGAAdapter:
     def __init__(self, error_recovery: ErrorRecoveryFramework):
         self.error_recovery = error_recovery
-    
+
     async def generate_narrative(
         self,
         prompt: str,
@@ -227,7 +227,7 @@ execution_id = tool_observer.start_execution(
 try:
     # Execute tool
     result = await execute_tool(params)
-    
+
     # End tracking (success)
     tool_observer.end_execution(
         execution_id=execution_id,
@@ -262,18 +262,18 @@ def observe_tool_execution(tool_name: str):
                 tool_name=tool_name,
                 input_params=kwargs
             )
-            
+
             try:
                 # Execute function
                 result = await func(*args, **kwargs)
-                
+
                 # End tracking (success)
                 tool_observer.end_execution(
                     execution_id=execution_id,
                     status=ToolExecutionStatus.SUCCESS,
                     output_result=result
                 )
-                
+
                 return result
             except Exception as e:
                 # End tracking (failure)
@@ -283,7 +283,7 @@ def observe_tool_execution(tool_name: str):
                     error=str(e)
                 )
                 raise
-        
+
         return wrapper
     return decorator
 
@@ -300,7 +300,7 @@ async def query_neo4j(query: str, params: dict) -> list:
 class ToolInvocationService:
     def __init__(self, observer: ToolObservabilityCollector):
         self.observer = observer
-    
+
     async def invoke_tool(
         self,
         tool_name: str,
@@ -314,25 +314,25 @@ class ToolInvocationService:
             agent_id=context.get("agent_id"),
             workflow_id=context.get("workflow_id")
         )
-        
+
         try:
             # Get tool from registry
             tool = self.registry.get_tool(tool_name)
-            
+
             # Execute tool
             result = await tool.execute(params)
-            
+
             # Validate result
             if not self._validate_result(result):
                 raise ValueError(f"Invalid result from {tool_name}")
-            
+
             # End tracking (success)
             self.observer.end_execution(
                 execution_id=execution_id,
                 status=ToolExecutionStatus.SUCCESS,
                 output_result=result
             )
-            
+
             return result
         except Exception as e:
             # End tracking (failure)
@@ -353,21 +353,21 @@ class ToolInvocationService:
 ```python
 class EnhancedOrchestrator:
     """Orchestrator using all agentic primitives."""
-    
+
     def __init__(self):
         # Context management
         self.context_manager = ContextWindowManager(
             max_tokens=8000,
             pruning_strategy=ContextPruningStrategy.HYBRID
         )
-        
+
         # Error recovery
         self.error_recovery = ErrorRecoveryFramework()
         self._register_recovery_strategies()
-        
+
         # Tool observability
         self.tool_observer = ToolObservabilityCollector()
-    
+
     async def process_input(
         self,
         user_input: str,
@@ -382,25 +382,25 @@ class EnhancedOrchestrator:
                 "role": "user",
                 "content": user_input
             })
-            
+
             # 2. Execute with tool observability
             execution_id = self.tool_observer.start_execution(
                 tool_name="process_input",
                 input_params={"input": user_input}
             )
-            
+
             # 3. Process with error recovery
             result = await self._process_with_recovery(window)
-            
+
             # 4. Track success
             self.tool_observer.end_execution(
                 execution_id=execution_id,
                 status=ToolExecutionStatus.SUCCESS,
                 output_result=result
             )
-            
+
             return result
-            
+
         except Exception as e:
             # Error recovery handles this
             return await self.error_recovery.handle_error(
@@ -421,31 +421,31 @@ async def process_with_safety(
     """Process input with therapeutic safety checks."""
     # Build context
     window = orchestrator.context_manager.create_window()
-    
+
     # Add safety context
     window = orchestrator.context_manager.add_message(window, {
         "role": "system",
         "content": "Maintain therapeutic safety. Flag concerning content."
     })
-    
+
     # Add user input
     window = orchestrator.context_manager.add_message(window, {
         "role": "user",
         "content": user_input
     })
-    
+
     try:
         # Process with safety validation
         result = await orchestrator.process_input(user_input, session_id)
-        
+
         # Validate safety
         safety_level = await validate_safety(result)
         if safety_level == SafetyLevel.UNSAFE:
             # Use error recovery for safety issues
             raise ValidationError("Unsafe content detected")
-        
+
         return result
-        
+
     except ValidationError as e:
         # Recovery framework handles safety fallback
         return await orchestrator.error_recovery.handle_error(
@@ -468,16 +468,16 @@ def test_context_pruning():
         max_tokens=100,
         pruning_threshold=0.5
     )
-    
+
     window = manager.create_window()
-    
+
     # Add messages until pruning triggers
     for i in range(20):
         window = manager.add_message(window, {
             "role": "user",
             "content": f"Message {i}" * 10
         })
-    
+
     # Should have pruned
     assert window.utilization <= 1.0
     assert len(window.messages) < 20
@@ -490,7 +490,7 @@ def test_context_pruning():
 async def test_error_recovery():
     """Test error recovery with retry."""
     recovery = ErrorRecoveryFramework()
-    
+
     # Register test strategy
     recovery.register_strategy(RecoveryStrategy(
         name="test_retry",
@@ -498,11 +498,11 @@ async def test_error_recovery():
         handler=mock_retry_handler,
         max_retries=3
     ))
-    
+
     # Simulate error
     error = Exception("Rate limit exceeded")
     result = await recovery.handle_error(error)
-    
+
     # Should have recovered
     assert result is not None
 ```
@@ -513,7 +513,7 @@ async def test_error_recovery():
 def test_tool_metrics():
     """Test tool metrics collection."""
     observer = ToolObservabilityCollector()
-    
+
     # Simulate executions
     for i in range(10):
         exec_id = observer.start_execution(
@@ -524,7 +524,7 @@ def test_tool_metrics():
             execution_id=exec_id,
             status=ToolExecutionStatus.SUCCESS
         )
-    
+
     # Check metrics
     metrics = observer.get_tool_metrics("test_tool")
     assert metrics["execution_count"] == 10
@@ -563,13 +563,13 @@ agent_orchestration:
     pruning_strategy: hybrid
     pruning_threshold: 0.8
     summarization_enabled: true
-  
+
   error_recovery:
     enabled: true
     max_retries: 3
     backoff_base: 2
     fallback_enabled: true
-  
+
   tool_observability:
     enabled: true
     trace_all_tools: true
@@ -583,26 +583,26 @@ agent_orchestration:
 
 ### Context Window Issues
 
-**Problem:** Token limit exceeded  
+**Problem:** Token limit exceeded
 **Solution:** Check `pruning_threshold`, ensure auto-pruning enabled
 
-**Problem:** Important context lost  
+**Problem:** Important context lost
 **Solution:** Implement custom pruning strategy that preserves critical messages
 
 ### Error Recovery Issues
 
-**Problem:** Errors not recovering  
+**Problem:** Errors not recovering
 **Solution:** Check recovery strategy registration, verify error classification
 
-**Problem:** Too many retries  
+**Problem:** Too many retries
 **Solution:** Adjust `max_retries`, implement circuit breaker
 
 ### Tool Observability Issues
 
-**Problem:** Missing metrics  
+**Problem:** Missing metrics
 **Solution:** Ensure `start_execution` and `end_execution` called for all tools
 
-**Problem:** High overhead  
+**Problem:** High overhead
 **Solution:** Use async logging, batch metrics updates
 
 ---
@@ -616,6 +616,5 @@ agent_orchestration:
 
 ---
 
-**Last Updated:** 2025-10-20  
+**Last Updated:** 2025-10-20
 **Maintainer:** Development Team
-
diff --git a/docs/architecture/agentic-primitives-recommendations.md b/docs/architecture/agentic-primitives-recommendations.md
index 9caece5e1..8c826a23f 100644
--- a/docs/architecture/agentic-primitives-recommendations.md
+++ b/docs/architecture/agentic-primitives-recommendations.md
@@ -1,7 +1,7 @@
 # Agentic Primitives: Prioritized Recommendations for TTA
 
-**Date:** 2025-10-20  
-**Audience:** Development Team, Product Owners  
+**Date:** 2025-10-20
+**Audience:** Development Team, Product Owners
 **Priority:** High
 
 ## Executive Summary
@@ -36,7 +36,7 @@ Based on analysis of the GitHub blog post on agentic primitives and context engi
 - 20-30% reduction in token usage
 - Better context quality through intelligent pruning
 
-**Effort:** 3-5 days  
+**Effort:** 3-5 days
 **Component Maturity:** Development â†’ Staging
 
 **Code Location:** `src/agent_orchestration/context/window_manager.py`
@@ -66,7 +66,7 @@ Based on analysis of the GitHub blog post on agentic primitives and context engi
 - Consistent error handling across all agents
 - Better user experience during failures
 
-**Effort:** 4-6 days  
+**Effort:** 4-6 days
 **Component Maturity:** Development â†’ Staging
 
 **Code Location:** `src/agent_orchestration/recovery/error_handler.py`
@@ -96,7 +96,7 @@ Based on analysis of the GitHub blog post on agentic primitives and context engi
 - Tool performance dashboards
 - Faster debugging and optimization
 
-**Effort:** 3-4 days  
+**Effort:** 3-4 days
 **Component Maturity:** Development â†’ Staging
 
 **Code Location:** Extend `src/agent_orchestration/tools/metrics.py`
@@ -116,7 +116,7 @@ Based on analysis of the GitHub blog post on agentic primitives and context engi
 
 **Why:** Enables more sophisticated therapeutic scenarios with coherent multi-turn interactions
 
-**Effort:** 5-7 days  
+**Effort:** 5-7 days
 **Component Maturity:** Development
 
 **Code Location:** `src/agent_orchestration/planning/`
@@ -129,7 +129,7 @@ Based on analysis of the GitHub blog post on agentic primitives and context engi
 
 **Why:** Improves prompt quality and agent performance through adaptive prompting
 
-**Effort:** 4-6 days  
+**Effort:** 4-6 days
 **Component Maturity:** Development
 
 **Code Location:** Extend `src/ai_components/prompts.py`
@@ -142,7 +142,7 @@ Based on analysis of the GitHub blog post on agentic primitives and context engi
 
 **Why:** Essential for performance optimization and debugging complex workflows
 
-**Effort:** 6-8 days  
+**Effort:** 6-8 days
 **Component Maturity:** Staging
 
 **Code Location:** `src/agent_orchestration/tracing/`
@@ -357,8 +357,7 @@ The recommended agentic primitives align closely with the GitHub blog's framewor
 
 ---
 
-**Document Status:** Ready for Review  
-**Next Review:** After team discussion  
-**Owner:** Development Team  
+**Document Status:** Ready for Review
+**Next Review:** After team discussion
+**Owner:** Development Team
 **Stakeholders:** Product, Engineering, Therapeutic Advisory
-
diff --git a/docs/prompt-versioning-guide.md b/docs/prompt-versioning-guide.md
index dfc411a10..16462f198 100644
--- a/docs/prompt-versioning-guide.md
+++ b/docs/prompt-versioning-guide.md
@@ -45,12 +45,12 @@ description: "Brief description of what this prompt does"
 agent_type: "agent_name"  # e.g., "safety_validator", "narrative_generator"
 template: |
   Your prompt template here with {variable_placeholders}.
-  
+
   Example:
   Analyze this input: "{user_input}"
-  
+
   Context: {context}
-  
+
   Respond with structured output.
 variables:
   - user_input
@@ -129,10 +129,10 @@ try:
         intent=intent,
         world_context=context
     )
-    
+
     response = await llm.ainvoke([SystemMessage(content=prompt_text)])
     latency_ms = (time.time() - start_time) * 1000
-    
+
     # Record metrics
     registry.record_metrics(
         "narrative_generation",
@@ -141,7 +141,7 @@ try:
         cost_usd=0.0005,
         quality_score=8.5,
     )
-    
+
 except Exception as e:
     # Record error
     registry.record_metrics(
@@ -358,4 +358,3 @@ See `src/agent_orchestration/langgraph_orchestrator.py` for production examples:
 - [AI Development Best Practices Audit](../docs/ai-development-audit.md)
 - [LLM Response Quality Testing](../docs/llm-quality-testing.md)
 - [Prompt Engineering Guide](../docs/prompt-engineering.md)
-
diff --git a/migration_report.json b/migration_report.json
index 34c3b30ec..8799496cc 100644
--- a/migration_report.json
+++ b/migration_report.json
@@ -1017,4 +1017,4 @@
     "files_with_import_updates": 198,
     "total_import_changes": 213
   }
-}
\ No newline at end of file
+}
diff --git a/packages/tta-ai-framework/src/tta_ai/models/api.py b/packages/tta-ai-framework/src/tta_ai/models/api.py
index 9abccf231..bda4d7c1e 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/api.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/api.py
@@ -82,14 +82,10 @@ async def get_model_management() -> ModelManagementComponent:
     # component = get_component("model_management")
     component = None  # Placeholder until component_registry is implemented
     if not component:
-        raise HTTPException(
-            status_code=503, detail="Model management component not available"
-        )
+        raise HTTPException(status_code=503, detail="Model management component not available")
 
     if not component.initialized:
-        raise HTTPException(
-            status_code=503, detail="Model management component not initialized"
-        )
+        raise HTTPException(status_code=503, detail="Model management component not initialized")
 
     return component
 
@@ -276,9 +272,7 @@ async def test_model_connectivity(
 ):
     """Test connectivity and performance of a specific model."""
     try:
-        result = await model_mgmt.test_model_connectivity(
-            request.model_id, request.provider_name
-        )
+        result = await model_mgmt.test_model_connectivity(request.model_id, request.provider_name)
 
         return ModelTestResponse(**result)
 
@@ -341,9 +335,7 @@ async def unload_model(
                 "model_id": model_id,
                 "status": "unloaded",
             }
-        raise HTTPException(
-            status_code=404, detail=f"Model {model_id} not found or not loaded"
-        )
+        raise HTTPException(status_code=404, detail=f"Model {model_id} not found or not loaded")
 
     except Exception as e:
         logger.error(f"Failed to unload model {model_id}: {e}")
@@ -359,13 +351,9 @@ async def get_model_performance(
     """Get performance metrics for a specific model."""
     try:
         if not model_mgmt.performance_monitor:
-            raise HTTPException(
-                status_code=503, detail="Performance monitoring not available"
-            )
+            raise HTTPException(status_code=503, detail="Performance monitoring not available")
 
-        return await model_mgmt.performance_monitor.get_model_performance(
-            model_id, timeframe_hours
-        )
+        return await model_mgmt.performance_monitor.get_model_performance(model_id, timeframe_hours)
 
     except Exception as e:
         logger.error(f"Failed to get performance metrics for {model_id}: {e}")
@@ -379,9 +367,7 @@ async def get_system_performance(
     """Get overall system performance metrics."""
     try:
         if not model_mgmt.performance_monitor:
-            raise HTTPException(
-                status_code=503, detail="Performance monitoring not available"
-            )
+            raise HTTPException(status_code=503, detail="Performance monitoring not available")
 
         return await model_mgmt.performance_monitor.get_system_performance()
 
@@ -397,9 +383,7 @@ async def get_fallback_statistics(
     """Get fallback handler statistics."""
     try:
         if not model_mgmt.fallback_handler:
-            raise HTTPException(
-                status_code=503, detail="Fallback handler not available"
-            )
+            raise HTTPException(status_code=503, detail="Fallback handler not available")
 
         return model_mgmt.fallback_handler.get_failure_statistics()
 
@@ -415,9 +399,7 @@ async def reset_model_failures(
     """Reset failure count for a model."""
     try:
         if not model_mgmt.fallback_handler:
-            raise HTTPException(
-                status_code=503, detail="Fallback handler not available"
-            )
+            raise HTTPException(status_code=503, detail="Fallback handler not available")
 
         success = model_mgmt.fallback_handler.reset_model_failures(model_id)
 
@@ -446,9 +428,7 @@ async def set_openrouter_filter(
 ):
     """Set OpenRouter free models filter settings."""
     try:
-        await model_mgmt.set_openrouter_filter(
-            show_free_only, prefer_free, max_cost_per_token
-        )
+        await model_mgmt.set_openrouter_filter(show_free_only, prefer_free, max_cost_per_token)
 
         return {
             "message": "OpenRouter filter settings updated",
@@ -473,9 +453,7 @@ async def get_openrouter_filter(
         settings = model_mgmt.get_openrouter_filter_settings()
 
         if settings is None:
-            raise HTTPException(
-                status_code=404, detail="OpenRouter provider not available"
-            )
+            raise HTTPException(status_code=404, detail="OpenRouter provider not available")
 
         return {"provider": "openrouter", "settings": settings}
 
diff --git a/packages/tta-ai-framework/src/tta_ai/models/interfaces.py b/packages/tta-ai-framework/src/tta_ai/models/interfaces.py
index 5f6e38658..9984d81ef 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/interfaces.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/interfaces.py
@@ -163,9 +163,7 @@ class IModelProvider(ABC):
         pass
 
     @abstractmethod
-    async def get_available_models(
-        self, filters: dict[str, Any] | None = None
-    ) -> list[ModelInfo]:
+    async def get_available_models(self, filters: dict[str, Any] | None = None) -> list[ModelInfo]:
         """Get list of available models from this provider."""
         pass
 
diff --git a/packages/tta-ai-framework/src/tta_ai/models/model_management_component.py b/packages/tta-ai-framework/src/tta_ai/models/model_management_component.py
index 0cb10db5f..f7c34424e 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/model_management_component.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/model_management_component.py
@@ -74,9 +74,7 @@ class ModelManagementComponent(Component):
 
             # Detect system resources
             logger.info("Detecting system resources...")
-            self.system_resources = (
-                await self.hardware_detector.detect_system_resources()
-            )
+            self.system_resources = await self.hardware_detector.detect_system_resources()
             logger.info(
                 f"System resources: {self.system_resources['total_ram_gb']:.1f}GB RAM, "
                 f"{self.system_resources['gpu_count']} GPUs"
@@ -123,9 +121,7 @@ class ModelManagementComponent(Component):
             logger.error(f"Failed to stop Model Management Component: {e}")
             return False
 
-    async def select_model(
-        self, requirements: ModelRequirements
-    ) -> IModelInstance | None:
+    async def select_model(self, requirements: ModelRequirements) -> IModelInstance | None:
         """Select and load the best model for the given requirements."""
         if not self.initialized:
             raise RuntimeError("Model Management Component not initialized")
@@ -137,9 +133,7 @@ class ModelManagementComponent(Component):
             selected_model_info = await self.model_selector.select_model(requirements)
 
             if not selected_model_info:
-                logger.warning(
-                    f"No suitable model found for requirements: {requirements}"
-                )
+                logger.warning(f"No suitable model found for requirements: {requirements}")
                 return None
 
             # Load the selected model
@@ -155,9 +149,7 @@ class ModelManagementComponent(Component):
 
             # Try fallback if available
             if self.fallback_handler:
-                fallback_model = await self.fallback_handler.get_fallback_model(
-                    "", requirements
-                )
+                fallback_model = await self.fallback_handler.get_fallback_model("", requirements)
                 if fallback_model:
                     return await self.load_model(
                         fallback_model.model_id, fallback_model.provider_type.value
@@ -192,9 +184,7 @@ class ModelManagementComponent(Component):
         logger.info(f"Loaded model {model_id} from provider {provider_name}")
         return instance
 
-    async def unload_model(
-        self, model_id: str, provider_name: str | None = None
-    ) -> bool:
+    async def unload_model(self, model_id: str, provider_name: str | None = None) -> bool:
         """Unload a specific model."""
         try:
             # Find the model to unload
@@ -236,9 +226,7 @@ class ModelManagementComponent(Component):
 
         all_models = []
 
-        providers_to_check = (
-            [provider_name] if provider_name else list(self.providers.keys())
-        )
+        providers_to_check = [provider_name] if provider_name else list(self.providers.keys())
 
         for prov_name in providers_to_check:
             if prov_name in self.providers:
@@ -246,9 +234,7 @@ class ModelManagementComponent(Component):
                     models = await self.providers[prov_name].get_available_models()
                     all_models.extend(models)
                 except Exception as e:
-                    logger.warning(
-                        f"Failed to get models from provider {prov_name}: {e}"
-                    )
+                    logger.warning(f"Failed to get models from provider {prov_name}: {e}")
 
         # Apply free filter if requested
         if free_only:
@@ -257,13 +243,9 @@ class ModelManagementComponent(Component):
 
         return all_models
 
-    async def get_free_models(
-        self, provider_name: str | None = None
-    ) -> list[ModelInfo]:
+    async def get_free_models(self, provider_name: str | None = None) -> list[ModelInfo]:
         """Get only free models from all or specific providers."""
-        return await self.get_available_models(
-            provider_name=provider_name, free_only=True
-        )
+        return await self.get_available_models(provider_name=provider_name, free_only=True)
 
     async def get_openrouter_free_models(self) -> list[ModelInfo]:
         """Get free models specifically from OpenRouter provider."""
@@ -276,9 +258,7 @@ class ModelManagementComponent(Component):
             if hasattr(provider, "get_free_models"):
                 return await provider.get_free_models()
             # Fallback to general free filter
-            return await self.get_available_models(
-                provider_name="openrouter", free_only=True
-            )
+            return await self.get_available_models(provider_name="openrouter", free_only=True)
         except Exception as e:
             logger.error(f"Failed to get OpenRouter free models: {e}")
             return []
@@ -292,8 +272,7 @@ class ModelManagementComponent(Component):
         affordable_models = []
         for model in all_models:
             if model.is_free or (
-                model.cost_per_token is not None
-                and model.cost_per_token <= max_cost_per_token
+                model.cost_per_token is not None and model.cost_per_token <= max_cost_per_token
             ):
                 affordable_models.append(model)
 
@@ -315,9 +294,7 @@ class ModelManagementComponent(Component):
 
         provider = self.providers["openrouter"]
         if hasattr(provider, "set_free_models_filter"):
-            await provider.set_free_models_filter(
-                show_free_only, prefer_free, max_cost_per_token
-            )
+            await provider.set_free_models_filter(show_free_only, prefer_free, max_cost_per_token)
             logger.info(
                 f"Updated OpenRouter filter: free_only={show_free_only}, prefer_free={prefer_free}, max_cost={max_cost_per_token}"
             )
@@ -339,9 +316,7 @@ class ModelManagementComponent(Component):
         """Get model recommendations for a specific task type."""
         return await self.hardware_detector.recommend_models(task_type)
 
-    async def test_model_connectivity(
-        self, model_id: str, provider_name: str
-    ) -> dict[str, Any]:
+    async def test_model_connectivity(self, model_id: str, provider_name: str) -> dict[str, Any]:
         """Test connectivity and performance of a specific model."""
         try:
             # Load the model
@@ -366,9 +341,7 @@ class ModelManagementComponent(Component):
                 "response_generated": bool(response.text),
                 "latency_ms": latency_ms,
                 "test_response": (
-                    response.text[:100] + "..."
-                    if len(response.text) > 100
-                    else response.text
+                    response.text[:100] + "..." if len(response.text) > 100 else response.text
                 ),
                 "status": "success",
             }
@@ -414,9 +387,7 @@ class ModelManagementComponent(Component):
             # Create configuration object with defaults
             return ModelManagementConfig(
                 enabled=model_config_dict.get("enabled", True),
-                default_provider=model_config_dict.get(
-                    "default_provider", "openrouter"
-                ),
+                default_provider=model_config_dict.get("default_provider", "openrouter"),
                 **model_config_dict,
             )
 
@@ -454,9 +425,7 @@ class ModelManagementComponent(Component):
         self.performance_monitor = PerformanceMonitor()
 
         # Initialize fallback handler
-        self.fallback_handler = FallbackHandler(
-            self.providers, self.model_config.fallback_config
-        )
+        self.fallback_handler = FallbackHandler(self.providers, self.model_config.fallback_config)
 
         # Initialize model selector
         self.model_selector = ModelSelector(
@@ -528,9 +497,7 @@ class ModelManagementComponent(Component):
                     timestamp=datetime.now(),
                     response_time_ms=response.latency_ms or 0,
                     tokens_per_second=0,  # Would need to calculate
-                    total_tokens=(
-                        response.usage.get("total_tokens", 0) if response.usage else 0
-                    ),
+                    total_tokens=(response.usage.get("total_tokens", 0) if response.usage else 0),
                     task_type=task_type,
                 )
                 await self.performance_monitor.record_metrics(
diff --git a/packages/tta-ai-framework/src/tta_ai/models/models.py b/packages/tta-ai-framework/src/tta_ai/models/models.py
index d7b77dc40..7113db166 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/models.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/models.py
@@ -176,9 +176,7 @@ class ProviderConfiguration:
 class ModelSelectionCriteria:
     """Criteria for model selection."""
 
-    primary_criteria: str = (
-        "cost_effectiveness"  # cost_effectiveness, performance, availability
-    )
+    primary_criteria: str = "cost_effectiveness"  # cost_effectiveness, performance, availability
     fallback_criteria: str = "availability"
 
     # Weights for selection scoring (should sum to 1.0)
@@ -234,12 +232,8 @@ class ModelManagementConfig:
     models: dict[str, ModelConfiguration] = field(default_factory=dict)
 
     # Selection and fallback
-    selection_strategy: ModelSelectionCriteria = field(
-        default_factory=ModelSelectionCriteria
-    )
-    fallback_config: FallbackConfiguration = field(
-        default_factory=FallbackConfiguration
-    )
+    selection_strategy: ModelSelectionCriteria = field(default_factory=ModelSelectionCriteria)
+    fallback_config: FallbackConfiguration = field(default_factory=FallbackConfiguration)
 
     # Monitoring and caching
     performance_monitoring_enabled: bool = True
diff --git a/packages/tta-ai-framework/src/tta_ai/models/providers/base.py b/packages/tta-ai-framework/src/tta_ai/models/providers/base.py
index fe64222d4..517e354fb 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/providers/base.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/providers/base.py
@@ -44,9 +44,7 @@ class BaseModelInstance(IModelInstance):
         """Default health check implementation."""
         try:
             # Simple test generation
-            test_request = GenerationRequest(
-                prompt="Hello", max_tokens=1, temperature=0.0
-            )
+            test_request = GenerationRequest(prompt="Hello", max_tokens=1, temperature=0.0)
             response = await self.generate(test_request)
             self._status = ModelStatus.READY if response.text else ModelStatus.ERROR
             self._last_health_check = datetime.now()
@@ -97,16 +95,12 @@ class BaseProvider(IModelProvider, ABC):
 
             # Validate required configuration
             if not await self._validate_config(config):
-                logger.error(
-                    f"Invalid configuration for {self.provider_type.value} provider"
-                )
+                logger.error(f"Invalid configuration for {self.provider_type.value} provider")
                 return False
 
             # Provider-specific initialization
             if not await self._initialize_provider():
-                logger.error(
-                    f"Failed to initialize {self.provider_type.value} provider"
-                )
+                logger.error(f"Failed to initialize {self.provider_type.value} provider")
                 return False
 
             # Initial model discovery
@@ -120,9 +114,7 @@ class BaseProvider(IModelProvider, ABC):
             logger.error(f"Error initializing {self.provider_type.value} provider: {e}")
             return False
 
-    async def get_available_models(
-        self, filters: dict[str, Any] | None = None
-    ) -> list[ModelInfo]:
+    async def get_available_models(self, filters: dict[str, Any] | None = None) -> list[ModelInfo]:
         """Get list of available models from this provider."""
         if not self._initialized:
             raise RuntimeError(f"Provider {self.provider_type.value} not initialized")
@@ -158,14 +150,10 @@ class BaseProvider(IModelProvider, ABC):
         try:
             instance = await self._load_model_impl(model_id, config or {})
             self._loaded_models[model_id] = instance
-            logger.info(
-                f"Successfully loaded model {model_id} from {self.provider_type.value}"
-            )
+            logger.info(f"Successfully loaded model {model_id} from {self.provider_type.value}")
             return instance
         except Exception as e:
-            logger.error(
-                f"Failed to load model {model_id} from {self.provider_type.value}: {e}"
-            )
+            logger.error(f"Failed to load model {model_id} from {self.provider_type.value}: {e}")
             raise
 
     async def unload_model(self, model_id: str) -> bool:
@@ -177,14 +165,10 @@ class BaseProvider(IModelProvider, ABC):
             instance = self._loaded_models[model_id]
             await self._unload_model_impl(instance)
             del self._loaded_models[model_id]
-            logger.info(
-                f"Successfully unloaded model {model_id} from {self.provider_type.value}"
-            )
+            logger.info(f"Successfully unloaded model {model_id} from {self.provider_type.value}")
             return True
         except Exception as e:
-            logger.error(
-                f"Failed to unload model {model_id} from {self.provider_type.value}: {e}"
-            )
+            logger.error(f"Failed to unload model {model_id} from {self.provider_type.value}: {e}")
             return False
 
     async def health_check(self) -> bool:
@@ -207,9 +191,7 @@ class BaseProvider(IModelProvider, ABC):
             return provider_healthy
 
         except Exception as e:
-            logger.error(
-                f"Health check failed for {self.provider_type.value} provider: {e}"
-            )
+            logger.error(f"Health check failed for {self.provider_type.value} provider: {e}")
             self._health_status = False
             return False
 
@@ -222,9 +204,7 @@ class BaseProvider(IModelProvider, ABC):
             "loaded_models_count": len(self._loaded_models),
             "available_models_count": len(self._available_models),
             "last_model_refresh": (
-                self._last_model_refresh.isoformat()
-                if self._last_model_refresh
-                else None
+                self._last_model_refresh.isoformat() if self._last_model_refresh else None
             ),
             "loaded_models": list(self._loaded_models.keys()),
         }
@@ -247,9 +227,7 @@ class BaseProvider(IModelProvider, ABC):
         pass
 
     @abstractmethod
-    async def _load_model_impl(
-        self, model_id: str, config: dict[str, Any]
-    ) -> IModelInstance:
+    async def _load_model_impl(self, model_id: str, config: dict[str, Any]) -> IModelInstance:
         """Provider-specific model loading implementation."""
         pass
 
@@ -305,9 +283,7 @@ class BaseProvider(IModelProvider, ABC):
         required_caps = filters.get("required_capabilities", [])
         if required_caps:
             filtered_models = [
-                m
-                for m in filtered_models
-                if all(cap in m.capabilities for cap in required_caps)
+                m for m in filtered_models if all(cap in m.capabilities for cap in required_caps)
             ]
 
         return filtered_models
diff --git a/packages/tta-ai-framework/src/tta_ai/models/providers/custom_api.py b/packages/tta-ai-framework/src/tta_ai/models/providers/custom_api.py
index 8611100f7..3c87c9d0d 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/providers/custom_api.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/providers/custom_api.py
@@ -96,9 +96,7 @@ class CustomAPIModelInstance(BaseModelInstance):
             self._status = ModelStatus.ERROR
             raise
 
-    async def generate_stream(
-        self, request: GenerationRequest
-    ) -> AsyncGenerator[str, None]:
+    async def generate_stream(self, request: GenerationRequest) -> AsyncGenerator[str, None]:
         """Generate text as a stream using custom API."""
         try:
             # Prepare request based on API type
@@ -190,8 +188,7 @@ class CustomAPIModelInstance(BaseModelInstance):
         return {
             "prompt_tokens": usage.get("input_tokens", 0),
             "completion_tokens": usage.get("output_tokens", 0),
-            "total_tokens": usage.get("input_tokens", 0)
-            + usage.get("output_tokens", 0),
+            "total_tokens": usage.get("input_tokens", 0) + usage.get("output_tokens", 0),
         }
 
     async def _stream_openai_response(self, response) -> AsyncGenerator[str, None]:
@@ -257,9 +254,7 @@ class CustomAPIProvider(BaseProvider):
             base_url = provider_config.get("base_url")
 
             if not api_key or not base_url:
-                logger.error(
-                    f"Missing api_key or base_url for provider {provider_name}"
-                )
+                logger.error(f"Missing api_key or base_url for provider {provider_name}")
                 return False
 
             self._api_configs[provider_name] = provider_config
@@ -309,9 +304,7 @@ class CustomAPIProvider(BaseProvider):
 
             for provider_name, config in self._api_configs.items():
                 try:
-                    provider_models = await self._get_provider_models(
-                        provider_name, config
-                    )
+                    provider_models = await self._get_provider_models(provider_name, config)
                     models.extend(provider_models)
                 except Exception as e:
                     logger.warning(f"Failed to get models from {provider_name}: {e}")
@@ -347,13 +340,9 @@ class CustomAPIProvider(BaseProvider):
                         continue
 
                     # Determine pricing and capabilities based on provider
-                    cost_per_token, is_free = self._get_model_pricing(
-                        provider_name, model_id
-                    )
+                    cost_per_token, is_free = self._get_model_pricing(provider_name, model_id)
                     capabilities = self._get_model_capabilities(provider_name, model_id)
-                    context_length = self._get_model_context_length(
-                        provider_name, model_id
-                    )
+                    context_length = self._get_model_context_length(provider_name, model_id)
 
                     model_info = ModelInfo(
                         model_id=model_id,
@@ -381,9 +370,7 @@ class CustomAPIProvider(BaseProvider):
             )
             return self._get_predefined_models(provider_name, config)
 
-    def _get_predefined_models(
-        self, provider_name: str, config: dict[str, Any]
-    ) -> list[ModelInfo]:
+    def _get_predefined_models(self, provider_name: str, config: dict[str, Any]) -> list[ModelInfo]:
         """Get predefined models for providers that don't support model listing."""
         api_type = config.get("api_type", "openai")
 
@@ -492,9 +479,7 @@ class CustomAPIProvider(BaseProvider):
         # Consider healthy if at least half of providers are working
         return healthy_count >= (total_count / 2) if total_count > 0 else False
 
-    async def _test_provider_connection(
-        self, provider_name: str, config: dict[str, Any]
-    ):
+    async def _test_provider_connection(self, provider_name: str, config: dict[str, Any]):
         """Test connection to a provider."""
         client = self._clients.get(provider_name)
         if not client:
@@ -512,9 +497,7 @@ class CustomAPIProvider(BaseProvider):
         except httpx.TimeoutException:
             raise RuntimeError(f"Provider {provider_name} connection timeout") from None
 
-    def _get_model_pricing(
-        self, provider_name: str, model_id: str
-    ) -> tuple[float | None, bool]:
+    def _get_model_pricing(self, provider_name: str, model_id: str) -> tuple[float | None, bool]:
         """Get pricing information for a model."""
         # This would typically be configured or fetched from the provider
         # For now, return reasonable defaults
@@ -533,9 +516,7 @@ class CustomAPIProvider(BaseProvider):
 
         return capabilities
 
-    def _get_model_context_length(
-        self, provider_name: str, model_id: str
-    ) -> int | None:
+    def _get_model_context_length(self, provider_name: str, model_id: str) -> int | None:
         """Get context length for a model."""
         # Common context lengths
         if "gpt-4" in model_id.lower():
@@ -544,9 +525,7 @@ class CustomAPIProvider(BaseProvider):
             return 200000
         return 4096  # Default
 
-    def _get_therapeutic_safety_score(
-        self, provider_name: str, model_id: str
-    ) -> float | None:
+    def _get_therapeutic_safety_score(self, provider_name: str, model_id: str) -> float | None:
         """Get therapeutic safety score for a model."""
         if "claude" in model_id.lower():
             return 9.0  # Claude models are generally very safe
diff --git a/packages/tta-ai-framework/src/tta_ai/models/providers/lm_studio.py b/packages/tta-ai-framework/src/tta_ai/models/providers/lm_studio.py
index d18e1b255..d3ff6b120 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/providers/lm_studio.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/providers/lm_studio.py
@@ -27,9 +27,7 @@ logger = logging.getLogger(__name__)
 class LMStudioModelInstance(BaseModelInstance):
     """LM Studio model instance implementation."""
 
-    def __init__(
-        self, model_id: str, provider: "LMStudioProvider", client: httpx.AsyncClient
-    ):
+    def __init__(self, model_id: str, provider: "LMStudioProvider", client: httpx.AsyncClient):
         super().__init__(model_id, provider)
         self._client = client
         self._status = ModelStatus.READY
@@ -53,9 +51,7 @@ class LMStudioModelInstance(BaseModelInstance):
                 payload["stop"] = request.stop_sequences
 
             # Make API request
-            response = await self._client.post(
-                "/v1/chat/completions", json=payload, timeout=60.0
-            )
+            response = await self._client.post("/v1/chat/completions", json=payload, timeout=60.0)
             response.raise_for_status()
 
             data = response.json()
@@ -92,9 +88,7 @@ class LMStudioModelInstance(BaseModelInstance):
             self._status = ModelStatus.ERROR
             raise
 
-    async def generate_stream(
-        self, request: GenerationRequest
-    ) -> AsyncGenerator[str, None]:
+    async def generate_stream(self, request: GenerationRequest) -> AsyncGenerator[str, None]:
         """Generate text as a stream using LM Studio API."""
         try:
             # Prepare request payload
diff --git a/packages/tta-ai-framework/src/tta_ai/models/providers/local.py b/packages/tta-ai-framework/src/tta_ai/models/providers/local.py
index b646048db..ecb309bac 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/providers/local.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/providers/local.py
@@ -144,9 +144,7 @@ class LocalModelInstance(BaseModelInstance):
             self._status = ModelStatus.ERROR
             raise
 
-    async def generate_stream(
-        self, request: GenerationRequest
-    ) -> AsyncGenerator[str, None]:
+    async def generate_stream(self, request: GenerationRequest) -> AsyncGenerator[str, None]:
         """Generate text as a stream (simplified implementation)."""
         try:
             # For now, generate full text and yield in chunks
@@ -279,9 +277,7 @@ class LocalModelProvider(BaseProvider):
             logger.error(f"Failed to refresh local models: {e}")
             raise
 
-    async def _load_model_impl(
-        self, model_id: str, config: dict[str, Any]
-    ) -> LocalModelInstance:
+    async def _load_model_impl(self, model_id: str, config: dict[str, Any]) -> LocalModelInstance:
         """Load a local model instance."""
         try:
             # Check if we can load another model
@@ -393,13 +389,9 @@ class LocalModelProvider(BaseProvider):
                     memory_cached = torch.cuda.memory_reserved(i)
                     total_memory = torch.cuda.get_device_properties(i).total_memory
 
-                    usage_percent = (
-                        (memory_allocated + memory_cached) / total_memory * 100
-                    )
+                    usage_percent = (memory_allocated + memory_cached) / total_memory * 100
                     if usage_percent > 95:
-                        logger.warning(
-                            f"GPU {i} memory usage is high ({usage_percent:.1f}%)"
-                        )
+                        logger.warning(f"GPU {i} memory usage is high ({usage_percent:.1f}%)")
                         return False
 
             return True
@@ -459,6 +451,4 @@ class LocalModelProvider(BaseProvider):
             },
         }
 
-        return size_estimates.get(
-            model_id, {"parameters": "Unknown", "disk_gb": 2, "ram_gb": 4}
-        )
+        return size_estimates.get(model_id, {"parameters": "Unknown", "disk_gb": 2, "ram_gb": 4})
diff --git a/packages/tta-ai-framework/src/tta_ai/models/providers/ollama.py b/packages/tta-ai-framework/src/tta_ai/models/providers/ollama.py
index aa47d618e..83777fba3 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/providers/ollama.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/providers/ollama.py
@@ -33,9 +33,7 @@ logger = logging.getLogger(__name__)
 class OllamaModelInstance(BaseModelInstance):
     """Ollama model instance implementation."""
 
-    def __init__(
-        self, model_id: str, provider: "OllamaProvider", client: httpx.AsyncClient
-    ):
+    def __init__(self, model_id: str, provider: "OllamaProvider", client: httpx.AsyncClient):
         super().__init__(model_id, provider)
         self._client = client
         self._status = ModelStatus.READY
@@ -64,9 +62,7 @@ class OllamaModelInstance(BaseModelInstance):
                 payload["options"]["stop"] = request.stop_sequences
 
             # Make API request
-            response = await self._client.post(
-                "/api/generate", json=payload, timeout=60.0
-            )
+            response = await self._client.post("/api/generate", json=payload, timeout=60.0)
             response.raise_for_status()
 
             data = response.json()
@@ -83,8 +79,7 @@ class OllamaModelInstance(BaseModelInstance):
             usage = {
                 "prompt_tokens": data.get("prompt_eval_count", 0),
                 "completion_tokens": data.get("eval_count", 0),
-                "total_tokens": data.get("prompt_eval_count", 0)
-                + data.get("eval_count", 0),
+                "total_tokens": data.get("prompt_eval_count", 0) + data.get("eval_count", 0),
             }
 
             # Update metrics
@@ -116,9 +111,7 @@ class OllamaModelInstance(BaseModelInstance):
             self._status = ModelStatus.ERROR
             raise
 
-    async def generate_stream(
-        self, request: GenerationRequest
-    ) -> AsyncGenerator[str, None]:
+    async def generate_stream(self, request: GenerationRequest) -> AsyncGenerator[str, None]:
         """Generate text as a stream using Ollama API."""
         try:
             # Prepare request payload
@@ -252,9 +245,7 @@ class OllamaProvider(BaseProvider):
                     name=name,
                     provider_type=ProviderType.OLLAMA,
                     description=(
-                        f"Local Ollama model ({size_gb:.1f}GB)"
-                        if size_gb
-                        else "Local Ollama model"
+                        f"Local Ollama model ({size_gb:.1f}GB)" if size_gb else "Local Ollama model"
                     ),
                     context_length=context_length,
                     cost_per_token=0.0,  # Local models are free
@@ -274,9 +265,7 @@ class OllamaProvider(BaseProvider):
             logger.error(f"Failed to refresh Ollama models: {e}")
             raise
 
-    async def _load_model_impl(
-        self, model_id: str, config: dict[str, Any]
-    ) -> OllamaModelInstance:
+    async def _load_model_impl(self, model_id: str, config: dict[str, Any]) -> OllamaModelInstance:
         """Load an Ollama model instance."""
         if not self._client:
             raise RuntimeError("Ollama client not initialized")
@@ -341,9 +330,7 @@ class OllamaProvider(BaseProvider):
                 container = self._docker_client.containers.get(self._container_name)
                 if container.status != "running":
                     container.start()
-                    logger.info(
-                        f"Started existing Ollama container: {self._container_name}"
-                    )
+                    logger.info(f"Started existing Ollama container: {self._container_name}")
                 return
             except docker.errors.NotFound:
                 pass
@@ -359,9 +346,7 @@ class OllamaProvider(BaseProvider):
                 # Check if NVIDIA runtime is available
                 runtime_info = self._docker_client.info()
                 if "nvidia" in runtime_info.get("Runtimes", {}):
-                    device_requests = [
-                        docker.types.DeviceRequest(count=-1, capabilities=[["gpu"]])
-                    ]
+                    device_requests = [docker.types.DeviceRequest(count=-1, capabilities=[["gpu"]])]
 
             # Type ignore for docker-py API compatibility (restart_policy dict format)
             container = self._docker_client.containers.run(
diff --git a/packages/tta-ai-framework/src/tta_ai/models/providers/openrouter.py b/packages/tta-ai-framework/src/tta_ai/models/providers/openrouter.py
index b0dee97b6..3c5c705f8 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/providers/openrouter.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/providers/openrouter.py
@@ -29,9 +29,7 @@ logger = logging.getLogger(__name__)
 class OpenRouterModelInstance(BaseModelInstance):
     """OpenRouter model instance implementation."""
 
-    def __init__(
-        self, model_id: str, provider: "OpenRouterProvider", client: httpx.AsyncClient
-    ):
+    def __init__(self, model_id: str, provider: "OpenRouterProvider", client: httpx.AsyncClient):
         super().__init__(model_id, provider)
         self._client = client
         self._status = ModelStatus.READY
@@ -94,9 +92,7 @@ class OpenRouterModelInstance(BaseModelInstance):
             self._status = ModelStatus.ERROR
             raise
 
-    async def generate_stream(
-        self, request: GenerationRequest
-    ) -> AsyncGenerator[str, None]:
+    async def generate_stream(self, request: GenerationRequest) -> AsyncGenerator[str, None]:
         """Generate text as a stream using OpenRouter API."""
         try:
             # Prepare request payload
@@ -236,9 +232,7 @@ class OpenRouterProvider(BaseProvider):
                 "X-Title": "TTA Platform",  # Optional: your app name
             }
 
-            self._client = httpx.AsyncClient(
-                base_url=self._base_url, headers=headers, timeout=30.0
-            )
+            self._client = httpx.AsyncClient(base_url=self._base_url, headers=headers, timeout=30.0)
 
             # Test connection
             response = await self._client.get("/api/v1/models")
@@ -280,9 +274,7 @@ class OpenRouterProvider(BaseProvider):
 
                 if prompt_cost is not None and completion_cost is not None:
                     # Convert from cost per million tokens to cost per token
-                    cost_per_token = (
-                        float(prompt_cost) + float(completion_cost)
-                    ) / 2000000
+                    cost_per_token = (float(prompt_cost) + float(completion_cost)) / 2000000
                     is_free = cost_per_token == 0.0
                 elif prompt_cost == "0" and completion_cost == "0":
                     is_free = True
@@ -353,9 +345,7 @@ class OpenRouterProvider(BaseProvider):
             logger.warning(f"OpenRouter health check failed: {e}")
             return False
 
-    async def get_available_models(
-        self, filters: dict[str, Any] | None = None
-    ) -> list[ModelInfo]:
+    async def get_available_models(self, filters: dict[str, Any] | None = None) -> list[ModelInfo]:
         """Get available models with optional free models filtering."""
         # Get all models from base implementation
         all_models = await super().get_available_models(filters)
@@ -412,8 +402,7 @@ class OpenRouterProvider(BaseProvider):
 
         for model in all_models:
             if model.is_free or (
-                model.cost_per_token is not None
-                and model.cost_per_token <= max_cost_per_token
+                model.cost_per_token is not None and model.cost_per_token <= max_cost_per_token
             ):
                 affordable_models.append(model)
 
diff --git a/packages/tta-ai-framework/src/tta_ai/models/services/fallback_handler.py b/packages/tta-ai-framework/src/tta_ai/models/services/fallback_handler.py
index 48fce9720..62ae0ab19 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/services/fallback_handler.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/services/fallback_handler.py
@@ -59,20 +59,14 @@ class FallbackHandler(IFallbackHandler):
                 return None
 
             # Filter out the failed model and recently failed models
-            candidate_models = self._filter_failed_models(
-                available_models, failed_model_id
-            )
+            candidate_models = self._filter_failed_models(available_models, failed_model_id)
 
             if not candidate_models:
-                logger.warning(
-                    "No candidate models available after filtering failed models"
-                )
+                logger.warning("No candidate models available after filtering failed models")
                 return None
 
             # Filter models based on requirements
-            compatible_models = await self._filter_compatible_models(
-                candidate_models, requirements
-            )
+            compatible_models = await self._filter_compatible_models(candidate_models, requirements)
 
             if not compatible_models:
                 logger.warning("No compatible fallback models found")
@@ -150,9 +144,7 @@ class FallbackHandler(IFallbackHandler):
                 healthy_models.extend(models)
 
             except Exception as e:
-                logger.warning(
-                    f"Failed to get models from provider {provider_name}: {e}"
-                )
+                logger.warning(f"Failed to get models from provider {provider_name}: {e}")
                 self._provider_health[provider_name] = False
 
         return healthy_models
@@ -206,8 +198,7 @@ class FallbackHandler(IFallbackHandler):
                 continue
 
             if (
-                requirements.therapeutic_safety_required
-                and model.therapeutic_safety_score
+                requirements.therapeutic_safety_required and model.therapeutic_safety_score
             ) and model.therapeutic_safety_score < 7.0:  # Minimum safety threshold
                 continue
 
@@ -245,9 +236,7 @@ class FallbackHandler(IFallbackHandler):
             models,
             key=lambda m: (
                 m.performance_score or 5.0,  # Default score for unknown performance
-                -self._failure_counts.get(
-                    m.model_id, 0
-                ),  # Prefer models with fewer failures
+                -self._failure_counts.get(m.model_id, 0),  # Prefer models with fewer failures
                 m.therapeutic_safety_score or 7.0,  # Prefer safer models
             ),
             reverse=True,
@@ -256,9 +245,7 @@ class FallbackHandler(IFallbackHandler):
         # Apply provider preference if configured
         if self.config.prefer_different_provider:
             # Try to find a model from a different provider than the failed one
-            failed_provider = self._get_model_provider(
-                models[0].model_id
-            )  # Approximate
+            failed_provider = self._get_model_provider(models[0].model_id)  # Approximate
 
             for model in sorted_models:
                 if model.provider_type.value != failed_provider:
@@ -276,9 +263,7 @@ class FallbackHandler(IFallbackHandler):
             key=lambda m: (
                 m.cost_per_token or 0.0,  # Prefer lower cost
                 -(m.performance_score or 5.0),  # Then by performance (descending)
-                -self._failure_counts.get(
-                    m.model_id, 0
-                ),  # Prefer models with fewer failures
+                -self._failure_counts.get(m.model_id, 0),  # Prefer models with fewer failures
             ),
         )
 
@@ -292,9 +277,7 @@ class FallbackHandler(IFallbackHandler):
         sorted_models = sorted(
             models,
             key=lambda m: (
-                self._failure_counts.get(
-                    m.model_id, 0
-                ),  # Prefer models with fewer failures
+                self._failure_counts.get(m.model_id, 0),  # Prefer models with fewer failures
                 -(m.performance_score or 5.0),  # Then by performance (descending)
                 m.cost_per_token or 0.0,  # Then by cost (ascending)
             ),
@@ -350,9 +333,7 @@ class FallbackHandler(IFallbackHandler):
                         )
 
                 except Exception as e:
-                    logger.error(
-                        f"Failed to check health of provider {provider_name}: {e}"
-                    )
+                    logger.error(f"Failed to check health of provider {provider_name}: {e}")
                     self._provider_health[provider_name] = False
 
         except Exception as e:
@@ -384,10 +365,7 @@ class FallbackHandler(IFallbackHandler):
                 recent_failures[model_id] = {
                     "last_failure": failure_time.isoformat(),
                     "total_failures": self._failure_counts[model_id],
-                    "minutes_since_failure": (
-                        current_time - failure_time
-                    ).total_seconds()
-                    / 60,
+                    "minutes_since_failure": (current_time - failure_time).total_seconds() / 60,
                 }
 
         return {
diff --git a/packages/tta-ai-framework/src/tta_ai/models/services/hardware_detector.py b/packages/tta-ai-framework/src/tta_ai/models/services/hardware_detector.py
index dc5d62d29..3426677fa 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/services/hardware_detector.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/services/hardware_detector.py
@@ -233,9 +233,7 @@ class HardwareDetector(IHardwareDetector):
         # Default estimates for unknown models
         default_requirements = {"ram_gb": 8, "vram_gb": 4, "disk_gb": 8}
 
-        requirements: dict[str, Any] = model_sizes.get(
-            model_id, default_requirements
-        ).copy()
+        requirements: dict[str, Any] = model_sizes.get(model_id, default_requirements).copy()
 
         # Add additional requirements
         requirements.update(
@@ -244,9 +242,7 @@ class HardwareDetector(IHardwareDetector):
                 "python_version_min": "3.8",
                 "supports_quantization": True,
                 "quantization_options": ["4bit", "8bit", "16bit"],
-                "estimated_load_time_seconds": (
-                    30 if requirements["ram_gb"] <= 16 else 120
-                ),
+                "estimated_load_time_seconds": (30 if requirements["ram_gb"] <= 16 else 120),
             }
         )
 
@@ -277,11 +273,7 @@ class HardwareDetector(IHardwareDetector):
             for i in range(device_count):
                 handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                 name_raw = pynvml.nvmlDeviceGetName(handle)
-                name = (
-                    name_raw.decode("utf-8")
-                    if isinstance(name_raw, bytes)
-                    else name_raw
-                )
+                name = name_raw.decode("utf-8") if isinstance(name_raw, bytes) else name_raw
                 memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                 memory_gb = float(memory_info.total) / (1024**3)
 
diff --git a/packages/tta-ai-framework/src/tta_ai/models/services/model_selector.py b/packages/tta-ai-framework/src/tta_ai/models/services/model_selector.py
index d2e8eb9b3..4d653bef3 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/services/model_selector.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/services/model_selector.py
@@ -50,14 +50,10 @@ class ModelSelector(IModelSelector):
                 return None
 
             # Filter models based on requirements
-            compatible_models = await self._filter_compatible_models(
-                available_models, requirements
-            )
+            compatible_models = await self._filter_compatible_models(available_models, requirements)
 
             if not compatible_models:
-                logger.warning(
-                    f"No compatible models found for requirements: {requirements}"
-                )
+                logger.warning(f"No compatible models found for requirements: {requirements}")
                 return None
 
             # Rank models by suitability
@@ -177,9 +173,7 @@ class ModelSelector(IModelSelector):
                 all_models.extend(models)
 
             except Exception as e:
-                logger.warning(
-                    f"Failed to get models from provider {provider_name}: {e}"
-                )
+                logger.warning(f"Failed to get models from provider {provider_name}: {e}")
                 continue
 
         return all_models
@@ -205,9 +199,7 @@ class ModelSelector(IModelSelector):
 
             # Base score from model performance
             if model.performance_score:
-                score += (
-                    model.performance_score * self.selection_criteria.performance_weight
-                )
+                score += model.performance_score * self.selection_criteria.performance_weight
             else:
                 # Default score for unknown performance
                 score += 5.0 * self.selection_criteria.performance_weight
@@ -232,9 +224,7 @@ class ModelSelector(IModelSelector):
             if model.cost_per_token and model.cost_per_token > 0:
                 # Normalize cost (assuming max acceptable cost of $0.01 per token)
                 max_cost = 0.01
-                cost_score = max(
-                    0, 10.0 * (1 - min(model.cost_per_token / max_cost, 1.0))
-                )
+                cost_score = max(0, 10.0 * (1 - min(model.cost_per_token / max_cost, 1.0)))
 
             score += cost_score * self.selection_criteria.cost_weight
 
@@ -257,9 +247,7 @@ class ModelSelector(IModelSelector):
             score += hardware_bonus
 
             # Performance history bonus
-            performance_bonus = await self._calculate_performance_history_bonus(
-                model.model_id
-            )
+            performance_bonus = await self._calculate_performance_history_bonus(model.model_id)
             score += performance_bonus
 
             return max(0.0, score)  # Ensure non-negative score
@@ -268,9 +256,7 @@ class ModelSelector(IModelSelector):
             logger.error(f"Score calculation failed for {model.model_id}: {e}")
             return 0.0
 
-    async def _calculate_task_bonus(
-        self, model: ModelInfo, task_type: TaskType
-    ) -> float:
+    async def _calculate_task_bonus(self, model: ModelInfo, task_type: TaskType) -> float:
         """Calculate bonus score based on task-specific model suitability."""
         bonus = 0.0
 
@@ -331,9 +317,7 @@ class ModelSelector(IModelSelector):
         """Calculate bonus based on hardware compatibility."""
         try:
             # Get hardware requirements for the model
-            requirements = await self.hardware_detector.estimate_model_requirements(
-                model.model_id
-            )
+            requirements = await self.hardware_detector.estimate_model_requirements(model.model_id)
 
             # Get system resources
             system_resources = await self.hardware_detector.detect_system_resources()
@@ -393,9 +377,7 @@ class ModelSelector(IModelSelector):
             quality_bonus = max(0, (average_quality - 5.0) / 5.0 + 2.0)
 
             # Reliability bonus based on success rate
-            success_rates = [
-                m.success_rate for m in recent_metrics if m.success_rate is not None
-            ]
+            success_rates = [m.success_rate for m in recent_metrics if m.success_rate is not None]
             if success_rates:
                 avg_success_rate = sum(success_rates) / len(success_rates)
                 reliability_bonus = max(
@@ -428,9 +410,7 @@ class ModelSelector(IModelSelector):
         # Keep only recent metrics (last 100 entries or 24 hours)
         cutoff_time = datetime.now() - timedelta(hours=24)
         self._performance_cache[model_id] = [
-            m
-            for m in self._performance_cache[model_id][-100:]
-            if m.timestamp > cutoff_time
+            m for m in self._performance_cache[model_id][-100:] if m.timestamp > cutoff_time
         ]
 
     def clear_cache(self):
diff --git a/packages/tta-ai-framework/src/tta_ai/models/services/performance_monitor.py b/packages/tta-ai-framework/src/tta_ai/models/services/performance_monitor.py
index 3f69d9178..31044c5fd 100644
--- a/packages/tta-ai-framework/src/tta_ai/models/services/performance_monitor.py
+++ b/packages/tta-ai-framework/src/tta_ai/models/services/performance_monitor.py
@@ -193,23 +193,17 @@ class PerformanceMonitor(IPerformanceMonitor):
                         )
 
                     if model_stats.get("success_rate"):
-                        total_success_rates.extend(
-                            [model_stats["success_rate"]] * model_requests
-                        )
+                        total_success_rates.extend([model_stats["success_rate"]] * model_requests)
 
                     system_stats["models"][model_id] = {
                         "requests": model_requests,
-                        "avg_response_time_ms": model_stats.get(
-                            "average_response_time_ms"
-                        ),
+                        "avg_response_time_ms": model_stats.get("average_response_time_ms"),
                         "success_rate": model_stats.get("success_rate"),
                     }
 
             # Calculate system averages
             if request_count > 0:
-                system_stats["average_response_time_ms"] = (
-                    total_response_time / request_count
-                )
+                system_stats["average_response_time_ms"] = total_response_time / request_count
 
             if total_quality_scores:
                 system_stats["average_quality_score"] = sum(total_quality_scores) / len(
@@ -229,9 +223,7 @@ class PerformanceMonitor(IPerformanceMonitor):
             logger.error(f"Failed to get system performance: {e}")
             return {"error": str(e)}
 
-    def _calculate_aggregated_stats(
-        self, metrics: list[PerformanceMetrics]
-    ) -> dict[str, Any]:
+    def _calculate_aggregated_stats(self, metrics: list[PerformanceMetrics]) -> dict[str, Any]:
         """Calculate aggregated statistics from metrics."""
         if not metrics:
             return {}
@@ -241,18 +233,12 @@ class PerformanceMonitor(IPerformanceMonitor):
 
         # Token statistics
         total_tokens = sum(m.total_tokens for m in metrics)
-        tokens_per_second = [
-            m.tokens_per_second for m in metrics if m.tokens_per_second > 0
-        ]
+        tokens_per_second = [m.tokens_per_second for m in metrics if m.tokens_per_second > 0]
 
         # Quality scores
-        quality_scores = [
-            m.quality_score for m in metrics if m.quality_score is not None
-        ]
+        quality_scores = [m.quality_score for m in metrics if m.quality_score is not None]
         safety_scores = [
-            m.therapeutic_safety_score
-            for m in metrics
-            if m.therapeutic_safety_score is not None
+            m.therapeutic_safety_score for m in metrics if m.therapeutic_safety_score is not None
         ]
 
         # Success rates
@@ -260,15 +246,11 @@ class PerformanceMonitor(IPerformanceMonitor):
         error_counts = [m.error_count for m in metrics]
 
         # Resource usage
-        memory_usage = [
-            m.memory_usage_mb for m in metrics if m.memory_usage_mb is not None
-        ]
+        memory_usage = [m.memory_usage_mb for m in metrics if m.memory_usage_mb is not None]
         gpu_memory_usage = [
             m.gpu_memory_usage_mb for m in metrics if m.gpu_memory_usage_mb is not None
         ]
-        cpu_usage = [
-            m.cpu_usage_percent for m in metrics if m.cpu_usage_percent is not None
-        ]
+        cpu_usage = [m.cpu_usage_percent for m in metrics if m.cpu_usage_percent is not None]
 
         stats: dict[str, Any] = {
             "total_requests": len(metrics),
@@ -280,8 +262,7 @@ class PerformanceMonitor(IPerformanceMonitor):
         if response_times:
             stats.update(
                 {
-                    "average_response_time_ms": sum(response_times)
-                    / len(response_times),
+                    "average_response_time_ms": sum(response_times) / len(response_times),
                     "min_response_time_ms": min(response_times),
                     "max_response_time_ms": max(response_times),
                     "p95_response_time_ms": self._percentile(response_times, 95),
@@ -293,8 +274,7 @@ class PerformanceMonitor(IPerformanceMonitor):
         if tokens_per_second:
             stats.update(
                 {
-                    "average_tokens_per_second": sum(tokens_per_second)
-                    / len(tokens_per_second),
+                    "average_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second),
                     "max_tokens_per_second": max(tokens_per_second),
                 }
             )
@@ -334,8 +314,7 @@ class PerformanceMonitor(IPerformanceMonitor):
         if gpu_memory_usage:
             stats.update(
                 {
-                    "average_gpu_memory_usage_mb": sum(gpu_memory_usage)
-                    / len(gpu_memory_usage),
+                    "average_gpu_memory_usage_mb": sum(gpu_memory_usage) / len(gpu_memory_usage),
                     "peak_gpu_memory_usage_mb": max(gpu_memory_usage),
                 }
             )
@@ -502,9 +481,7 @@ class PerformanceMonitor(IPerformanceMonitor):
 
         logger.debug(f"Cleaned up metrics older than {cutoff_time}")
 
-    async def get_model_usage_stats(
-        self, model_id: str, period_hours: int = 24
-    ) -> ModelUsageStats:
+    async def get_model_usage_stats(self, model_id: str, period_hours: int = 24) -> ModelUsageStats:
         """Get usage statistics for a model."""
         end_time = datetime.now()
         start_time = end_time - timedelta(hours=period_hours)
@@ -522,9 +499,7 @@ class PerformanceMonitor(IPerformanceMonitor):
             total_tokens_generated=performance_data.get("total_tokens", 0),
             average_tokens_per_request=performance_data.get("total_tokens", 0)
             / max(performance_data.get("total_requests", 1), 1),
-            average_response_time_ms=performance_data.get(
-                "average_response_time_ms", 0
-            ),
+            average_response_time_ms=performance_data.get("average_response_time_ms", 0),
             p95_response_time_ms=performance_data.get("p95_response_time_ms", 0),
             average_quality_score=performance_data.get("average_quality_score"),
             peak_memory_usage_mb=performance_data.get("peak_memory_usage_mb"),
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/__init__.py b/packages/tta-ai-framework/src/tta_ai/orchestration/__init__.py
index 828d74895..701073bb5 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/__init__.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/__init__.py
@@ -101,9 +101,7 @@ try:
     from .tools.redis_tool_registry import RedisToolRegistry
 except Exception:
     # Tools package may be optional during partial builds/tests
-    ToolSpec = ToolParameter = ToolRegistration = ToolInvocation = ToolPolicy = (
-        ToolStatus
-    ) = None
+    ToolSpec = ToolParameter = ToolRegistration = ToolInvocation = ToolPolicy = ToolStatus = None
     RedisToolRegistry = ToolCoordinator = None
 
 __all__ = [
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/adapters.py b/packages/tta-ai-framework/src/tta_ai/orchestration/adapters.py
index 4bac11d09..515c867b9 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/adapters.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/adapters.py
@@ -24,9 +24,7 @@ try:
     from agents.ipa import IntentSchema, process_input
     from agents.narrative_generator import generate_narrative_response
 except ImportError as e:
-    logging.warning(
-        f"Could not import real agents: {e}. Using fallback implementations."
-    )
+    logging.warning(f"Could not import real agents: {e}. Using fallback implementations.")
     process_input = None
     IntentSchema = None
     generate_narrative_response = None
@@ -60,9 +58,7 @@ class RetryConfig:
         self.jitter = jitter
 
 
-async def retry_with_backoff(
-    func: callable, retry_config: RetryConfig, *args, **kwargs
-) -> Any:
+async def retry_with_backoff(func: callable, retry_config: RetryConfig, *args, **kwargs) -> Any:
     """
     Execute a function with exponential backoff retry logic.
 
@@ -105,9 +101,7 @@ async def retry_with_backoff(
             if retry_config.jitter:
                 delay *= 0.5 + random.random() * 0.5
 
-            logger.warning(
-                f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s..."
-            )
+            logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s...")
 
             await asyncio.sleep(delay)
 
@@ -120,9 +114,7 @@ async def retry_with_backoff(
 class IPAAdapter:
     """Adapter for communicating with the real Input Processor Agent."""
 
-    def __init__(
-        self, fallback_to_mock: bool = True, retry_config: RetryConfig | None = None
-    ):
+    def __init__(self, fallback_to_mock: bool = True, retry_config: RetryConfig | None = None):
         self.fallback_to_mock = fallback_to_mock
         self.retry_config = retry_config or RetryConfig()
         self._available = process_input is not None and IntentSchema is not None
@@ -142,9 +134,7 @@ class IPAAdapter:
         """
         if not self._available:
             if self.fallback_to_mock:
-                logger.warning(
-                    "Real IPA not available, using fallback mock implementation"
-                )
+                logger.warning("Real IPA not available, using fallback mock implementation")
                 return self._mock_process_input(text)
             raise AgentCommunicationError("Real IPA implementation not available")
 
@@ -154,9 +144,7 @@ class IPAAdapter:
                 loop = asyncio.get_event_loop()
                 return await loop.run_in_executor(None, process_input, text)
 
-            intent_result = await retry_with_backoff(
-                _process_with_executor, self.retry_config
-            )
+            intent_result = await retry_with_backoff(_process_with_executor, self.retry_config)
 
             # Convert IntentSchema to dict format expected by orchestration system
             if hasattr(intent_result, "dict"):
@@ -240,9 +228,7 @@ class WBAAdapter:
         """
         if not self._available or not self._wba_instance:
             if self.fallback_to_mock:
-                logger.warning(
-                    "Real WBA not available, using fallback mock implementation"
-                )
+                logger.warning("Real WBA not available, using fallback mock implementation")
                 return self._mock_process_world(world_id, updates)
             raise AgentCommunicationError("Real WBA implementation not available")
 
@@ -257,9 +243,7 @@ class WBAAdapter:
             # Run WBA processing in thread pool with retry logic
             async def _process_with_executor():
                 loop = asyncio.get_event_loop()
-                return await loop.run_in_executor(
-                    None, self._wba_instance.process, wba_input
-                )
+                return await loop.run_in_executor(None, self._wba_instance.process, wba_input)
 
             result = await retry_with_backoff(_process_with_executor, self.retry_config)
 
@@ -300,9 +284,7 @@ class WBAAdapter:
 class NGAAdapter:
     """Adapter for communicating with the real Narrative Generator Agent."""
 
-    def __init__(
-        self, fallback_to_mock: bool = True, retry_config: RetryConfig | None = None
-    ):
+    def __init__(self, fallback_to_mock: bool = True, retry_config: RetryConfig | None = None):
         self.fallback_to_mock = fallback_to_mock
         self.retry_config = retry_config or RetryConfig()
         self._available = generate_narrative_response is not None
@@ -325,9 +307,7 @@ class NGAAdapter:
         """
         if not self._available:
             if self.fallback_to_mock:
-                logger.warning(
-                    "Real NGA not available, using fallback mock implementation"
-                )
+                logger.warning("Real NGA not available, using fallback mock implementation")
                 return self._mock_generate_narrative(prompt, context)
             raise AgentCommunicationError("Real NGA implementation not available")
 
@@ -342,9 +322,7 @@ class NGAAdapter:
             # Run NGA processing in thread pool with retry logic
             async def _process_with_executor():
                 loop = asyncio.get_event_loop()
-                return await loop.run_in_executor(
-                    None, generate_narrative_response, nga_input
-                )
+                return await loop.run_in_executor(None, generate_narrative_response, nga_input)
 
             result = await retry_with_backoff(_process_with_executor, self.retry_config)
 
@@ -400,9 +378,7 @@ class AgentAdapterFactory:
 
     def create_ipa_adapter(self) -> IPAAdapter:
         """Create an IPA adapter instance."""
-        return IPAAdapter(
-            fallback_to_mock=self.fallback_to_mock, retry_config=self.retry_config
-        )
+        return IPAAdapter(fallback_to_mock=self.fallback_to_mock, retry_config=self.retry_config)
 
     def create_wba_adapter(self) -> WBAAdapter:
         """Create a WBA adapter instance."""
@@ -415,6 +391,4 @@ class AgentAdapterFactory:
 
     def create_nga_adapter(self) -> NGAAdapter:
         """Create an NGA adapter instance."""
-        return NGAAdapter(
-            fallback_to_mock=self.fallback_to_mock, retry_config=self.retry_config
-        )
+        return NGAAdapter(fallback_to_mock=self.fallback_to_mock, retry_config=self.retry_config)
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/agents.py b/packages/tta-ai-framework/src/tta_ai/orchestration/agents.py
index c8ceb8396..f475c36a2 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/agents.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/agents.py
@@ -170,21 +170,15 @@ class Agent(AgentProxy):
             if router and isinstance(router, AgentRouter):
                 recipient = await router.resolve_target(recipient)
 
-        msg = self.serialize(
-            recipient, payload, priority=priority, message_type=message_type
-        )
+        msg = self.serialize(recipient, payload, priority=priority, message_type=message_type)
         return await self._coordinator.send_message(
             sender=self.agent_id, recipient=recipient, message=msg
         )
 
-    async def receive_next(
-        self, *, visibility_timeout: int = 5
-    ) -> ReceivedMessage | None:
+    async def receive_next(self, *, visibility_timeout: int = 5) -> ReceivedMessage | None:
         if not self._coordinator:
             return None
-        return await self._coordinator.receive(
-            self.agent_id, visibility_timeout=visibility_timeout
-        )
+        return await self._coordinator.receive(self.agent_id, visibility_timeout=visibility_timeout)
 
     async def ack(self, token: str) -> bool:
         if not self._coordinator:
@@ -200,9 +194,7 @@ class Agent(AgentProxy):
     ) -> bool:
         if not self._coordinator:
             return False
-        return await self._coordinator.nack(
-            self.agent_id, token, failure=failure, error=error
-        )
+        return await self._coordinator.nack(self.agent_id, token, failure=failure, error=error)
 
     # ---- Processing API ----
     async def process(
@@ -228,20 +220,16 @@ class Agent(AgentProxy):
                 self._metrics.requests = 0
             # perf aggregation per agent instance
             with contextlib.suppress(Exception):
-                get_step_aggregator().record(
-                    key, (time.time() - start) * 1000.0, success=True
-                )
+                get_step_aggregator().record(key, (time.time() - start) * 1000.0, success=True)
             return result
-        except asyncio.TimeoutError:
+        except TimeoutError:
             with contextlib.suppress(Exception):
                 self._metrics.record_error()
             # Fallback if record_error fails
             if not hasattr(self._metrics, "errors"):
                 self._metrics.errors = 0
             with contextlib.suppress(Exception):
-                get_step_aggregator().record(
-                    key, (time.time() - start) * 1000.0, success=False
-                )
+                get_step_aggregator().record(key, (time.time() - start) * 1000.0, success=False)
             raise
         except Exception as e:
             with contextlib.suppress(Exception):
@@ -250,23 +238,15 @@ class Agent(AgentProxy):
             if not hasattr(self._metrics, "errors"):
                 self._metrics.errors = 0
             with contextlib.suppress(Exception):
-                get_step_aggregator().record(
-                    key, (time.time() - start) * 1000.0, success=False
-                )
+                get_step_aggregator().record(key, (time.time() - start) * 1000.0, success=False)
             logger.exception("Agent %s processing error: %s", self.name, e)
             raise
         finally:
-            self._status = (
-                AgentRuntimeStatus.IDLE if self._running else AgentRuntimeStatus.ERROR
-            )
+            self._status = AgentRuntimeStatus.IDLE if self._running else AgentRuntimeStatus.ERROR
 
     # ---- Sync wrappers ----
-    def process_sync(
-        self, input_payload: dict, *, timeout_s: float | None = None
-    ) -> dict:
-        return asyncio.run(
-            self.process_with_timeout(input_payload, timeout_s=timeout_s)
-        )
+    def process_sync(self, input_payload: dict, *, timeout_s: float | None = None) -> dict:
+        return asyncio.run(self.process_with_timeout(input_payload, timeout_s=timeout_s))
 
     def health_check_sync(self) -> dict[str, Any]:
         return asyncio.run(self.health_check())
@@ -407,9 +387,7 @@ class AgentRegistry:
     def _key(self, agent_id: AgentId) -> tuple[str, str]:
         return (agent_id.type.value, agent_id.instance or "default")
 
-    def set_fallback_callback(
-        self, cb: Callable[[Agent, Agent], Awaitable[bool]]
-    ) -> None:
+    def set_fallback_callback(self, cb: Callable[[Agent, Agent], Awaitable[bool]]) -> None:
         self._fallback_cb = cb
 
     def set_restart_callback(self, cb: Callable[[Agent], Awaitable[bool]]) -> None:
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/api/diagnostics.py b/packages/tta-ai-framework/src/tta_ai/orchestration/api/diagnostics.py
index b3a8e6f61..9ee7c8bab 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/api/diagnostics.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/api/diagnostics.py
@@ -9,7 +9,7 @@ from __future__ import annotations
 
 import logging
 import time
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 from typing import Any
 
 from fastapi import APIRouter, Depends, HTTPException, Query
@@ -107,12 +107,8 @@ class DiagnosticsAPI:
         @self.router.get("/agents", response_model=list[AgentDiagnosticInfo])
         async def get_agents_diagnostics(
             agent_type: str | None = Query(None, description="Filter by agent type"),
-            include_performance: bool = Query(
-                True, description="Include performance metrics"
-            ),
-            include_discovery: bool = Query(
-                True, description="Include discovery information"
-            ),
+            include_performance: bool = Query(True, description="Include performance metrics"),
+            include_discovery: bool = Query(True, description="Include discovery information"),
             credentials: HTTPAuthorizationCredentials | None = Depends(security),
         ):
             """Get comprehensive diagnostics for all agents."""
@@ -128,12 +124,8 @@ class DiagnosticsAPI:
         @self.router.get("/agents/{agent_id}", response_model=AgentDiagnosticInfo)
         async def get_agent_diagnostics(
             agent_id: str,
-            include_performance: bool = Query(
-                True, description="Include performance metrics"
-            ),
-            include_discovery: bool = Query(
-                True, description="Include discovery information"
-            ),
+            include_performance: bool = Query(True, description="Include performance metrics"),
+            include_discovery: bool = Query(True, description="Include discovery information"),
             credentials: HTTPAuthorizationCredentials | None = Depends(security),
         ):
             """Get diagnostics for a specific agent."""
@@ -187,9 +179,7 @@ class DiagnosticsAPI:
                 raise HTTPException(status_code=401, detail="Authentication required")
 
             if not self.auto_discovery_manager:
-                raise HTTPException(
-                    status_code=404, detail="Auto-discovery not available"
-                )
+                raise HTTPException(status_code=404, detail="Auto-discovery not available")
 
             return self.auto_discovery_manager.get_discovery_statistics()
 
@@ -225,9 +215,7 @@ class DiagnosticsAPI:
 
         except Exception as e:
             logger.error(f"Failed to get agents diagnostics: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to retrieve diagnostics"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to retrieve diagnostics") from e
 
     async def _get_agent_diagnostics(
         self,
@@ -271,9 +259,7 @@ class DiagnosticsAPI:
             health_status = await self._build_health_status(agent_id, agent_info)
 
             # Build capabilities information
-            capabilities = self._build_capabilities_info(
-                agent_info.get("capabilities", [])
-            )
+            capabilities = self._build_capabilities_info(agent_info.get("capabilities", []))
 
             # Build performance metrics
             performance_metrics = {}
@@ -308,9 +294,7 @@ class DiagnosticsAPI:
         last_heartbeat_timestamp = agent_info.get("last_heartbeat")
         last_heartbeat = None
         if last_heartbeat_timestamp:
-            last_heartbeat = datetime.fromtimestamp(
-                last_heartbeat_timestamp, tz=timezone.utc
-            )
+            last_heartbeat = datetime.fromtimestamp(last_heartbeat_timestamp, tz=UTC)
 
         # Determine health status
         current_time = time.time()
@@ -379,12 +363,8 @@ class DiagnosticsAPI:
             performance_summary = monitor.get_performance_summary()
             performance_metrics.update(
                 {
-                    "system_performance": performance_summary.get(
-                        "overall_performance", "unknown"
-                    ),
-                    "active_operations": performance_summary.get(
-                        "active_operations", 0
-                    ),
+                    "system_performance": performance_summary.get("overall_performance", "unknown"),
+                    "active_operations": performance_summary.get("active_operations", 0),
                 }
             )
 
@@ -446,7 +426,7 @@ class DiagnosticsAPI:
                 overall_health = "degraded"
 
             return SystemDiagnosticSummary(
-                timestamp=datetime.now(tz=timezone.utc),
+                timestamp=datetime.now(tz=UTC),
                 total_agents=total_agents,
                 healthy_agents=health_counts["healthy"],
                 degraded_agents=health_counts["degraded"],
@@ -459,9 +439,7 @@ class DiagnosticsAPI:
 
         except Exception as e:
             logger.error(f"Failed to get system summary: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to retrieve system summary"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to retrieve system summary") from e
 
     async def _get_system_health(self) -> dict[str, Any]:
         """Get simple system health check."""
@@ -480,6 +458,6 @@ class DiagnosticsAPI:
             logger.error(f"Failed to get system health: {e}")
             return {
                 "status": "error",
-                "timestamp": datetime.now(tz=timezone.utc).isoformat(),
+                "timestamp": datetime.now(tz=UTC).isoformat(),
                 "message": f"Health check failed: {str(e)}",
             }
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/capabilities/auto_discovery.py b/packages/tta-ai-framework/src/tta_ai/orchestration/capabilities/auto_discovery.py
index ed657d6b1..85bfc4cde 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/capabilities/auto_discovery.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/capabilities/auto_discovery.py
@@ -89,9 +89,7 @@ class ComponentInfo:
 class AutoDiscoveryManager:
     """Manages auto-discovery of agent capabilities during startup."""
 
-    def __init__(
-        self, registry: RedisAgentRegistry, config: DiscoveryConfig | None = None
-    ):
+    def __init__(self, registry: RedisAgentRegistry, config: DiscoveryConfig | None = None):
         self.registry = registry
         self.config = config or DiscoveryConfig()
 
@@ -112,9 +110,7 @@ class AutoDiscoveryManager:
             self.config.enabled = False
             logger.info(f"Auto-discovery disabled for environment: {self.environment}")
 
-        logger.info(
-            f"AutoDiscoveryManager initialized (enabled: {self.config.enabled})"
-        )
+        logger.info(f"AutoDiscoveryManager initialized (enabled: {self.config.enabled})")
 
     def _detect_environment(self) -> str:
         """Detect current environment."""
@@ -246,9 +242,7 @@ class AutoDiscoveryManager:
                 and not await self._validate_component_capabilities(component)
             ):
                 component.discovery_status = DiscoveryStatus.FAILED
-                logger.warning(
-                    f"Component capability validation failed: {component_id}"
-                )
+                logger.warning(f"Component capability validation failed: {component_id}")
                 return False
 
             # Register with registry
@@ -256,9 +250,7 @@ class AutoDiscoveryManager:
 
             if success:
                 component.discovery_status = DiscoveryStatus.REGISTERED
-                logger.info(
-                    f"Component successfully discovered and registered: {component_id}"
-                )
+                logger.info(f"Component successfully discovered and registered: {component_id}")
 
                 # Notify callbacks
                 await self._notify_discovery_callbacks(component, "registered")
@@ -276,29 +268,21 @@ class AutoDiscoveryManager:
         """Validate component capabilities."""
         if not component.capabilities and component.agent_type:
             # If no capabilities specified, try to infer from agent type
-            component.capabilities = self._infer_capabilities_from_agent_type(
-                component.agent_type
-            )
+            component.capabilities = self._infer_capabilities_from_agent_type(component.agent_type)
 
         # Basic validation - ensure capabilities are properly formed
         for capability in component.capabilities:
             if not isinstance(capability, AgentCapability):
-                logger.warning(
-                    f"Invalid capability type for component {component.component_id}"
-                )
+                logger.warning(f"Invalid capability type for component {component.component_id}")
                 return False
 
             if not capability.capability_type or not capability.name:
-                logger.warning(
-                    f"Incomplete capability for component {component.component_id}"
-                )
+                logger.warning(f"Incomplete capability for component {component.component_id}")
                 return False
 
         return True
 
-    def _infer_capabilities_from_agent_type(
-        self, agent_type: AgentType
-    ) -> list[AgentCapability]:
+    def _infer_capabilities_from_agent_type(self, agent_type: AgentType) -> list[AgentCapability]:
         """Infer capabilities from agent type."""
         capability_mappings = {
             AgentType.INPUT_PROCESSOR: [
@@ -447,20 +431,14 @@ class AutoDiscoveryManager:
         """Send heartbeat for a component."""
         try:
             if component.agent_type:
-                agent_id = AgentId(
-                    agent_type=component.agent_type, instance=component.component_id
-                )
+                agent_id = AgentId(agent_type=component.agent_type, instance=component.component_id)
 
                 await self.registry.update_heartbeat(agent_id)
 
         except Exception as e:
-            logger.error(
-                f"Failed to send heartbeat for component {component.component_id}: {e}"
-            )
+            logger.error(f"Failed to send heartbeat for component {component.component_id}: {e}")
 
-    async def _notify_discovery_callbacks(
-        self, component: ComponentInfo, event: str
-    ) -> None:
+    async def _notify_discovery_callbacks(self, component: ComponentInfo, event: str) -> None:
         """Notify discovery callbacks."""
         for callback in self.discovery_callbacks:
             try:
@@ -491,9 +469,7 @@ class AutoDiscoveryManager:
             "strategy": self.config.strategy.value,
             "total_components": len(self.components),
             "status_counts": status_counts,
-            "discovery_attempts": sum(
-                c.discovery_attempts for c in self.components.values()
-            ),
+            "discovery_attempts": sum(c.discovery_attempts for c in self.components.values()),
             "is_running": self.is_running,
         }
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/capability_matcher.py b/packages/tta-ai-framework/src/tta_ai/orchestration/capability_matcher.py
index 3b484ef4d..c3ed93ff4 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/capability_matcher.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/capability_matcher.py
@@ -36,9 +36,7 @@ class CapabilityMatcher:
     Advanced capability matching engine with multiple algorithms.
     """
 
-    def __init__(
-        self, default_strategy: MatchingStrategy = MatchingStrategy.WEIGHTED_SCORE
-    ):
+    def __init__(self, default_strategy: MatchingStrategy = MatchingStrategy.WEIGHTED_SCORE):
         self.default_strategy = default_strategy
         self._strategy_weights = {
             "name_match": 0.4,
@@ -113,9 +111,7 @@ class CapabilityMatcher:
                         match_score=1.0,
                         exact_match=True,
                         version_match=True,
-                        performance_match=self._check_performance_match(
-                            capability, criteria
-                        ),
+                        performance_match=self._check_performance_match(capability, criteria),
                         agent_load_factor=cap_set.load_factor,
                         agent_availability=cap_set.availability,
                     )
@@ -136,9 +132,7 @@ class CapabilityMatcher:
                 continue
 
             for capability in cap_set.get_active_capabilities():
-                score_components = self._calculate_score_components(
-                    capability, criteria
-                )
+                score_components = self._calculate_score_components(capability, criteria)
 
                 if score_components["total_score"] > 0:
                     final_score = self._calculate_weighted_score(score_components)
@@ -179,9 +173,7 @@ class CapabilityMatcher:
                         match_score=fuzzy_score,
                         exact_match=False,
                         version_match=self._check_version_match(capability, criteria),
-                        performance_match=self._check_performance_match(
-                            capability, criteria
-                        ),
+                        performance_match=self._check_performance_match(capability, criteria),
                         agent_load_factor=cap_set.load_factor,
                         agent_availability=cap_set.availability,
                     )
@@ -204,9 +196,7 @@ class CapabilityMatcher:
             for capability in cap_set.get_active_capabilities():
                 if self._meets_basic_criteria(capability, criteria):
                     # Score based on agent priority and load factor
-                    priority_score = (cap_set.priority / 10.0) * (
-                        1.0 - cap_set.load_factor
-                    )
+                    priority_score = (cap_set.priority / 10.0) * (1.0 - cap_set.load_factor)
 
                     match_result = CapabilityMatchResult(
                         agent_id=cap_set.agent_id,
@@ -214,9 +204,7 @@ class CapabilityMatcher:
                         match_score=priority_score,
                         exact_match=self._is_exact_match(capability, criteria),
                         version_match=self._check_version_match(capability, criteria),
-                        performance_match=self._check_performance_match(
-                            capability, criteria
-                        ),
+                        performance_match=self._check_performance_match(capability, criteria),
                         agent_load_factor=cap_set.load_factor,
                         agent_availability=cap_set.availability,
                     )
@@ -246,9 +234,7 @@ class CapabilityMatcher:
                         match_score=semantic_score,
                         exact_match=False,
                         version_match=self._check_version_match(capability, criteria),
-                        performance_match=self._check_performance_match(
-                            capability, criteria
-                        ),
+                        performance_match=self._check_performance_match(capability, criteria),
                         agent_load_factor=cap_set.load_factor,
                         agent_availability=cap_set.availability,
                     )
@@ -303,9 +289,7 @@ class CapabilityMatcher:
         try:
             if criteria.min_version and capability.version < criteria.min_version:
                 return False
-            return not (
-                criteria.max_version and capability.version > criteria.max_version
-            )
+            return not (criteria.max_version and capability.version > criteria.max_version)
         except Exception:
             return True  # If version comparison fails, assume compatible
 
@@ -355,10 +339,7 @@ class CapabilityMatcher:
         # Version matching
         components["version_match"] = self._check_version_match(capability, criteria)
         if components["version_match"]:
-            if (
-                criteria.preferred_version
-                and capability.version == criteria.preferred_version
-            ):
+            if criteria.preferred_version and capability.version == criteria.preferred_version:
                 components["version_score"] = 1.0
             else:
                 components["version_score"] = 0.8
@@ -366,15 +347,11 @@ class CapabilityMatcher:
             return components  # Version mismatch, skip this capability
 
         # Performance matching
-        components["performance_match"] = self._check_performance_match(
-            capability, criteria
-        )
+        components["performance_match"] = self._check_performance_match(capability, criteria)
         if components["performance_match"]:
             components["performance_score"] = 1.0
         else:
-            components["performance_score"] = (
-                0.5  # Partial score for performance issues
-            )
+            components["performance_score"] = 0.5  # Partial score for performance issues
 
         # Check for exact match
         components["exact_match"] = (
@@ -401,8 +378,7 @@ class CapabilityMatcher:
             components["name_score"] * self._strategy_weights["name_match"]
             + components["type_score"] * self._strategy_weights["type_match"]
             + components["version_score"] * self._strategy_weights["version_match"]
-            + components["performance_score"]
-            * self._strategy_weights["performance_match"]
+            + components["performance_score"] * self._strategy_weights["performance_match"]
         )
         return min(1.0, weighted_score)
 
@@ -414,9 +390,7 @@ class CapabilityMatcher:
             return 0.5  # No name criteria, moderate score
 
         # Simple fuzzy matching using common subsequences
-        name_similarity = self._string_similarity(
-            capability.name, criteria.capability_name
-        )
+        name_similarity = self._string_similarity(capability.name, criteria.capability_name)
 
         # Boost score if type matches
         type_boost = 0.0
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker.py b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker.py
index 0d8d729f3..d2625e53e 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker.py
@@ -107,19 +107,13 @@ class CircuitBreaker:
         try:
             data = await self._redis.get(self._state_key())
             if data:
-                state_data = json.loads(
-                    data if isinstance(data, str) else data.decode()
-                )
+                state_data = json.loads(data if isinstance(data, str) else data.decode())
                 self._state = CircuitBreakerState(state_data.get("state", "closed"))
                 self._failure_count = int(state_data.get("failure_count", 0))
                 self._half_open_calls = int(state_data.get("half_open_calls", 0))
-                self._half_open_successes = int(
-                    state_data.get("half_open_successes", 0)
-                )
+                self._half_open_successes = int(state_data.get("half_open_successes", 0))
                 self._last_failure_time = state_data.get("last_failure_time")
-                self._state_changed_at = float(
-                    state_data.get("state_changed_at", time.time())
-                )
+                self._state_changed_at = float(state_data.get("state_changed_at", time.time()))
         except Exception as e:
             logger.debug(f"Failed to load circuit breaker state for {self._name}: {e}")
 
@@ -138,18 +132,14 @@ class CircuitBreaker:
             # TTL of 24 hours for state cleanup
             await self._redis.setex(self._state_key(), 86400, json.dumps(state_data))
         except Exception as e:
-            logger.warning(
-                f"Failed to persist circuit breaker state for {self._name}: {e}"
-            )
+            logger.warning(f"Failed to persist circuit breaker state for {self._name}: {e}")
 
     async def _load_metrics(self) -> None:
         """Load metrics from Redis."""
         try:
             data = await self._redis.get(self._metrics_key())
             if data:
-                metrics_data = json.loads(
-                    data if isinstance(data, str) else data.decode()
-                )
+                metrics_data = json.loads(data if isinstance(data, str) else data.decode())
                 self._metrics = CircuitBreakerMetrics(
                     total_calls=int(metrics_data.get("total_calls", 0)),
                     failed_calls=int(metrics_data.get("failed_calls", 0)),
@@ -159,9 +149,7 @@ class CircuitBreaker:
                     last_success_time=metrics_data.get("last_success_time"),
                 )
         except Exception as e:
-            logger.debug(
-                f"Failed to load circuit breaker metrics for {self._name}: {e}"
-            )
+            logger.debug(f"Failed to load circuit breaker metrics for {self._name}: {e}")
 
     async def _persist_metrics(self) -> None:
         """Persist metrics to Redis with TTL."""
@@ -176,13 +164,9 @@ class CircuitBreaker:
                 "updated_at": time.time(),
             }
             # TTL of 7 days for metrics
-            await self._redis.setex(
-                self._metrics_key(), 604800, json.dumps(metrics_data)
-            )
+            await self._redis.setex(self._metrics_key(), 604800, json.dumps(metrics_data))
         except Exception as e:
-            logger.warning(
-                f"Failed to persist circuit breaker metrics for {self._name}: {e}"
-            )
+            logger.warning(f"Failed to persist circuit breaker metrics for {self._name}: {e}")
 
     # ---- State management ----
     async def _transition_to_open(self) -> None:
@@ -196,9 +180,7 @@ class CircuitBreaker:
             # Record metrics and log transition
             from .circuit_breaker_metrics import record_state_transition
 
-            record_state_transition(
-                self._name, old_state, self._state, self._correlation_id
-            )
+            record_state_transition(self._name, old_state, self._state, self._correlation_id)
 
             logger.warning(
                 f"Circuit breaker {self._name} transitioning to OPEN state",
@@ -228,9 +210,7 @@ class CircuitBreaker:
             # Record metrics and log transition
             from .circuit_breaker_metrics import record_state_transition
 
-            record_state_transition(
-                self._name, old_state, self._state, self._correlation_id
-            )
+            record_state_transition(self._name, old_state, self._state, self._correlation_id)
 
             logger.info(
                 f"Circuit breaker {self._name} transitioning to HALF_OPEN state",
@@ -260,9 +240,7 @@ class CircuitBreaker:
             # Record metrics and log transition
             from .circuit_breaker_metrics import record_state_transition
 
-            record_state_transition(
-                self._name, old_state, self._state, self._correlation_id
-            )
+            record_state_transition(self._name, old_state, self._state, self._correlation_id)
 
             logger.info(
                 f"Circuit breaker {self._name} transitioning to CLOSED state",
@@ -321,9 +299,7 @@ class CircuitBreaker:
                     metrics_collector.record_call_rejected(
                         self._name, "circuit_breaker_open", self._correlation_id
                     )
-                    raise CircuitBreakerOpenError(
-                        f"Circuit breaker {self._name} is OPEN"
-                    )
+                    raise CircuitBreakerOpenError(f"Circuit breaker {self._name} is OPEN")
 
             # Check if we should limit calls in HALF_OPEN state
             if self._state == CircuitBreakerState.HALF_OPEN:
@@ -347,9 +323,7 @@ class CircuitBreaker:
             result = await func()
             duration_ms = (time.time() - start_time) * 1000
             await self._record_success()
-            metrics_collector.record_successful_call(
-                self._name, duration_ms, self._correlation_id
-            )
+            metrics_collector.record_successful_call(self._name, duration_ms, self._correlation_id)
             return result
         except Exception as e:
             duration_ms = (time.time() - start_time) * 1000
@@ -423,9 +397,7 @@ class CircuitBreaker:
             if self._state == CircuitBreakerState.CLOSED:
                 return True
             if self._state == CircuitBreakerState.OPEN:
-                return (
-                    time.time() - self._state_changed_at >= self._config.timeout_seconds
-                )
+                return time.time() - self._state_changed_at >= self._config.timeout_seconds
             if self._state == CircuitBreakerState.HALF_OPEN:
                 return self._half_open_calls < self._config.half_open_max_calls
             return False
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_config.py b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_config.py
index 43d4f7d3c..a7b889288 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_config.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_config.py
@@ -22,9 +22,7 @@ logger = logging.getLogger(__name__)
 class CircuitBreakerConfigSchema(BaseModel):
     """Pydantic schema for circuit breaker configuration validation."""
 
-    enabled: bool = Field(
-        default=True, description="Enable circuit breaker functionality"
-    )
+    enabled: bool = Field(default=True, description="Enable circuit breaker functionality")
     failure_threshold: int = Field(
         default=5, ge=1, le=100, description="Number of failures before opening circuit"
     )
@@ -73,12 +71,8 @@ class WorkflowErrorHandlingConfigSchema(BaseModel):
     """Schema for workflow error handling configuration."""
 
     enabled: bool = Field(default=True, description="Enable workflow error handling")
-    timeout_seconds: int = Field(
-        default=300, ge=1, le=3600, description="Default workflow timeout"
-    )
-    step_timeout_seconds: int = Field(
-        default=60, ge=1, le=600, description="Default step timeout"
-    )
+    timeout_seconds: int = Field(default=300, ge=1, le=3600, description="Default workflow timeout")
+    step_timeout_seconds: int = Field(default=60, ge=1, le=600, description="Default step timeout")
     rollback_retention_days: int = Field(
         default=30, ge=1, le=365, description="Rollback history retention"
     )
@@ -158,28 +152,20 @@ class CircuitBreakerConfigManager:
             ).lower() in ("true", "1", "yes")
 
         if os.getenv("TTA_WORKFLOW_TIMEOUT_SECONDS"):
-            env_config["timeout_seconds"] = int(
-                os.getenv("TTA_WORKFLOW_TIMEOUT_SECONDS")
-            )
+            env_config["timeout_seconds"] = int(os.getenv("TTA_WORKFLOW_TIMEOUT_SECONDS"))
 
         if os.getenv("TTA_WORKFLOW_STEP_TIMEOUT_SECONDS"):
-            env_config["step_timeout_seconds"] = int(
-                os.getenv("TTA_WORKFLOW_STEP_TIMEOUT_SECONDS")
-            )
+            env_config["step_timeout_seconds"] = int(os.getenv("TTA_WORKFLOW_STEP_TIMEOUT_SECONDS"))
 
         # Resource monitoring settings
         if os.getenv("TTA_RESOURCE_MEMORY_THRESHOLD"):
-            env_config["resource_monitoring"] = env_config.get(
-                "resource_monitoring", {}
-            )
+            env_config["resource_monitoring"] = env_config.get("resource_monitoring", {})
             env_config["resource_monitoring"]["memory_threshold_percent"] = int(
                 os.getenv("TTA_RESOURCE_MEMORY_THRESHOLD")
             )
 
         if os.getenv("TTA_RESOURCE_CPU_THRESHOLD"):
-            env_config["resource_monitoring"] = env_config.get(
-                "resource_monitoring", {}
-            )
+            env_config["resource_monitoring"] = env_config.get("resource_monitoring", {})
             env_config["resource_monitoring"]["cpu_threshold_percent"] = int(
                 os.getenv("TTA_RESOURCE_CPU_THRESHOLD")
             )
@@ -190,9 +176,7 @@ class CircuitBreakerConfigManager:
     def _validate_config(self) -> None:
         """Validate configuration using Pydantic schema."""
         try:
-            self.validated_config = WorkflowErrorHandlingConfigSchema(
-                **self.config_data
-            )
+            self.validated_config = WorkflowErrorHandlingConfigSchema(**self.config_data)
             logger.info("Circuit breaker configuration validated successfully")
         except Exception as e:
             logger.error(f"Circuit breaker configuration validation failed: {e}")
@@ -200,9 +184,7 @@ class CircuitBreakerConfigManager:
             self.validated_config = WorkflowErrorHandlingConfigSchema()
             logger.info("Using default circuit breaker configuration")
 
-    def get_circuit_breaker_config(
-        self, name: str | None = None
-    ) -> CircuitBreakerConfig:
+    def get_circuit_breaker_config(self, name: str | None = None) -> CircuitBreakerConfig:
         """Get CircuitBreakerConfig instance for a specific circuit breaker."""
         if not self.validated_config:
             self._validate_config()
@@ -221,10 +203,7 @@ class CircuitBreakerConfigManager:
         """Check if circuit breaker functionality is enabled."""
         if not self.validated_config:
             self._validate_config()
-        return (
-            self.validated_config.enabled
-            and self.validated_config.circuit_breaker.enabled
-        )
+        return self.validated_config.enabled and self.validated_config.circuit_breaker.enabled
 
     def get_workflow_timeout_config(self) -> dict[str, int]:
         """Get workflow timeout configuration."""
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_metrics.py b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_metrics.py
index 300ecec3d..788e832bb 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_metrics.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_metrics.py
@@ -47,9 +47,7 @@ class CircuitBreakerMetricsCollector:
     ) -> None:
         """Record a circuit breaker state transition."""
         transition_key = f"{from_state.value}_to_{to_state.value}"
-        self.state_transitions[transition_key] = (
-            self.state_transitions.get(transition_key, 0) + 1
-        )
+        self.state_transitions[transition_key] = self.state_transitions.get(transition_key, 0) + 1
         self.last_update = time.time()
 
         # Structured logging with correlation ID
@@ -358,6 +356,4 @@ def record_degraded_mode_activation(
     circuit_breaker_name: str, reason: str, correlation_id: str | None = None
 ) -> None:
     """Convenience function to record degraded mode activation."""
-    _global_logger.log_degraded_mode_activation(
-        circuit_breaker_name, reason, correlation_id
-    )
+    _global_logger.log_degraded_mode_activation(circuit_breaker_name, reason, correlation_id)
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_registry.py b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_registry.py
index 4fd1f1dff..83eea5c3f 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_registry.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/circuit_breaker_registry.py
@@ -96,9 +96,7 @@ class CircuitBreakerRegistry:
         """List all registered circuit breaker names."""
         try:
             members = await self._redis.smembers(self._registry_key())
-            return [
-                m.decode() if isinstance(m, (bytes, bytearray)) else m for m in members
-            ]
+            return [m.decode() if isinstance(m, (bytes, bytearray)) else m for m in members]
         except Exception as e:
             logger.warning(f"Failed to list circuit breaker names: {e}")
             return []
@@ -111,9 +109,7 @@ class CircuitBreakerRegistry:
                 try:
                     metrics[name] = await cb.get_metrics()
                 except Exception as e:
-                    logger.warning(
-                        f"Failed to get metrics for circuit breaker {name}: {e}"
-                    )
+                    logger.warning(f"Failed to get metrics for circuit breaker {name}: {e}")
                     metrics[name] = {"error": str(e)}
         return metrics
 
@@ -125,9 +121,7 @@ class CircuitBreakerRegistry:
                 try:
                     states[name] = await cb.get_state()
                 except Exception as e:
-                    logger.warning(
-                        f"Failed to get state for circuit breaker {name}: {e}"
-                    )
+                    logger.warning(f"Failed to get state for circuit breaker {name}: {e}")
                     states[name] = CircuitBreakerState.CLOSED  # Default fallback
         return states
 
@@ -186,13 +180,9 @@ class CircuitBreakerRegistry:
                             # Clean up if older than 24 hours
                             if time.time() - updated_at > 86400:
                                 await self._redis.delete(key)
-                                await self._redis.delete(
-                                    f"{self._pfx}:cb:metrics:{name}"
-                                )
+                                await self._redis.delete(f"{self._pfx}:cb:metrics:{name}")
                                 cleaned_count += 1
-                                logger.debug(
-                                    f"Cleaned up expired circuit breaker state: {name}"
-                                )
+                                logger.debug(f"Cleaned up expired circuit breaker state: {name}")
                     except Exception as e:
                         logger.warning(f"Failed to process state key {key}: {e}")
 
@@ -250,9 +240,7 @@ class CircuitBreakerRegistry:
             # Initialize circuit breakers that exist in Redis but not in memory
             for name in redis_names - memory_names:
                 try:
-                    cb = CircuitBreaker(
-                        redis=self._redis, name=name, key_prefix=self._pfx
-                    )
+                    cb = CircuitBreaker(redis=self._redis, name=name, key_prefix=self._pfx)
                     await cb.initialize()
                     self._circuit_breakers[name] = cb
                     logger.debug(f"Loaded circuit breaker from Redis: {name}")
@@ -268,9 +256,7 @@ class CircuitBreakerRegistry:
             states = await self.get_all_states()
             state_counts = {}
             for state in CircuitBreakerState:
-                state_counts[state.value] = sum(
-                    1 for s in states.values() if s == state
-                )
+                state_counts[state.value] = sum(1 for s in states.values() if s == state)
 
             # Get last cleanup time
             last_cleanup = await self._redis.get(self._cleanup_key())
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/config/real_agent_config.py b/packages/tta-ai-framework/src/tta_ai/orchestration/config/real_agent_config.py
index be079d081..d3fc6dc74 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/config/real_agent_config.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/config/real_agent_config.py
@@ -76,18 +76,10 @@ class RealAgentConfig:
             enable_monitoring=_get_env_bool("TTA_ENABLE_MONITORING", True),
             enable_profiling=_get_env_bool("TTA_ENABLE_PROFILING", False),
             enable_alerts=_get_env_bool("TTA_ENABLE_ALERTS", True),
-            high_memory_threshold_mb=_get_env_float(
-                "TTA_HIGH_MEMORY_THRESHOLD_MB", 1500.0
-            ),
-            high_queue_depth_threshold=_get_env_int(
-                "TTA_HIGH_QUEUE_DEPTH_THRESHOLD", 100
-            ),
-            high_error_rate_threshold=_get_env_float(
-                "TTA_HIGH_ERROR_RATE_THRESHOLD", 0.2
-            ),
-            slow_response_time_threshold_s=_get_env_float(
-                "TTA_SLOW_RESPONSE_TIME_THRESHOLD", 10.0
-            ),
+            high_memory_threshold_mb=_get_env_float("TTA_HIGH_MEMORY_THRESHOLD_MB", 1500.0),
+            high_queue_depth_threshold=_get_env_int("TTA_HIGH_QUEUE_DEPTH_THRESHOLD", 100),
+            high_error_rate_threshold=_get_env_float("TTA_HIGH_ERROR_RATE_THRESHOLD", 0.2),
+            slow_response_time_threshold_s=_get_env_float("TTA_SLOW_RESPONSE_TIME_THRESHOLD", 10.0),
         )
 
     def to_dict(self) -> dict[str, Any]:
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/config_schema.py b/packages/tta-ai-framework/src/tta_ai/orchestration/config_schema.py
index 7a1de5a0d..8495c93df 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/config_schema.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/config_schema.py
@@ -49,9 +49,7 @@ class CapabilityMatchingConfig(BaseModel):
     score_threshold: float = Field(
         default=0.5, ge=0.0, le=1.0, description="Minimum match score threshold"
     )
-    prefer_exact_version: bool = Field(
-        default=True, description="Prefer exact version matches"
-    )
+    prefer_exact_version: bool = Field(default=True, description="Prefer exact version matches")
     include_deprecated: bool = Field(
         default=False, description="Include deprecated capabilities in search results"
     )
@@ -60,9 +58,7 @@ class CapabilityMatchingConfig(BaseModel):
 class AgentCapabilityConfig(BaseModel):
     """Configuration for agent capability advertisement."""
 
-    advertise: bool = Field(
-        default=True, description="Advertise capabilities for discovery"
-    )
+    advertise: bool = Field(default=True, description="Advertise capabilities for discovery")
     version: str = Field(default="1.0.0", description="Capability set version")
 
     @validator("version")
@@ -81,12 +77,8 @@ class AgentConfig(BaseModel):
     """Configuration for individual agent types."""
 
     enabled: bool = Field(default=True, description="Enable this agent type")
-    max_instances: int = Field(
-        default=1, ge=1, le=100, description="Maximum number of instances"
-    )
-    timeout: int = Field(
-        default=30, ge=1, le=300, description="Agent timeout in seconds"
-    )
+    max_instances: int = Field(default=1, ge=1, le=100, description="Maximum number of instances")
+    timeout: int = Field(default=30, ge=1, le=300, description="Agent timeout in seconds")
 
     # Auto-registration settings
     auto_register_enabled: bool = Field(
@@ -154,9 +146,7 @@ class AgentsConfig(BaseModel):
         if v is not None:
             ttl = values.get("heartbeat_ttl", 30.0)
             if v <= 0 or v >= ttl:
-                raise ValueError(
-                    "Heartbeat interval must be positive and less than heartbeat_ttl"
-                )
+                raise ValueError("Heartbeat interval must be positive and less than heartbeat_ttl")
         return v
 
     def get_effective_heartbeat_interval(self) -> float:
@@ -194,9 +184,7 @@ class AgentOrchestrationConfig(BaseModel):
     )
 
     # Agent configuration
-    agents: AgentsConfig = Field(
-        default_factory=AgentsConfig, description="Agent configuration"
-    )
+    agents: AgentsConfig = Field(default_factory=AgentsConfig, description="Agent configuration")
 
     # Other existing configurations (simplified for now)
     resources: dict[str, Any] = Field(
@@ -208,9 +196,7 @@ class AgentOrchestrationConfig(BaseModel):
     monitoring: dict[str, Any] = Field(
         default_factory=dict, description="Performance monitoring settings"
     )
-    diagnostics: dict[str, Any] = Field(
-        default_factory=dict, description="Diagnostics settings"
-    )
+    diagnostics: dict[str, Any] = Field(default_factory=dict, description="Diagnostics settings")
     realtime: dict[str, Any] = Field(
         default_factory=dict, description="Real-time interaction settings"
     )
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/coordinators/redis_message_coordinator.py b/packages/tta-ai-framework/src/tta_ai/orchestration/coordinators/redis_message_coordinator.py
index 0f0d633e3..cdede8fe2 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/coordinators/redis_message_coordinator.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/coordinators/redis_message_coordinator.py
@@ -24,7 +24,7 @@ import json
 import logging
 import time
 import uuid
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 from redis.asyncio import Redis
 
@@ -46,7 +46,7 @@ def _now_us() -> int:
 
 
 def _iso_now() -> str:
-    return datetime.now(timezone.utc).isoformat()
+    return datetime.now(UTC).isoformat()
 
 
 class RedisMessageCoordinator(MessageCoordinator):
@@ -129,9 +129,7 @@ class RedisMessageCoordinator(MessageCoordinator):
         except Exception as e:
             with contextlib.suppress(Exception):
                 self.metrics.inc_delivered_error(1)
-            return MessageResult(
-                message_id=message.message_id, delivered=False, error=str(e)
-            )
+            return MessageResult(message_id=message.message_id, delivered=False, error=str(e))
 
     async def broadcast_message(
         self, sender: AgentId, message: AgentMessage, recipients: list[AgentId]
@@ -169,32 +167,26 @@ class RedisMessageCoordinator(MessageCoordinator):
         for prio in (9, 5, 1):
             skey = self._sched_key(agent_id, prio)
             # Find one due item (score <= now)
-            members = await self._redis.zrangebyscore(
-                skey, min=-1, max=now, start=0, num=1
-            )
+            members = await self._redis.zrangebyscore(skey, min=-1, max=now, start=0, num=1)
             if not members:
                 continue
             member = members[0]
             # Remove it from sched set
             await self._redis.zrem(skey, member)
-            payload = (
-                member.decode() if isinstance(member, (bytes, bytearray)) else member
-            )
+            payload = member.decode() if isinstance(member, (bytes, bytearray)) else member
             # Create reservation
             token = f"res_{uuid.uuid4().hex[:16]}"
             deadline = now + int(visibility_timeout * 1_000_000)
             # Store reservation
             await self._redis.hset(self._reserved_hash(agent_id), token, payload)
-            await self._redis.zadd(
-                self._reserved_deadlines(agent_id), {token: deadline}
-            )
+            await self._redis.zadd(self._reserved_deadlines(agent_id), {token: deadline})
             # Return wrapper
             qmsg_dict = json.loads(payload)
             return ReceivedMessage(
                 token=token,
                 queue_message=QueueMessage(**qmsg_dict),
                 visibility_deadline=datetime.fromtimestamp(
-                    deadline / 1_000_000, tz=timezone.utc
+                    deadline / 1_000_000, tz=UTC
                 ).isoformat(),
             )
         return None
@@ -235,10 +227,7 @@ class RedisMessageCoordinator(MessageCoordinator):
             await self._redis.rpush(self._queue_key(agent_id), updated)
 
             # DLQ on permanent failure or when attempts exceed allowed retries
-            if (
-                failure == FailureType.PERMANENT
-                or qm.delivery_attempts > self._retry_attempts
-            ):
+            if failure == FailureType.PERMANENT or qm.delivery_attempts > self._retry_attempts:
                 await self._redis.rpush(self._dlq_key(agent_id), updated)
                 with contextlib.suppress(Exception):
                     self.metrics.inc_permanent(1)
@@ -258,14 +247,11 @@ class RedisMessageCoordinator(MessageCoordinator):
 
             # Schedule retry with exponential backoff
             delay = min(
-                self._backoff_base
-                * (self._backoff_factor ** (qm.delivery_attempts - 1)),
+                self._backoff_base * (self._backoff_factor ** (qm.delivery_attempts - 1)),
                 self._backoff_max,
             )
             score = _now_us() + int(delay * 1_000_000)
-            await self._redis.zadd(
-                self._sched_key(agent_id, int(qm.priority)), {updated: score}
-            )
+            await self._redis.zadd(self._sched_key(agent_id, int(qm.priority)), {updated: score})
             with contextlib.suppress(Exception):
                 self.metrics.inc_retries_scheduled(1, last_backoff_seconds=delay)
                 self.metrics.inc_nacks(1)
@@ -294,9 +280,7 @@ class RedisMessageCoordinator(MessageCoordinator):
                     f"{self._pfx}:reserved:{at.value}:*",
                 ):
                     async for key in self._redis.scan_iter(match=pattern):
-                        parts = (
-                            key.decode() if isinstance(key, (bytes, bytearray)) else key
-                        )
+                        parts = key.decode() if isinstance(key, (bytes, bytearray)) else key
                         inst = parts.split(":")[-1]
                         sig = f"{at.value}:{inst}"
                         if sig in seen:
@@ -316,9 +300,7 @@ class RedisMessageCoordinator(MessageCoordinator):
                 payload = await self._redis.hget(self._reserved_hash(aid), token)
                 if payload:
                     try:
-                        pdata = (
-                            payload if isinstance(payload, str) else payload.decode()
-                        )
+                        pdata = payload if isinstance(payload, str) else payload.decode()
                         data = json.loads(pdata)
                         qm = QueueMessage(**data)
                         score = _now_us()
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/enhanced_coordinator.py b/packages/tta-ai-framework/src/tta_ai/orchestration/enhanced_coordinator.py
index 780329daf..45a5af23f 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/enhanced_coordinator.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/enhanced_coordinator.py
@@ -132,9 +132,7 @@ class EnhancedRedisMessageCoordinator(RedisMessageCoordinator):
             if self.fallback_to_mock:
                 logger.warning("Falling back to standard Redis message delivery")
                 return await super().send_message(sender, recipient, message)
-            return MessageResult(
-                message_id=message.message_id, delivered=False, error=str(e)
-            )
+            return MessageResult(message_id=message.message_id, delivered=False, error=str(e))
 
     async def send_message(
         self, sender: AgentId, recipient: AgentId, message: AgentMessage
@@ -159,9 +157,7 @@ class EnhancedRedisMessageCoordinator(RedisMessageCoordinator):
                 if self.fallback_to_mock:
                     logger.info("Falling back to standard Redis delivery")
                     return await super().send_message(sender, recipient, message)
-                return MessageResult(
-                    message_id=message.message_id, delivered=False, error=str(e)
-                )
+                return MessageResult(message_id=message.message_id, delivered=False, error=str(e))
 
         # Use standard Redis message delivery
         return await super().send_message(sender, recipient, message)
@@ -325,9 +321,7 @@ class BatchedMessageProcessor:
         self._processing_semaphore = asyncio.Semaphore(max_concurrent_batches)
         self._batch_timer: asyncio.Task | None = None
 
-    async def add_message(
-        self, message: dict[str, Any], processor_func: Callable
-    ) -> Any:
+    async def add_message(self, message: dict[str, Any], processor_func: Callable) -> Any:
         """Add message to batch for processing."""
         async with self._batch_lock:
             # Add message to pending batch
@@ -388,24 +382,18 @@ class BatchedMessageProcessor:
             for group in processor_groups.values():
                 await self._process_group(group["processor"], group["entries"])
 
-    async def _process_group(
-        self, processor_func: Callable, entries: list[dict[str, Any]]
-    ):
+    async def _process_group(self, processor_func: Callable, entries: list[dict[str, Any]]):
         """Process a group of messages with the same processor."""
         tasks = []
 
         for entry in entries:
-            task = asyncio.create_task(
-                self._process_single_message(processor_func, entry)
-            )
+            task = asyncio.create_task(self._process_single_message(processor_func, entry))
             tasks.append(task)
 
         # Wait for all messages in the group to complete
         await asyncio.gather(*tasks, return_exceptions=True)
 
-    async def _process_single_message(
-        self, processor_func: Callable, entry: dict[str, Any]
-    ):
+    async def _process_single_message(self, processor_func: Callable, entry: dict[str, Any]):
         """Process a single message and set its future result."""
         try:
             result = await processor_func(entry["message"])
@@ -469,9 +457,7 @@ class ScalableWorkflowCoordinator:
 
                 # Execute workflow with timeout
                 result = await asyncio.wait_for(
-                    self._execute_workflow_steps(
-                        workflow_id, workflow_steps, context or {}
-                    ),
+                    self._execute_workflow_steps(workflow_id, workflow_steps, context or {}),
                     timeout=self.workflow_timeout_s,
                 )
 
@@ -485,10 +471,8 @@ class ScalableWorkflowCoordinator:
 
                 return result
 
-            except asyncio.TimeoutError:
-                logger.error(
-                    f"Workflow {workflow_id} timed out after {self.workflow_timeout_s}s"
-                )
+            except TimeoutError:
+                logger.error(f"Workflow {workflow_id} timed out after {self.workflow_timeout_s}s")
                 self._system_monitor.end_workflow(workflow_id, success=False)
 
                 async with self._workflow_lock:
@@ -528,21 +512,13 @@ class ScalableWorkflowCoordinator:
 
             try:
                 if step_type == "ipa":
-                    result = await self._execute_ipa_step(
-                        step_config, accumulated_context
-                    )
+                    result = await self._execute_ipa_step(step_config, accumulated_context)
                 elif step_type == "wba":
-                    result = await self._execute_wba_step(
-                        step_config, accumulated_context
-                    )
+                    result = await self._execute_wba_step(step_config, accumulated_context)
                 elif step_type == "nga":
-                    result = await self._execute_nga_step(
-                        step_config, accumulated_context
-                    )
+                    result = await self._execute_nga_step(step_config, accumulated_context)
                 elif step_type == "parallel":
-                    result = await self._execute_parallel_steps(
-                        step_config, accumulated_context
-                    )
+                    result = await self._execute_parallel_steps(step_config, accumulated_context)
                 else:
                     raise ValueError(f"Unknown step type: {step_type}")
 
@@ -576,13 +552,9 @@ class ScalableWorkflowCoordinator:
         if self._batch_processor:
             # Use batched processing
             async def ipa_processor(message):
-                return await self.enhanced_coordinator.ipa_adapter.process_input(
-                    message["text"]
-                )
+                return await self.enhanced_coordinator.ipa_adapter.process_input(message["text"])
 
-            result = await self._batch_processor.add_message(
-                {"text": text}, ipa_processor
-            )
+            result = await self._batch_processor.add_message({"text": text}, ipa_processor)
         else:
             # Direct processing
             result = await self.enhanced_coordinator.ipa_adapter.process_input(text)
@@ -599,10 +571,8 @@ class ScalableWorkflowCoordinator:
         if self._batch_processor:
             # Use batched processing
             async def wba_processor(message):
-                return (
-                    await self.enhanced_coordinator.wba_adapter.process_world_request(
-                        message["world_id"], message.get("updates")
-                    )
+                return await self.enhanced_coordinator.wba_adapter.process_world_request(
+                    message["world_id"], message.get("updates")
                 )
 
             result = await self._batch_processor.add_message(
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_integration.py b/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_integration.py
index 798ca079c..8e41d5d5a 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_integration.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_integration.py
@@ -57,9 +57,7 @@ class LangGraphWorkflowBuilder:
         if definition.agent_sequence:
             first = definition.agent_sequence[0].agent.value
             sg.add_edge(START, first)
-            for a, b in zip(
-                definition.agent_sequence, definition.agent_sequence[1:], strict=False
-            ):
+            for a, b in zip(definition.agent_sequence, definition.agent_sequence[1:], strict=False):
                 sg.add_edge(a.agent.value, b.agent.value)
             sg.add_edge(definition.agent_sequence[-1].agent.value, END)
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_orchestrator.py b/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_orchestrator.py
index 8a47ad838..b5bb16826 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_orchestrator.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/langgraph_orchestrator.py
@@ -236,9 +236,7 @@ class LangGraphAgentOrchestrator:
                 "safety_level": "safe",
             }
 
-    async def _process_input_node(
-        self, state: AgentWorkflowState
-    ) -> AgentWorkflowState:
+    async def _process_input_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
         """Process input and perform initial validation."""
         logger.info(f"Processing input for session {state['session_id']}")
 
@@ -289,9 +287,7 @@ class LangGraphAgentOrchestrator:
 
         return state
 
-    async def _coordinate_agents_node(
-        self, state: AgentWorkflowState
-    ) -> AgentWorkflowState:
+    async def _coordinate_agents_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
         """Coordinate IPA, WBA, and NGA agents."""
         logger.info(f"Coordinating agents for session {state['session_id']}")
 
@@ -321,9 +317,7 @@ class LangGraphAgentOrchestrator:
 
         return state
 
-    async def _generate_response_node(
-        self, state: AgentWorkflowState
-    ) -> AgentWorkflowState:
+    async def _generate_response_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
         """Generate final response with therapeutic framing."""
         logger.info(f"Generating response for session {state['session_id']}")
 
@@ -366,9 +360,7 @@ class LangGraphAgentOrchestrator:
 
         except Exception as e:
             logger.error(f"Response generation error: {e}")
-            state["narrative_response"] = (
-                "Thank you for sharing. How would you like to proceed?"
-            )
+            state["narrative_response"] = "Thank you for sharing. How would you like to proceed?"
             state["messages"].append(AIMessage(content=state["narrative_response"]))
 
             # Record error
@@ -382,9 +374,7 @@ class LangGraphAgentOrchestrator:
 
         return state
 
-    async def _handle_crisis_node(
-        self, state: AgentWorkflowState
-    ) -> AgentWorkflowState:
+    async def _handle_crisis_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
         """Handle crisis situations with appropriate interventions."""
         logger.warning(f"Crisis intervention for session {state['session_id']}")
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/metrics.py b/packages/tta-ai-framework/src/tta_ai/orchestration/metrics.py
index 1e0187c55..db2a275ed 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/metrics.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/metrics.py
@@ -51,9 +51,7 @@ class MessageMetrics:
     def inc_permanent(self, n: int = 1) -> None:
         self.retry.total_permanent_failures += n
 
-    def inc_retries_scheduled(
-        self, n: int = 1, last_backoff_seconds: float = 0.0
-    ) -> None:
+    def inc_retries_scheduled(self, n: int = 1, last_backoff_seconds: float = 0.0) -> None:
         self.retry.total_retries_scheduled += n
         self.retry.last_backoff_seconds = last_backoff_seconds
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/models.py b/packages/tta-ai-framework/src/tta_ai/orchestration/models.py
index 2591de652..da47f2526 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/models.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/models.py
@@ -107,29 +107,19 @@ class AgentCapability(BaseModel):
 
     name: str = Field(..., description="Unique name of the capability")
     type: CapabilityType = Field(..., description="Type of capability")
-    version: str = Field(
-        ..., description="Semantic version of the capability (e.g., '1.0.0')"
-    )
+    version: str = Field(..., description="Semantic version of the capability (e.g., '1.0.0')")
     description: str | None = Field(None, description="Human-readable description")
 
     # Operational metadata
     scope: CapabilityScope = Field(
         default=CapabilityScope.SESSION, description="Scope of operation"
     )
-    status: CapabilityStatus = Field(
-        default=CapabilityStatus.ACTIVE, description="Current status"
-    )
+    status: CapabilityStatus = Field(default=CapabilityStatus.ACTIVE, description="Current status")
 
     # Capability requirements and constraints
-    required_inputs: set[str] = Field(
-        default_factory=set, description="Required input fields"
-    )
-    optional_inputs: set[str] = Field(
-        default_factory=set, description="Optional input fields"
-    )
-    output_schema: dict[str, Any] | None = Field(
-        None, description="JSON schema for outputs"
-    )
+    required_inputs: set[str] = Field(default_factory=set, description="Required input fields")
+    optional_inputs: set[str] = Field(default_factory=set, description="Optional input fields")
+    output_schema: dict[str, Any] | None = Field(None, description="JSON schema for outputs")
 
     # Performance and resource metadata
     estimated_duration_ms: int | None = Field(
@@ -143,15 +133,11 @@ class AgentCapability(BaseModel):
     compatible_versions: set[str] = Field(
         default_factory=set, description="Compatible capability versions"
     )
-    dependencies: set[str] = Field(
-        default_factory=set, description="Required dependencies"
-    )
+    dependencies: set[str] = Field(default_factory=set, description="Required dependencies")
 
     # Metadata
     tags: set[str] = Field(default_factory=set, description="Searchable tags")
-    metadata: dict[str, Any] = Field(
-        default_factory=dict, description="Additional metadata"
-    )
+    metadata: dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
 
     @validator("version")
     def validate_version(cls, v):
@@ -200,9 +186,7 @@ class AgentCapabilitySet(BaseModel):
         default=True, description="Whether agent is available for new requests"
     )
 
-    def get_capability(
-        self, name: str, version: str | None = None
-    ) -> AgentCapability | None:
+    def get_capability(self, name: str, version: str | None = None) -> AgentCapability | None:
         """Get a specific capability by name and optionally version."""
         for cap in self.capabilities:
             if cap.name == name:
@@ -214,39 +198,27 @@ class AgentCapabilitySet(BaseModel):
         """Check if agent has a specific capability."""
         return self.get_capability(name, version) is not None
 
-    def get_capabilities_by_type(
-        self, capability_type: CapabilityType
-    ) -> list[AgentCapability]:
+    def get_capabilities_by_type(self, capability_type: CapabilityType) -> list[AgentCapability]:
         """Get all capabilities of a specific type."""
         return [cap for cap in self.capabilities if cap.type == capability_type]
 
     def get_active_capabilities(self) -> list[AgentCapability]:
         """Get all active capabilities."""
-        return [
-            cap for cap in self.capabilities if cap.status == CapabilityStatus.ACTIVE
-        ]
+        return [cap for cap in self.capabilities if cap.status == CapabilityStatus.ACTIVE]
 
 
 class CapabilityMatchCriteria(BaseModel):
     """Criteria for matching agent capabilities."""
 
     # Basic matching criteria
-    capability_name: str | None = Field(
-        None, description="Specific capability name to match"
-    )
-    capability_type: CapabilityType | None = Field(
-        None, description="Type of capability required"
-    )
-    required_inputs: set[str] = Field(
-        default_factory=set, description="Required input fields"
-    )
+    capability_name: str | None = Field(None, description="Specific capability name to match")
+    capability_type: CapabilityType | None = Field(None, description="Type of capability required")
+    required_inputs: set[str] = Field(default_factory=set, description="Required input fields")
 
     # Version constraints
     min_version: str | None = Field(None, description="Minimum capability version")
     max_version: str | None = Field(None, description="Maximum capability version")
-    preferred_version: str | None = Field(
-        None, description="Preferred capability version"
-    )
+    preferred_version: str | None = Field(None, description="Preferred capability version")
 
     # Performance constraints
     max_duration_ms: int | None = Field(None, description="Maximum acceptable duration")
@@ -255,20 +227,12 @@ class CapabilityMatchCriteria(BaseModel):
     )
 
     # Availability constraints
-    require_available: bool = Field(
-        default=True, description="Require agent to be available"
-    )
-    max_load_factor: float = Field(
-        default=0.8, description="Maximum acceptable load factor"
-    )
+    require_available: bool = Field(default=True, description="Require agent to be available")
+    max_load_factor: float = Field(default=0.8, description="Maximum acceptable load factor")
 
     # Tags and metadata
-    required_tags: set[str] = Field(
-        default_factory=set, description="Required capability tags"
-    )
-    metadata_filters: dict[str, Any] = Field(
-        default_factory=dict, description="Metadata filters"
-    )
+    required_tags: set[str] = Field(default_factory=set, description="Required capability tags")
+    metadata_filters: dict[str, Any] = Field(default_factory=dict, description="Metadata filters")
 
 
 class CapabilityMatchResult(BaseModel):
@@ -279,35 +243,23 @@ class CapabilityMatchResult(BaseModel):
     match_score: float = Field(..., description="Match score (0.0-1.0)")
 
     # Match details
-    exact_match: bool = Field(
-        default=False, description="Whether this is an exact match"
-    )
-    version_match: bool = Field(
-        default=False, description="Whether version requirements are met"
-    )
+    exact_match: bool = Field(default=False, description="Whether this is an exact match")
+    version_match: bool = Field(default=False, description="Whether version requirements are met")
     performance_match: bool = Field(
         default=False, description="Whether performance requirements are met"
     )
 
     # Agent status at match time
-    agent_load_factor: float = Field(
-        default=0.0, description="Agent load factor at match time"
-    )
-    agent_availability: bool = Field(
-        default=True, description="Agent availability at match time"
-    )
-    estimated_wait_time_ms: int | None = Field(
-        None, description="Estimated wait time for agent"
-    )
+    agent_load_factor: float = Field(default=0.0, description="Agent load factor at match time")
+    agent_availability: bool = Field(default=True, description="Agent availability at match time")
+    estimated_wait_time_ms: int | None = Field(None, description="Estimated wait time for agent")
 
 
 class CapabilityDiscoveryRequest(BaseModel):
     """Request for discovering agents with specific capabilities."""
 
     criteria: CapabilityMatchCriteria = Field(..., description="Matching criteria")
-    max_results: int = Field(
-        default=10, description="Maximum number of results to return"
-    )
+    max_results: int = Field(default=10, description="Maximum number of results to return")
     include_degraded: bool = Field(
         default=False, description="Include agents with degraded performance"
     )
@@ -321,17 +273,11 @@ class CapabilityDiscoveryResponse(BaseModel):
     matches: list[CapabilityMatchResult] = Field(
         default_factory=list, description="Matched capabilities"
     )
-    total_agents_searched: int = Field(
-        default=0, description="Total number of agents searched"
-    )
-    search_duration_ms: int = Field(
-        default=0, description="Search duration in milliseconds"
-    )
+    total_agents_searched: int = Field(default=0, description="Total number of agents searched")
+    search_duration_ms: int = Field(default=0, description="Search duration in milliseconds")
 
     # Discovery metadata
     discovery_timestamp: datetime = Field(
         default_factory=datetime.utcnow, description="Discovery timestamp"
     )
-    cache_hit: bool = Field(
-        default=False, description="Whether results came from cache"
-    )
+    cache_hit: bool = Field(default=False, description="Whether results came from cache")
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/monitoring.py b/packages/tta-ai-framework/src/tta_ai/orchestration/monitoring.py
index c6320769c..189eec5c2 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/monitoring.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/monitoring.py
@@ -75,9 +75,7 @@ class AgentMonitor:
         self._lock = threading.Lock()
         self._start_time = time.time()
 
-    def record_request(
-        self, response_time: float, success: bool, error: str | None = None
-    ):
+    def record_request(self, response_time: float, success: bool, error: str | None = None):
         """Record a request and its outcome."""
         with self._lock:
             self.metrics.total_requests += 1
@@ -90,19 +88,13 @@ class AgentMonitor:
 
             # Update response time metrics
             self.metrics.total_response_time += response_time
-            self.metrics.min_response_time = min(
-                self.metrics.min_response_time, response_time
-            )
-            self.metrics.max_response_time = max(
-                self.metrics.max_response_time, response_time
-            )
+            self.metrics.min_response_time = min(self.metrics.min_response_time, response_time)
+            self.metrics.max_response_time = max(self.metrics.max_response_time, response_time)
             self.metrics.recent_response_times.append(response_time)
 
             # Calculate derived metrics
             if self.metrics.total_requests > 0:
-                self.metrics.error_rate = (
-                    self.metrics.failed_requests / self.metrics.total_requests
-                )
+                self.metrics.error_rate = self.metrics.failed_requests / self.metrics.total_requests
                 self.metrics.average_response_time = (
                     self.metrics.total_response_time / self.metrics.total_requests
                 )
@@ -110,9 +102,7 @@ class AgentMonitor:
                 # Calculate RPS over the monitoring period
                 elapsed_time = time.time() - self._start_time
                 if elapsed_time > 0:
-                    self.metrics.requests_per_second = (
-                        self.metrics.total_requests / elapsed_time
-                    )
+                    self.metrics.requests_per_second = self.metrics.total_requests / elapsed_time
 
     def get_current_metrics(self) -> AgentMetrics:
         """Get current metrics snapshot."""
@@ -137,9 +127,7 @@ class AgentMonitor:
                 recent_response_times=self.metrics.recent_response_times.copy(),
             )
 
-    async def health_check(
-        self, health_check_func: Callable | None = None
-    ) -> HealthStatus:
+    async def health_check(self, health_check_func: Callable | None = None) -> HealthStatus:
         """Perform health check for this agent."""
         start_time = time.time()
 
@@ -198,9 +186,7 @@ class SystemMonitor:
         if monitor_key not in self.agent_monitors:
             with self._lock:
                 if monitor_key not in self.agent_monitors:
-                    self.agent_monitors[monitor_key] = AgentMonitor(
-                        agent_type, agent_instance
-                    )
+                    self.agent_monitors[monitor_key] = AgentMonitor(agent_type, agent_instance)
 
         return self.agent_monitors[monitor_key]
 
@@ -221,9 +207,7 @@ class SystemMonitor:
             workflow_time = time.time() - start_time
 
             self.metrics.total_workflows += 1
-            self.metrics.concurrent_workflows = max(
-                0, self.metrics.concurrent_workflows - 1
-            )
+            self.metrics.concurrent_workflows = max(0, self.metrics.concurrent_workflows - 1)
 
             if success:
                 self.metrics.successful_workflows += 1
@@ -233,13 +217,10 @@ class SystemMonitor:
             # Update average workflow time
             if self.metrics.total_workflows > 0:
                 total_time = (
-                    self.metrics.average_workflow_time
-                    * (self.metrics.total_workflows - 1)
+                    self.metrics.average_workflow_time * (self.metrics.total_workflows - 1)
                     + workflow_time
                 )
-                self.metrics.average_workflow_time = (
-                    total_time / self.metrics.total_workflows
-                )
+                self.metrics.average_workflow_time = total_time / self.metrics.total_workflows
 
     def update_system_resources(
         self,
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/optimization_engine.py b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/optimization_engine.py
index 3ddb6e879..4ff27467d 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/optimization_engine.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/optimization_engine.py
@@ -130,9 +130,7 @@ class ConservativeOptimizer(OptimizationAlgorithm):
 
         # Analyze message processing performance
         message_stats = {
-            k: v
-            for k, v in stats.items()
-            if v.category == ResponseTimeCategory.MESSAGE_PROCESSING
+            k: v for k, v in stats.items() if v.category == ResponseTimeCategory.MESSAGE_PROCESSING
         }
 
         for stat in message_stats.values():
@@ -141,9 +139,7 @@ class ConservativeOptimizer(OptimizationAlgorithm):
                 # Look for queue size parameter
                 queue_param = parameters.get("message_queue_size")
                 if queue_param and queue_param.current_value > queue_param.min_value:
-                    adjustment = -max(
-                        1, int(queue_param.current_value * self.adjustment_factor)
-                    )
+                    adjustment = -max(1, int(queue_param.current_value * self.adjustment_factor))
 
                     result = OptimizationResult(
                         optimization_id=uuid4().hex,
@@ -186,9 +182,7 @@ class AggressiveOptimizer(OptimizationAlgorithm):
         for stat in stats.values():
             if stat.success_rate < 0.9:  # Less than 90% success rate
                 # Increase timeout parameters
-                timeout_params = [
-                    p for p in parameters.values() if "timeout" in p.name.lower()
-                ]
+                timeout_params = [p for p in parameters.values() if "timeout" in p.name.lower()]
 
                 for param in timeout_params:
                     if param.current_value < param.max_value:
@@ -201,9 +195,7 @@ class AggressiveOptimizer(OptimizationAlgorithm):
                             optimization_id=uuid4().hex,
                             parameter_name=param.name,
                             old_value=param.current_value,
-                            new_value=min(
-                                param.max_value, param.current_value + adjustment
-                            ),
+                            new_value=min(param.max_value, param.current_value + adjustment),
                             strategy=self.get_strategy(),
                             target=OptimizationTarget.SUCCESS_RATE,
                             improvement_expected=0.2,  # 20% improvement expected
@@ -307,9 +299,7 @@ class OptimizationEngine:
         }
 
         # Enabled strategies
-        self.enabled_strategies = enabled_strategies or [
-            OptimizationStrategy.CONSERVATIVE
-        ]
+        self.enabled_strategies = enabled_strategies or [OptimizationStrategy.CONSERVATIVE]
 
         # System parameters that can be optimized
         self.parameters: dict[str, OptimizationParameter] = {}
@@ -446,15 +436,11 @@ class OptimizationEngine:
                     confidence_score=result.confidence_score,
                 )
 
-            logger.info(
-                f"Applied optimization: {result.parameter_name} {old_value} -> {new_value}"
-            )
+            logger.info(f"Applied optimization: {result.parameter_name} {old_value} -> {new_value}")
             return True
 
         except Exception as e:
-            logger.error(
-                f"Failed to apply optimization for {result.parameter_name}: {e}"
-            )
+            logger.error(f"Failed to apply optimization for {result.parameter_name}: {e}")
             return False
 
     async def _optimization_loop(self) -> None:
@@ -479,11 +465,7 @@ class OptimizationEngine:
             "registered_parameters": len(self.parameters),
             "optimization_history": len(self.optimization_history),
             "recent_optimizations": len(
-                [
-                    r
-                    for r in self.optimization_history
-                    if time.time() - r.timestamp < 3600
-                ]
+                [r for r in self.optimization_history if time.time() - r.timestamp < 3600]
             ),
             "configuration": {
                 "optimization_interval": self.optimization_interval,
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/performance_analytics.py b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/performance_analytics.py
index 8b793d87e..34a2f8e18 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/performance_analytics.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/performance_analytics.py
@@ -115,9 +115,7 @@ class PerformanceAnalytics:
         # Analytics data storage
         self.performance_trends: dict[str, PerformanceTrend] = {}
         self.optimization_effectiveness: dict[str, OptimizationEffectiveness] = {}
-        self.system_health_history: deque = deque(
-            maxlen=1440
-        )  # 24 hours of minute-by-minute data
+        self.system_health_history: deque = deque(maxlen=1440)  # 24 hours of minute-by-minute data
 
         # Cached analytics
         self.cached_analytics: dict[str, Any] = {}
@@ -138,9 +136,7 @@ class PerformanceAnalytics:
 
         self._is_running = True
         self._analytics_task = asyncio.create_task(self._analytics_loop())
-        self._effectiveness_task = asyncio.create_task(
-            self._effectiveness_analysis_loop()
-        )
+        self._effectiveness_task = asyncio.create_task(self._effectiveness_analysis_loop())
         logger.info("PerformanceAnalytics started")
 
     async def stop(self) -> None:
@@ -159,9 +155,7 @@ class PerformanceAnalytics:
 
         logger.info("PerformanceAnalytics stopped")
 
-    async def get_performance_dashboard(
-        self, force_refresh: bool = False
-    ) -> dict[str, Any]:
+    async def get_performance_dashboard(self, force_refresh: bool = False) -> dict[str, Any]:
         """Get comprehensive performance dashboard data."""
         current_time = time.time()
 
@@ -226,9 +220,7 @@ class PerformanceAnalytics:
                 score = 20.0
             success_rate_scores.append(score)
 
-        success_rate_health = (
-            statistics.mean(success_rate_scores) if success_rate_scores else 50.0
-        )
+        success_rate_health = statistics.mean(success_rate_scores) if success_rate_scores else 50.0
 
         # Calculate resource utilization health
         resource_health = 75.0  # Default if no resource manager
@@ -256,8 +248,7 @@ class PerformanceAnalytics:
         optimization_health = 75.0  # Default if no optimization engine
         if self.optimization_engine and self.optimization_effectiveness:
             effectiveness_scores = [
-                eff.effectiveness_score * 100.0
-                for eff in self.optimization_effectiveness.values()
+                eff.effectiveness_score * 100.0 for eff in self.optimization_effectiveness.values()
             ]
             optimization_health = (
                 statistics.mean(effectiveness_scores) if effectiveness_scores else 75.0
@@ -314,9 +305,7 @@ class PerformanceAnalytics:
                 }
 
         # Find slowest operations
-        slowest_operations = sorted(
-            stats.values(), key=lambda s: s.p95_duration, reverse=True
-        )[:10]
+        slowest_operations = sorted(stats.values(), key=lambda s: s.p95_duration, reverse=True)[:10]
 
         return {
             "category_summaries": category_summaries,
@@ -346,19 +335,14 @@ class PerformanceAnalytics:
         effectiveness_summary = {}
         if self.optimization_effectiveness:
             effectiveness_scores = [
-                eff.effectiveness_score
-                for eff in self.optimization_effectiveness.values()
+                eff.effectiveness_score for eff in self.optimization_effectiveness.values()
             ]
             effectiveness_summary = {
                 "total_optimizations": len(self.optimization_effectiveness),
                 "avg_effectiveness": statistics.mean(effectiveness_scores),
                 "median_effectiveness": statistics.median(effectiveness_scores),
-                "successful_optimizations": len(
-                    [e for e in effectiveness_scores if e >= 0.8]
-                ),
-                "failed_optimizations": len(
-                    [e for e in effectiveness_scores if e < 0.5]
-                ),
+                "successful_optimizations": len([e for e in effectiveness_scores if e >= 0.8]),
+                "failed_optimizations": len([e for e in effectiveness_scores if e < 0.5]),
             }
 
         return {
@@ -391,9 +375,7 @@ class PerformanceAnalytics:
 
         # Calculate resource efficiency metrics
         efficiency_metrics = {}
-        for resource_type, pool_stats in resource_stats.get(
-            "resource_pools", {}
-        ).items():
+        for resource_type, pool_stats in resource_stats.get("resource_pools", {}).items():
             utilization = pool_stats.get("utilization_percent", 0.0)
 
             # Efficiency score based on utilization (sweet spot around 70-80%)
@@ -419,10 +401,7 @@ class PerformanceAnalytics:
             "efficiency_metrics": efficiency_metrics,
             "overall_efficiency": (
                 statistics.mean(
-                    [
-                        metrics["efficiency_score"]
-                        for metrics in efficiency_metrics.values()
-                    ]
+                    [metrics["efficiency_score"] for metrics in efficiency_metrics.values()]
                 )
                 if efficiency_metrics
                 else 0.0
@@ -435,34 +414,20 @@ class PerformanceAnalytics:
 
         for period in self.trend_analysis_periods:
             period_trends = {
-                k: v
-                for k, v in self.performance_trends.items()
-                if v.time_period == period
+                k: v for k, v in self.performance_trends.items() if v.time_period == period
             }
 
             if period_trends:
                 trends_by_period[period] = {
                     "total_trends": len(period_trends),
                     "improving_trends": len(
-                        [
-                            t
-                            for t in period_trends.values()
-                            if t.trend_direction == "improving"
-                        ]
+                        [t for t in period_trends.values() if t.trend_direction == "improving"]
                     ),
                     "degrading_trends": len(
-                        [
-                            t
-                            for t in period_trends.values()
-                            if t.trend_direction == "degrading"
-                        ]
+                        [t for t in period_trends.values() if t.trend_direction == "degrading"]
                     ),
                     "stable_trends": len(
-                        [
-                            t
-                            for t in period_trends.values()
-                            if t.trend_direction == "stable"
-                        ]
+                        [t for t in period_trends.values() if t.trend_direction == "stable"]
                     ),
                     "trends": [
                         {
@@ -500,8 +465,7 @@ class PerformanceAnalytics:
                     "description": f"{len(slow_operations)} operations have P95 response times > 5 seconds",
                     "action": "Consider optimizing these operations or increasing timeout values",
                     "affected_operations": [
-                        f"{op.category.value}:{op.operation}"
-                        for op in slow_operations[:5]
+                        f"{op.category.value}:{op.operation}" for op in slow_operations[:5]
                     ],
                 }
             )
@@ -517,8 +481,7 @@ class PerformanceAnalytics:
                     "description": f"{len(failing_operations)} operations have success rates < 90%",
                     "action": "Investigate error causes and consider increasing retry limits or timeouts",
                     "affected_operations": [
-                        f"{op.category.value}:{op.operation}"
-                        for op in failing_operations[:5]
+                        f"{op.category.value}:{op.operation}" for op in failing_operations[:5]
                     ],
                 }
             )
@@ -528,9 +491,7 @@ class PerformanceAnalytics:
             resource_stats = self.resource_manager.get_statistics()
             high_util_resources = []
 
-            for resource_type, pool_stats in resource_stats.get(
-                "resource_pools", {}
-            ).items():
+            for resource_type, pool_stats in resource_stats.get("resource_pools", {}).items():
                 if pool_stats.get("utilization_percent", 0.0) > 90.0:
                     high_util_resources.append(resource_type)
 
@@ -702,14 +663,10 @@ def create_analytics_endpoints(analytics: PerformanceAnalytics):
     async def get_dashboard(force_refresh: bool = False):
         """Get comprehensive performance dashboard."""
         try:
-            return await analytics.get_performance_dashboard(
-                force_refresh=force_refresh
-            )
+            return await analytics.get_performance_dashboard(force_refresh=force_refresh)
         except Exception as e:
             logger.error(f"Error getting dashboard: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to get dashboard data"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to get dashboard data") from e
 
     @router.get("/health")
     async def get_system_health():
@@ -719,9 +676,7 @@ def create_analytics_endpoints(analytics: PerformanceAnalytics):
             return health.to_dict()
         except Exception as e:
             logger.error(f"Error getting system health: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to get system health"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to get system health") from e
 
     @router.get("/trends/{period}")
     async def get_performance_trends(period: str):
@@ -731,9 +686,7 @@ def create_analytics_endpoints(analytics: PerformanceAnalytics):
             return trends.get(period, {})
         except Exception as e:
             logger.error(f"Error getting trends: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to get performance trends"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to get performance trends") from e
 
     @router.get("/recommendations")
     async def get_recommendations():
@@ -742,9 +695,7 @@ def create_analytics_endpoints(analytics: PerformanceAnalytics):
             return await analytics._generate_recommendations()
         except Exception as e:
             logger.error(f"Error getting recommendations: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to get recommendations"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to get recommendations") from e
 
     @router.get("/statistics")
     async def get_analytics_statistics():
@@ -753,8 +704,6 @@ def create_analytics_endpoints(analytics: PerformanceAnalytics):
             return analytics.get_statistics()
         except Exception as e:
             logger.error(f"Error getting statistics: {e}")
-            raise HTTPException(
-                status_code=500, detail="Failed to get analytics statistics"
-            ) from e
+            raise HTTPException(status_code=500, detail="Failed to get analytics statistics") from e
 
     return router
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/response_time_monitor.py b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/response_time_monitor.py
index b4f2bd3e8..e14c54ae9 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/response_time_monitor.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/response_time_monitor.py
@@ -134,10 +134,8 @@ class ResponseTimeCollector:
         self.metric_retention_seconds = metric_retention_hours * 3600
 
         # Metric storage: category -> operation -> deque of metrics
-        self.metrics: dict[
-            ResponseTimeCategory, dict[str, deque[ResponseTimeMetric]]
-        ] = defaultdict(
-            lambda: defaultdict(lambda: deque(maxlen=max_metrics_per_operation))
+        self.metrics: dict[ResponseTimeCategory, dict[str, deque[ResponseTimeMetric]]] = (
+            defaultdict(lambda: defaultdict(lambda: deque(maxlen=max_metrics_per_operation)))
         )
 
         # Active timing contexts
@@ -321,9 +319,7 @@ class ResponseTimeCollector:
 
         return stats
 
-    def get_all_stats(
-        self, force_refresh: bool = False
-    ) -> dict[str, ResponseTimeStats]:
+    def get_all_stats(self, force_refresh: bool = False) -> dict[str, ResponseTimeStats]:
         """Get statistics for all operations."""
         all_stats = {}
 
@@ -427,10 +423,7 @@ class ResponseTimeCollector:
                 # Clean up orphaned active timings (older than 1 hour)
                 orphaned_contexts = []
                 for context_id, start_time in self.active_timings.items():
-                    if (
-                        isinstance(start_time, float)
-                        and current_time - start_time > 3600
-                    ):
+                    if isinstance(start_time, float) and current_time - start_time > 3600:
                         orphaned_contexts.append(context_id)
 
                 for context_id in orphaned_contexts:
@@ -453,9 +446,7 @@ class ResponseTimeCollector:
     def get_statistics(self) -> dict[str, Any]:
         """Get collector statistics."""
         total_metrics = sum(
-            len(operations[op])
-            for operations in self.metrics.values()
-            for op in operations
+            len(operations[op]) for operations in self.metrics.values() for op in operations
         )
 
         return {
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/workflow_resource_manager.py b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/workflow_resource_manager.py
index caae45038..521e2dfcd 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/workflow_resource_manager.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/optimization/workflow_resource_manager.py
@@ -55,11 +55,7 @@ class ResourceAllocation:
 
     def utilization_percentage(self) -> float:
         """Calculate resource utilization percentage."""
-        return (
-            (self.allocated_amount / self.max_amount) * 100.0
-            if self.max_amount > 0
-            else 0.0
-        )
+        return (self.allocated_amount / self.max_amount) * 100.0 if self.max_amount > 0 else 0.0
 
 
 @dataclass
@@ -93,8 +89,7 @@ class ResourcePool:
     def utilization_percentage(self) -> float:
         """Get utilization percentage."""
         return (
-            ((self.allocated_capacity + self.reserved_capacity) / self.total_capacity)
-            * 100.0
+            ((self.allocated_capacity + self.reserved_capacity) / self.total_capacity) * 100.0
             if self.total_capacity > 0
             else 0.0
         )
@@ -192,9 +187,7 @@ class WorkflowScheduler:
         else:
             self.total_failed += 1
 
-        logger.info(
-            f"Completed workflow {workflow_id} ({'success' if success else 'failed'})"
-        )
+        logger.info(f"Completed workflow {workflow_id} ({'success' if success else 'failed'})")
         return True
 
     def get_queue_stats(self) -> dict[str, Any]:
@@ -203,8 +196,7 @@ class WorkflowScheduler:
             "running_workflows": len(self.running_workflows),
             "max_concurrent": self.max_concurrent_workflows,
             "queued_workflows": {
-                priority.value: len(queue)
-                for priority, queue in self.workflow_queues.items()
+                priority.value: len(queue) for priority, queue in self.workflow_queues.items()
             },
             "total_queued": sum(len(queue) for queue in self.workflow_queues.values()),
             "total_scheduled": self.total_scheduled,
@@ -233,12 +225,8 @@ class WorkflowResourceManager:
         # Resource pools
         self.resource_pools: dict[ResourceType, ResourcePool] = {
             ResourceType.CPU: ResourcePool(ResourceType.CPU, 100.0),  # 100% CPU
-            ResourceType.MEMORY: ResourcePool(
-                ResourceType.MEMORY, 8192.0
-            ),  # 8GB memory
-            ResourceType.NETWORK: ResourcePool(
-                ResourceType.NETWORK, 1000.0
-            ),  # 1000 Mbps
+            ResourceType.MEMORY: ResourcePool(ResourceType.MEMORY, 8192.0),  # 8GB memory
+            ResourceType.NETWORK: ResourcePool(ResourceType.NETWORK, 1000.0),  # 1000 Mbps
             ResourceType.AGENT_SLOTS: ResourcePool(
                 ResourceType.AGENT_SLOTS, 50.0
             ),  # 50 agent slots
@@ -356,9 +344,7 @@ class WorkflowResourceManager:
                 return False
         return True
 
-    async def _allocate_workflow_resources(
-        self, request: WorkflowResourceRequest
-    ) -> bool:
+    async def _allocate_workflow_resources(self, request: WorkflowResourceRequest) -> bool:
         """Allocate resources for a workflow."""
         if not self._can_allocate_resources(request):
             return False
@@ -436,9 +422,7 @@ class WorkflowResourceManager:
             utilization = pool.utilization_percentage()
 
             if utilization > 90.0:  # High utilization warning
-                logger.warning(
-                    f"High {resource_type.value} utilization: {utilization:.1f}%"
-                )
+                logger.warning(f"High {resource_type.value} utilization: {utilization:.1f}%")
 
             # Record utilization metric
             if self.response_time_collector:
@@ -461,15 +445,10 @@ class WorkflowResourceManager:
 
         for workflow_id, allocations in self.allocations.items():
             # Check if any allocation is stale
-            if (
-                allocations
-                and current_time - allocations[0].allocated_at > stale_threshold
-            ):
+            if allocations and current_time - allocations[0].allocated_at > stale_threshold:
                 # Check if workflow is still active in tracker
                 if self.workflow_tracker:
-                    workflow_status = self.workflow_tracker.get_workflow_status(
-                        workflow_id
-                    )
+                    workflow_status = self.workflow_tracker.get_workflow_status(workflow_id)
                     if not workflow_status:  # Workflow not found in tracker
                         stale_workflows.append(workflow_id)
                 else:
@@ -496,9 +475,7 @@ class WorkflowResourceManager:
             },
             "scheduler_stats": self.scheduler.get_queue_stats(),
             "active_allocations": len(self.allocations),
-            "total_allocated_workflows": sum(
-                len(allocs) for allocs in self.allocations.values()
-            ),
+            "total_allocated_workflows": sum(len(allocs) for allocs in self.allocations.values()),
         }
 
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/alerting.py b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/alerting.py
index 566b975d8..2a4754cc2 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/alerting.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/alerting.py
@@ -112,9 +112,7 @@ class PerformanceAlerting:
         escalation_rules: list[EscalationRule] | None = None,
     ):
         # Alert configuration
-        self.thresholds: list[AlertThreshold] = (
-            default_thresholds or self._get_default_thresholds()
-        )
+        self.thresholds: list[AlertThreshold] = default_thresholds or self._get_default_thresholds()
         self.escalation_rules: dict[tuple, EscalationRule] = {}
 
         if escalation_rules:
@@ -204,7 +202,9 @@ class PerformanceAlerting:
             return None
 
         # Create alert
-        alert_id = f"{AlertType.RESPONSE_TIME_VIOLATION.value}_{operation_type.value}_{int(time.time())}"
+        alert_id = (
+            f"{AlertType.RESPONSE_TIME_VIOLATION.value}_{operation_type.value}_{int(time.time())}"
+        )
 
         alert = Alert(
             alert_id=alert_id,
@@ -230,9 +230,7 @@ class PerformanceAlerting:
         metadata: dict[str, Any] | None = None,
     ) -> Alert:
         """Create SLA breach alert."""
-        alert_id = (
-            f"{AlertType.SLA_BREACH.value}_{operation_type.value}_{int(time.time())}"
-        )
+        alert_id = f"{AlertType.SLA_BREACH.value}_{operation_type.value}_{int(time.time())}"
 
         alert = Alert(
             alert_id=alert_id,
@@ -250,9 +248,7 @@ class PerformanceAlerting:
         await self._process_alert(alert)
         return alert
 
-    async def create_bottleneck_alert(
-        self, bottleneck: BottleneckIdentification
-    ) -> Alert:
+    async def create_bottleneck_alert(self, bottleneck: BottleneckIdentification) -> Alert:
         """Create bottleneck detection alert."""
         alert_id = f"{AlertType.BOTTLENECK_DETECTED.value}_{bottleneck.bottleneck_type.value}_{int(time.time())}"
 
@@ -273,18 +269,14 @@ class PerformanceAlerting:
             title=f"Performance Bottleneck Detected - {bottleneck.bottleneck_type.value}",
             description=bottleneck.description,
             operation_type=(
-                bottleneck.affected_operations[0]
-                if bottleneck.affected_operations
-                else None
+                bottleneck.affected_operations[0] if bottleneck.affected_operations else None
             ),
             metric_value=bottleneck.severity,
             threshold_value=0.3,  # Minimum severity for alerting
             timestamp=time.time(),
             metadata={
                 "bottleneck_type": bottleneck.bottleneck_type.value,
-                "affected_operations": [
-                    op.value for op in bottleneck.affected_operations
-                ],
+                "affected_operations": [op.value for op in bottleneck.affected_operations],
                 "evidence": bottleneck.evidence,
                 "recommendations": bottleneck.recommendations,
                 "confidence": bottleneck.confidence,
@@ -390,9 +382,7 @@ class PerformanceAlerting:
         # Send to handlers
         await self._send_alert(alert)
 
-        logger.warning(
-            f"Alert generated: {alert.title} (Severity: {alert.severity.value})"
-        )
+        logger.warning(f"Alert generated: {alert.title} (Severity: {alert.severity.value})")
 
     async def _send_alert(self, alert: Alert) -> None:
         """Send alert to all registered handlers."""
@@ -439,9 +429,7 @@ class PerformanceAlerting:
 
         # Escalate
         alert.escalation_count += 1
-        alert.escalation_level = EscalationLevel(
-            min(4, alert.escalation_level.value + 1)
-        )
+        alert.escalation_level = EscalationLevel(min(4, alert.escalation_level.value + 1))
         alert.last_escalation_time = current_time
 
         self.escalation_counts[alert.escalation_level] += 1
@@ -449,13 +437,9 @@ class PerformanceAlerting:
         # Send escalation notifications
         await self._send_escalation(alert, escalation_rule)
 
-        logger.warning(
-            f"Alert escalated: {alert.alert_id} to level {alert.escalation_level.value}"
-        )
+        logger.warning(f"Alert escalated: {alert.alert_id} to level {alert.escalation_level.value}")
 
-    async def _send_escalation(
-        self, alert: Alert, escalation_rule: EscalationRule
-    ) -> None:
+    async def _send_escalation(self, alert: Alert, escalation_rule: EscalationRule) -> None:
         """Send escalation notifications."""
         for target in escalation_rule.escalation_targets:
             handler = self.escalation_handlers.get(target)
@@ -496,18 +480,12 @@ class PerformanceAlerting:
         """Find applicable threshold for operation type and metric."""
         # Look for specific threshold first
         for threshold in self.thresholds:
-            if (
-                threshold.operation_type == operation_type
-                and threshold.metric_name == metric_name
-            ):
+            if threshold.operation_type == operation_type and threshold.metric_name == metric_name:
                 return threshold
 
         # Look for global threshold
         for threshold in self.thresholds:
-            if (
-                threshold.operation_type is None
-                and threshold.metric_name == metric_name
-            ):
+            if threshold.operation_type is None and threshold.metric_name == metric_name:
                 return threshold
 
         return None
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/analytics.py b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/analytics.py
index 8d12ed089..68ea1a447 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/analytics.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/analytics.py
@@ -137,9 +137,7 @@ class PerformanceAnalytics:
         trends = await self._analyze_trends(statistics)
 
         # Generate recommendations
-        recommendations = await self._generate_recommendations(
-            bottlenecks, trends, statistics
-        )
+        recommendations = await self._generate_recommendations(bottlenecks, trends, statistics)
 
         # Calculate overall health score
         overall_health = self._calculate_health_score(statistics, bottlenecks)
@@ -149,9 +147,7 @@ class PerformanceAnalytics:
         return {
             "bottlenecks": [self._bottleneck_to_dict(b) for b in bottlenecks],
             "trends": [self._trend_to_dict(t) for t in trends],
-            "recommendations": [
-                self._recommendation_to_dict(r) for r in recommendations
-            ],
+            "recommendations": [self._recommendation_to_dict(r) for r in recommendations],
             "overall_health": overall_health,
             "analysis_timestamp": current_time,
             "analysis_window_minutes": self.analysis_window_minutes,
@@ -304,9 +300,7 @@ class PerformanceAnalytics:
 
         # High variance in response times suggests contention
         if stats.total_operations > 5:
-            variance_indicator = (
-                stats.max_duration - stats.min_duration
-            ) / stats.average_duration
+            variance_indicator = (stats.max_duration - stats.min_duration) / stats.average_duration
 
             if variance_indicator > 3.0 and stats.average_duration > 1.0:
                 severity = min(1.0, variance_indicator / 10.0)
@@ -336,9 +330,7 @@ class PerformanceAnalytics:
 
         return bottlenecks
 
-    async def _analyze_trends(
-        self, statistics: dict[OperationType, Any]
-    ) -> list[PerformanceTrend]:
+    async def _analyze_trends(self, statistics: dict[OperationType, Any]) -> list[PerformanceTrend]:
         """Analyze performance trends."""
         trends = []
 
@@ -347,9 +339,7 @@ class PerformanceAnalytics:
             self.trend_history[op_type].append(stats.average_duration)
 
             if len(self.trend_history[op_type]) >= 3:
-                trend = self._calculate_trend(
-                    op_type, list(self.trend_history[op_type])
-                )
+                trend = self._calculate_trend(op_type, list(self.trend_history[op_type]))
                 if trend:
                     trends.append(trend)
 
@@ -371,8 +361,7 @@ class PerformanceAnalytics:
         y_mean = statistics.mean(data_points)
 
         numerator = sum(
-            (x - x_mean) * (y - y_mean)
-            for x, y in zip(x_values, data_points, strict=False)
+            (x - x_mean) * (y - y_mean) for x, y in zip(x_values, data_points, strict=False)
         )
         denominator = sum((x - x_mean) ** 2 for x in x_values)
 
@@ -429,18 +418,14 @@ class PerformanceAnalytics:
                     priority=1,
                     estimated_improvement=bottleneck.estimated_impact,
                     implementation_effort="medium",
-                    affected_components=[
-                        op.value for op in bottleneck.affected_operations
-                    ],
+                    affected_components=[op.value for op in bottleneck.affected_operations],
                     prerequisites=[],
                     risks=["Temporary performance impact during implementation"],
                 )
                 recommendations.append(rec)
 
         # Trend-based recommendations
-        degrading_trends = [
-            t for t in trends if t.trend_direction == TrendDirection.DEGRADING
-        ]
+        degrading_trends = [t for t in trends if t.trend_direction == TrendDirection.DEGRADING]
         if degrading_trends:
             rec = OptimizationRecommendation(
                 recommendation_id="trend_degradation",
@@ -502,9 +487,7 @@ class PerformanceAnalytics:
             return "poor"
         return "critical"
 
-    def _bottleneck_to_dict(
-        self, bottleneck: BottleneckIdentification
-    ) -> dict[str, Any]:
+    def _bottleneck_to_dict(self, bottleneck: BottleneckIdentification) -> dict[str, Any]:
         """Convert bottleneck to dictionary."""
         return {
             "type": bottleneck.bottleneck_type.value,
@@ -529,9 +512,7 @@ class PerformanceAnalytics:
             "confidence": trend.confidence,
         }
 
-    def _recommendation_to_dict(
-        self, recommendation: OptimizationRecommendation
-    ) -> dict[str, Any]:
+    def _recommendation_to_dict(self, recommendation: OptimizationRecommendation) -> dict[str, Any]:
         """Convert recommendation to dictionary."""
         return {
             "id": recommendation.recommendation_id,
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/optimization.py b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/optimization.py
index b901afd9a..a0d9b47ba 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/optimization.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/optimization.py
@@ -172,9 +172,7 @@ class IntelligentAgentCoordinator:
 
         logger.info("IntelligentAgentCoordinator stopped")
 
-    def register_agent(
-        self, agent_id: str, agent_type: AgentType, max_concurrent: int = 5
-    ) -> None:
+    def register_agent(self, agent_id: str, agent_type: AgentType, max_concurrent: int = 5) -> None:
         """Register an agent with the coordinator."""
         profile = AgentPerformanceProfile(
             agent_id=agent_id,
@@ -220,9 +218,7 @@ class IntelligentAgentCoordinator:
         # ADAPTIVE
         return await self._schedule_adaptive(request)
 
-    async def _schedule_fastest_first(
-        self, request: WorkflowRequest
-    ) -> SchedulingDecision | None:
+    async def _schedule_fastest_first(self, request: WorkflowRequest) -> SchedulingDecision | None:
         """Schedule using fastest available agents."""
         selected_agents = {}
         total_estimated_time = 0.0
@@ -246,9 +242,7 @@ class IntelligentAgentCoordinator:
             best_agent = available_agents[0]
 
             selected_agents[agent_type] = best_agent.agent_id
-            total_estimated_time = max(
-                total_estimated_time, best_agent.average_response_time
-            )
+            total_estimated_time = max(total_estimated_time, best_agent.average_response_time)
 
         return SchedulingDecision(
             request_id=request.request_id,
@@ -258,9 +252,7 @@ class IntelligentAgentCoordinator:
             reasoning="Selected fastest available agents",
         )
 
-    async def _schedule_load_balanced(
-        self, request: WorkflowRequest
-    ) -> SchedulingDecision | None:
+    async def _schedule_load_balanced(self, request: WorkflowRequest) -> SchedulingDecision | None:
         """Schedule using load balancing."""
         selected_agents = {}
         total_estimated_time = 0.0
@@ -286,9 +278,7 @@ class IntelligentAgentCoordinator:
             selected_agents[agent_type] = best_agent.agent_id
 
             # Estimate completion time considering current load
-            load_factor = 1.0 + (
-                best_agent.current_load * 0.2
-            )  # 20% penalty per concurrent task
+            load_factor = 1.0 + (best_agent.current_load * 0.2)  # 20% penalty per concurrent task
             estimated_time = best_agent.average_response_time * load_factor
             total_estimated_time = max(total_estimated_time, estimated_time)
 
@@ -300,9 +290,7 @@ class IntelligentAgentCoordinator:
             reasoning="Selected least loaded agents",
         )
 
-    async def _schedule_predictive(
-        self, request: WorkflowRequest
-    ) -> SchedulingDecision | None:
+    async def _schedule_predictive(self, request: WorkflowRequest) -> SchedulingDecision | None:
         """Schedule using predictive algorithms."""
         selected_agents = {}
         total_estimated_time = 0.0
@@ -341,9 +329,7 @@ class IntelligentAgentCoordinator:
                 total_estimated_time = max(total_estimated_time, best_predicted_time)
                 confidence_scores.append(best_confidence)
 
-        overall_confidence = (
-            statistics.mean(confidence_scores) if confidence_scores else 0.0
-        )
+        overall_confidence = statistics.mean(confidence_scores) if confidence_scores else 0.0
 
         return SchedulingDecision(
             request_id=request.request_id,
@@ -353,9 +339,7 @@ class IntelligentAgentCoordinator:
             reasoning="Selected agents using predictive performance modeling",
         )
 
-    async def _schedule_adaptive(
-        self, request: WorkflowRequest
-    ) -> SchedulingDecision | None:
+    async def _schedule_adaptive(self, request: WorkflowRequest) -> SchedulingDecision | None:
         """Schedule using adaptive strategy based on current conditions."""
         # Analyze current system state
         system_load = self._calculate_system_load()
@@ -391,9 +375,7 @@ class IntelligentAgentCoordinator:
 
         # Calculate confidence based on reliability and recent activity
         time_since_activity = time.time() - agent.last_activity
-        activity_factor = max(
-            0.5, 1.0 - (time_since_activity / 3600)
-        )  # Decay over 1 hour
+        activity_factor = max(0.5, 1.0 - (time_since_activity / 3600))  # Decay over 1 hour
 
         confidence = agent.reliability_score * activity_factor
 
@@ -407,9 +389,7 @@ class IntelligentAgentCoordinator:
         # Adjust for deadline urgency
         if request.deadline:
             time_to_deadline = request.deadline - time.time()
-            urgency_factor = max(
-                0.1, 1.0 / max(time_to_deadline / 60, 1)
-            )  # Minutes to deadline
+            urgency_factor = max(0.1, 1.0 / max(time_to_deadline / 60, 1))  # Minutes to deadline
             base_priority *= urgency_factor
 
         return base_priority
@@ -431,9 +411,7 @@ class IntelligentAgentCoordinator:
         if len(self.agent_profiles) < 2:
             return 0.0
 
-        response_times = [
-            profile.average_response_time for profile in self.agent_profiles.values()
-        ]
+        response_times = [profile.average_response_time for profile in self.agent_profiles.values()]
 
         try:
             return statistics.stdev(response_times) / statistics.mean(response_times)
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/response_time_monitor.py b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/response_time_monitor.py
index ed64b9e8d..c42c3e5fa 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/performance/response_time_monitor.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/performance/response_time_monitor.py
@@ -107,9 +107,7 @@ class ResponseTimeMonitor:
         self.enable_real_time_analysis = enable_real_time_analysis
 
         # Metrics storage
-        self.metrics_history: deque[ResponseTimeMetric] = deque(
-            maxlen=max_metrics_history
-        )
+        self.metrics_history: deque[ResponseTimeMetric] = deque(maxlen=max_metrics_history)
         self.active_operations: dict[str, dict[str, Any]] = {}
 
         # Statistics cache
@@ -287,9 +285,7 @@ class ResponseTimeMonitor:
 
         if operation_type:
             recent_metrics = [
-                metric
-                for metric in recent_metrics
-                if metric.operation_type == operation_type
+                metric for metric in recent_metrics if metric.operation_type == operation_type
             ]
 
         # Group metrics by operation type
@@ -310,9 +306,7 @@ class ResponseTimeMonitor:
     ) -> PerformanceStatistics:
         """Calculate performance statistics for a set of metrics."""
         durations = [metric.duration for metric in metrics]
-        successful_operations = [
-            metric for metric in metrics if not metric.metadata.get("error")
-        ]
+        successful_operations = [metric for metric in metrics if not metric.metadata.get("error")]
 
         # Performance level distribution
         performance_distribution = defaultdict(int)
@@ -349,9 +343,7 @@ class ResponseTimeMonitor:
         if upper_index >= len(sorted_data):
             return sorted_data[lower_index]
 
-        return (
-            sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight
-        )
+        return sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight
 
     async def _cleanup_loop(self) -> None:
         """Background task to clean up old metrics."""
@@ -402,10 +394,7 @@ class ResponseTimeMonitor:
                 await self._trigger_sla_violation_alert(op_type, stats)
 
             # Check for performance degradation
-            if (
-                stats.average_duration
-                > self.performance_thresholds[PerformanceLevel.ACCEPTABLE]
-            ):
+            if stats.average_duration > self.performance_thresholds[PerformanceLevel.ACCEPTABLE]:
                 await self._trigger_degradation_alert(op_type, stats)
 
     async def _trigger_sla_violation_alert(
@@ -435,9 +424,7 @@ class ResponseTimeMonitor:
             "type": "performance_degradation",
             "operation_type": operation_type.value,
             "average_duration": statistics.average_duration,
-            "acceptable_threshold": self.performance_thresholds[
-                PerformanceLevel.ACCEPTABLE
-            ],
+            "acceptable_threshold": self.performance_thresholds[PerformanceLevel.ACCEPTABLE],
             "total_operations": statistics.total_operations,
             "timestamp": time.time(),
         }
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/profiling.py b/packages/tta-ai-framework/src/tta_ai/orchestration/profiling.py
index 194b6630e..ae6b06f51 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/profiling.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/profiling.py
@@ -83,9 +83,7 @@ class AgentCoordinationProfiler:
             memory_usage = self._memory_tracker.stop_tracking(profile_name)
 
             # Generate profile result
-            result = self._generate_profile_result(
-                profiler, end_time - start_time, memory_usage
-            )
+            result = self._generate_profile_result(profiler, end_time - start_time, memory_usage)
 
             with self._profile_lock:
                 self._active_profiles.pop(profile_name, None)
@@ -129,9 +127,7 @@ class AgentCoordinationProfiler:
             function_calls=stats.total_calls,
             primitive_calls=stats.prim_calls,
             total_time=stats.total_tt,
-            cumulative_time=sum(
-                ct for (cc, nc, tt, ct, callers) in stats.stats.values()
-            ),
+            cumulative_time=sum(ct for (cc, nc, tt, ct, callers) in stats.stats.values()),
             top_functions=top_functions,
             memory_usage=memory_usage,
         )
@@ -408,9 +404,7 @@ class CoordinationBenchmark:
         latency_degradation = []
 
         baseline_throughput = concurrency_results[concurrency_levels[0]].throughput_rps
-        baseline_latency = concurrency_results[
-            concurrency_levels[0]
-        ].average_response_time
+        baseline_latency = concurrency_results[concurrency_levels[0]].average_response_time
 
         for concurrency in concurrency_levels:
             metrics = concurrency_results[concurrency]
@@ -419,17 +413,13 @@ class CoordinationBenchmark:
             expected_throughput = baseline_throughput * concurrency
             actual_throughput = metrics.throughput_rps
             scaling_efficiency = (
-                actual_throughput / expected_throughput
-                if expected_throughput > 0
-                else 0
+                actual_throughput / expected_throughput if expected_throughput > 0 else 0
             )
             throughput_scaling.append(scaling_efficiency)
 
             # Latency degradation
             latency_increase = (
-                metrics.average_response_time / baseline_latency
-                if baseline_latency > 0
-                else 1
+                metrics.average_response_time / baseline_latency if baseline_latency > 0 else 1
             )
             latency_degradation.append(latency_increase)
 
@@ -441,9 +431,7 @@ class CoordinationBenchmark:
             "scalability_score": min(throughput_scaling) if throughput_scaling else 0,
         }
 
-    def _find_optimal_concurrency(
-        self, concurrency_results: dict[int, ConcurrencyMetrics]
-    ) -> int:
+    def _find_optimal_concurrency(self, concurrency_results: dict[int, ConcurrencyMetrics]) -> int:
         """Find optimal concurrency level based on throughput and latency trade-off."""
         best_score = 0
         optimal_concurrency = 1
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/protocol_bridge.py b/packages/tta-ai-framework/src/tta_ai/orchestration/protocol_bridge.py
index 2eefacbb4..b486cc3c4 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/protocol_bridge.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/protocol_bridge.py
@@ -89,9 +89,7 @@ class ProtocolTranslator:
             # Determine translation rule
             rule_key = f"{source_protocol.value}_to_{target_protocol.value}"
             if agent_type:
-                specific_rule_key = (
-                    f"{source_protocol.value}_to_{agent_type.value.lower()}"
-                )
+                specific_rule_key = f"{source_protocol.value}_to_{agent_type.value.lower()}"
                 if specific_rule_key in self._translation_rules:
                     rule_key = specific_rule_key
 
@@ -207,9 +205,7 @@ class ProtocolTranslator:
 class MessageRouter:
     """Routes messages between orchestration system and real agents."""
 
-    def __init__(
-        self, ipa_adapter: IPAAdapter, wba_adapter: WBAAdapter, nga_adapter: NGAAdapter
-    ):
+    def __init__(self, ipa_adapter: IPAAdapter, wba_adapter: WBAAdapter, nga_adapter: NGAAdapter):
         self.ipa_adapter = ipa_adapter
         self.wba_adapter = wba_adapter
         self.nga_adapter = nga_adapter
@@ -252,9 +248,7 @@ class MessageRouter:
                 message_id=self._get_message_id(message), delivered=False, error=str(e)
             )
 
-    async def _route_to_ipa(
-        self, message: AgentMessage | dict[str, Any]
-    ) -> MessageResult:
+    async def _route_to_ipa(self, message: AgentMessage | dict[str, Any]) -> MessageResult:
         """Route message to IPA adapter."""
         try:
             # Translate message to IPA format
@@ -295,9 +289,7 @@ class MessageRouter:
                 message_id=self._get_message_id(message), delivered=False, error=str(e)
             )
 
-    async def _route_to_wba(
-        self, message: AgentMessage | dict[str, Any]
-    ) -> MessageResult:
+    async def _route_to_wba(self, message: AgentMessage | dict[str, Any]) -> MessageResult:
         """Route message to WBA adapter."""
         try:
             # Translate message to WBA format
@@ -332,9 +324,7 @@ class MessageRouter:
                 message_id=self._get_message_id(message), delivered=False, error=str(e)
             )
 
-    async def _route_to_nga(
-        self, message: AgentMessage | dict[str, Any]
-    ) -> MessageResult:
+    async def _route_to_nga(self, message: AgentMessage | dict[str, Any]) -> MessageResult:
         """Route message to NGA adapter."""
         try:
             # Translate message to NGA format
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/proxies.py b/packages/tta-ai-framework/src/tta_ai/orchestration/proxies.py
index e21fab460..56dba4593 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/proxies.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/proxies.py
@@ -109,9 +109,7 @@ class InputProcessorAgentProxy(Agent):
                         "source": ipa_result.get("source", "real_ipa"),
                     }
 
-                    await operation["publish_progress"](
-                        1.0, "Input processing completed"
-                    )
+                    await operation["publish_progress"](1.0, "Input processing completed")
                     logger.info(
                         f"IPA processed input with intent: {result['routing'].get('intent', 'unknown')}"
                     )
@@ -122,9 +120,7 @@ class InputProcessorAgentProxy(Agent):
                     if not self.fallback_to_mock:
                         raise
                     logger.warning("Falling back to mock implementation")
-                    await operation["publish_progress"](
-                        0.5, "Falling back to mock implementation"
-                    )
+                    await operation["publish_progress"](0.5, "Falling back to mock implementation")
 
             # Fallback to mock implementation
             await operation["publish_progress"](0.7, "Using mock implementation")
@@ -146,7 +142,7 @@ class InputProcessorAgentProxy(Agent):
         while True:
             try:
                 return await self.process_with_timeout(payload)
-            except asyncio.TimeoutError:
+            except TimeoutError:
                 attempt += 1
                 if attempt > retries:
                     raise
@@ -200,9 +196,7 @@ class InputProcessorAgentProxy(Agent):
 
         # Simple selection: prefer instances with lower load
         # In a real implementation, this could consider more factors
-        best_instance = min(
-            instances, key=lambda x: x.get("status", {}).get("load", 1.0)
-        )
+        best_instance = min(instances, key=lambda x: x.get("status", {}).get("load", 1.0))
 
         logger.debug(f"Selected IPA instance: {best_instance.get('name', 'unknown')}")
         return best_instance
@@ -306,9 +300,7 @@ class WorldBuilderAgentProxy(Agent):
         # Process through real WBA if available
         if self.enable_real_agent and self.wba_adapter:
             try:
-                wba_result = await self.wba_adapter.process_world_request(
-                    world_id, updates
-                )
+                wba_result = await self.wba_adapter.process_world_request(world_id, updates)
 
                 # Cache the result for performance
                 if wba_result.get("world_state"):
@@ -377,9 +369,7 @@ class WorldBuilderAgentProxy(Agent):
                 if version:
                     enhanced_updates["_version"] = version
 
-                result = await self.wba_adapter.process_world_request(
-                    world_id, enhanced_updates
-                )
+                result = await self.wba_adapter.process_world_request(world_id, enhanced_updates)
 
                 # Check if conflict was detected and resolved
                 if result.get("conflict_resolved"):
@@ -497,18 +487,14 @@ class NarrativeGeneratorAgentProxy(Agent):
                 enhanced_context = context.copy()
                 enhanced_context.update(
                     {
-                        "narrative_context": self._narrative_context.get(
-                            session_id, {}
-                        ),
+                        "narrative_context": self._narrative_context.get(session_id, {}),
                         "narrative_history": self._get_recent_narrative_history(
                             session_id, limit=5
                         ),
                     }
                 )
 
-                nga_result = await self.nga_adapter.generate_narrative(
-                    prompt, enhanced_context
-                )
+                nga_result = await self.nga_adapter.generate_narrative(prompt, enhanced_context)
                 story = nga_result.get("story", "")
 
                 # Track narrative state
@@ -615,9 +601,7 @@ class NarrativeGeneratorAgentProxy(Agent):
     ) -> list[dict[str, Any]]:
         """Get recent narrative history for a session."""
         session_history = [
-            entry
-            for entry in self._narrative_history
-            if entry.get("session_id") == session_id
+            entry for entry in self._narrative_history if entry.get("session_id") == session_id
         ]
         return session_history[-limit:] if session_history else []
 
@@ -638,9 +622,7 @@ class NarrativeGeneratorAgentProxy(Agent):
         self._narrative_history.append(narrative_entry)
 
         # Keep only recent history (last 100 entries per session)
-        session_entries = [
-            e for e in self._narrative_history if e.get("session_id") == session_id
-        ]
+        session_entries = [e for e in self._narrative_history if e.get("session_id") == session_id]
         if len(session_entries) > 100:
             # Remove oldest entries for this session
             entries_to_remove = session_entries[:-100]
@@ -702,13 +684,10 @@ class NarrativeGeneratorAgentProxy(Agent):
             "characters_mentioned": list(characters_mentioned),
             "locations_visited": list(locations_visited),
             "narrative_arc_length": len(recent_history),
-            "session_duration": time.time()
-            - session_context.get("session_start", time.time()),
+            "session_duration": time.time() - session_context.get("session_start", time.time()),
         }
 
-    async def format_narrative_output(
-        self, story: str, context: dict[str, Any]
-    ) -> dict[str, Any]:
+    async def format_narrative_output(self, story: str, context: dict[str, Any]) -> dict[str, Any]:
         """
         Format narrative output with enhanced metadata and structure.
 
@@ -729,20 +708,15 @@ class NarrativeGeneratorAgentProxy(Agent):
             if len(sentences) > 3:
                 mid_point = len(sentences) // 2
                 formatted_story = (
-                    ". ".join(sentences[:mid_point])
-                    + ".\n\n"
-                    + ". ".join(sentences[mid_point:])
+                    ". ".join(sentences[:mid_point]) + ".\n\n" + ". ".join(sentences[mid_point:])
                 )
 
         # Extract narrative elements
         narrative_elements = {
             "word_count": len(formatted_story.split()),
-            "estimated_reading_time": len(formatted_story.split())
-            / 200,  # ~200 words per minute
+            "estimated_reading_time": len(formatted_story.split()) / 200,  # ~200 words per minute
             "narrative_tone": self._analyze_narrative_tone(formatted_story),
-            "therapeutic_elements": self._identify_therapeutic_elements(
-                formatted_story
-            ),
+            "therapeutic_elements": self._identify_therapeutic_elements(formatted_story),
         }
 
         return {
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/agent_event_integration.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/agent_event_integration.py
index dffde23c2..4cfb3688c 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/agent_event_integration.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/agent_event_integration.py
@@ -41,9 +41,7 @@ class AgentEventIntegrator:
         self.active_operations: dict[str, dict[str, Any]] = {}
         self.operation_counter = 0
 
-        logger.debug(
-            f"AgentEventIntegrator initialized for {agent_id}, enabled: {self.enabled}"
-        )
+        logger.debug(f"AgentEventIntegrator initialized for {agent_id}, enabled: {self.enabled}")
 
     def _generate_operation_id(self) -> str:
         """Generate unique operation ID."""
@@ -111,9 +109,7 @@ class AgentEventIntegrator:
 
             # Send completion event
             duration = time.time() - start_time
-            agent_type = (
-                self.agent_id.split(":")[0] if ":" in self.agent_id else "unknown"
-            )
+            agent_type = self.agent_id.split(":")[0] if ":" in self.agent_id else "unknown"
             completion_event = create_agent_status_event(
                 agent_id=self.agent_id,
                 agent_type=agent_type,
@@ -131,9 +127,7 @@ class AgentEventIntegrator:
         except Exception as e:
             # Send error event
             duration = time.time() - start_time
-            agent_type = (
-                self.agent_id.split(":")[0] if ":" in self.agent_id else "unknown"
-            )
+            agent_type = self.agent_id.split(":")[0] if ":" in self.agent_id else "unknown"
             error_event = create_agent_status_event(
                 agent_id=self.agent_id,
                 agent_type=agent_type,
@@ -198,8 +192,7 @@ class AgentEventIntegrator:
             operation_type=operation["type"],
             stage=feedback_data.get("stage", "processing"),
             message=feedback_data.get("message", "Processing..."),
-            progress_percentage=feedback_data.get("progress", 0.0)
-            * 100,  # Convert to percentage
+            progress_percentage=feedback_data.get("progress", 0.0) * 100,  # Convert to percentage
             intermediate_result=feedback_data.get("intermediate_result"),
             estimated_remaining=feedback_data.get("estimated_remaining"),
             user_id=feedback_data.get("user_id"),
@@ -235,9 +228,7 @@ class AgentEventIntegrator:
 class WorkflowEventIntegrator:
     """Integrates workflow operations with real-time event publishing."""
 
-    def __init__(
-        self, event_publisher: EventPublisher | None = None, enabled: bool = True
-    ):
+    def __init__(self, event_publisher: EventPublisher | None = None, enabled: bool = True):
         self.event_publisher = event_publisher
         self.enabled = enabled and event_publisher is not None
 
@@ -296,8 +287,7 @@ class WorkflowEventIntegrator:
                 "advance_step": lambda message="": self._advance_workflow_step(
                     workflow_id, message
                 ),
-                "update_progress": lambda progress,
-                message="": self._update_workflow_progress(
+                "update_progress": lambda progress, message="": self._update_workflow_progress(
                     workflow_id, progress, message
                 ),
                 "add_metadata": lambda key, value: self._add_workflow_metadata(
@@ -348,9 +338,7 @@ class WorkflowEventIntegrator:
         workflow["current_step"] += 1
 
         progress = workflow["current_step"] / max(workflow["total_steps"], 1)
-        step_message = (
-            message or f"Step {workflow['current_step']}/{workflow['total_steps']}"
-        )
+        step_message = message or f"Step {workflow['current_step']}/{workflow['total_steps']}"
 
         return await self._update_workflow_progress(workflow_id, progress, step_message)
 
@@ -503,9 +491,7 @@ class AgentWorkflowCoordinator:
 
             nga_result = await self.nga_proxy.process(nga_input)
 
-            await workflow["add_metadata"](
-                "story_generated", bool(nga_result.get("story"))
-            )
+            await workflow["add_metadata"]("story_generated", bool(nga_result.get("story")))
 
             # Compile final result
             return {
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/config_manager.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/config_manager.py
index 7afdef2b5..ff9409672 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/config_manager.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/config_manager.py
@@ -116,9 +116,7 @@ class RealtimeConfigManager:
         try:
             return RealtimeEnvironment(env_name)
         except ValueError:
-            logger.warning(
-                f"Unknown environment '{env_name}', defaulting to development"
-            )
+            logger.warning(f"Unknown environment '{env_name}', defaulting to development")
             return RealtimeEnvironment.DEVELOPMENT
 
     def load_config(self) -> RealtimeConfig:
@@ -127,24 +125,18 @@ class RealtimeConfigManager:
             return self._config
 
         # Extract real-time config from base config
-        realtime_config = self.base_config.get("agent_orchestration", {}).get(
-            "realtime", {}
-        )
+        realtime_config = self.base_config.get("agent_orchestration", {}).get("realtime", {})
 
         # Create configuration with environment-specific defaults
         config = RealtimeConfig(
-            enabled=self._get_bool_config(
-                "enabled", realtime_config, self._get_default_enabled()
-            ),
+            enabled=self._get_bool_config("enabled", realtime_config, self._get_default_enabled()),
             environment=self._environment,
             websocket=self._load_websocket_config(realtime_config.get("websocket", {})),
             events=self._load_event_config(realtime_config.get("events", {})),
             progressive_feedback=self._load_progressive_feedback_config(
                 realtime_config.get("progressive_feedback", {})
             ),
-            optimization=self._load_optimization_config(
-                realtime_config.get("optimization", {})
-            ),
+            optimization=self._load_optimization_config(realtime_config.get("optimization", {})),
         )
 
         # Validate configuration
@@ -154,9 +146,7 @@ class RealtimeConfigManager:
         self._apply_feature_flags(config)
 
         self._config = config
-        logger.info(
-            f"Real-time configuration loaded for environment: {self._environment}"
-        )
+        logger.info(f"Real-time configuration loaded for environment: {self._environment}")
         return config
 
     def _get_default_enabled(self) -> bool:
@@ -171,9 +161,7 @@ class RealtimeConfigManager:
     def _load_websocket_config(self, ws_config: dict[str, Any]) -> WebSocketConfig:
         """Load WebSocket configuration."""
         return WebSocketConfig(
-            enabled=self._get_bool_config(
-                "enabled", ws_config, self._get_default_enabled()
-            ),
+            enabled=self._get_bool_config("enabled", ws_config, self._get_default_enabled()),
             path=ws_config.get("path", "/ws"),
             heartbeat_interval=float(ws_config.get("heartbeat_interval", 30.0)),
             connection_timeout=float(ws_config.get("connection_timeout", 60.0)),
@@ -185,9 +173,7 @@ class RealtimeConfigManager:
     def _load_event_config(self, event_config: dict[str, Any]) -> EventConfig:
         """Load event configuration."""
         return EventConfig(
-            enabled=self._get_bool_config(
-                "enabled", event_config, self._get_default_enabled()
-            ),
+            enabled=self._get_bool_config("enabled", event_config, self._get_default_enabled()),
             redis_channel_prefix=event_config.get("redis_channel_prefix", "ao:events"),
             buffer_size=int(event_config.get("buffer_size", 1000)),
             broadcast_agent_status=self._get_bool_config(
@@ -207,13 +193,9 @@ class RealtimeConfigManager:
     ) -> ProgressiveFeedbackConfig:
         """Load progressive feedback configuration."""
         return ProgressiveFeedbackConfig(
-            enabled=self._get_bool_config(
-                "enabled", pf_config, self._get_default_enabled()
-            ),
+            enabled=self._get_bool_config("enabled", pf_config, self._get_default_enabled()),
             update_interval=float(pf_config.get("update_interval", 1.0)),
-            max_updates_per_workflow=int(
-                pf_config.get("max_updates_per_workflow", 100)
-            ),
+            max_updates_per_workflow=int(pf_config.get("max_updates_per_workflow", 100)),
             stream_intermediate_results=self._get_bool_config(
                 "stream_intermediate_results", pf_config, True
             ),
@@ -221,24 +203,18 @@ class RealtimeConfigManager:
             batch_size=int(pf_config.get("batch_size", 10)),
         )
 
-    def _load_optimization_config(
-        self, opt_config: dict[str, Any]
-    ) -> OptimizationConfig:
+    def _load_optimization_config(self, opt_config: dict[str, Any]) -> OptimizationConfig:
         """Load optimization configuration."""
         return OptimizationConfig(
             enabled=self._get_bool_config("enabled", opt_config, False),
             response_time_monitoring=self._get_bool_config(
                 "response_time_monitoring", opt_config, True
             ),
-            statistical_analysis=self._get_bool_config(
-                "statistical_analysis", opt_config, True
-            ),
+            statistical_analysis=self._get_bool_config("statistical_analysis", opt_config, True),
             auto_parameter_adjustment=self._get_bool_config(
                 "auto_parameter_adjustment", opt_config, False
             ),
-            speed_creativity_balance=float(
-                opt_config.get("speed_creativity_balance", 0.5)
-            ),
+            speed_creativity_balance=float(opt_config.get("speed_creativity_balance", 0.5)),
         )
 
     def _get_bool_config(self, key: str, config: dict[str, Any], default: bool) -> bool:
@@ -274,32 +250,23 @@ class RealtimeConfigManager:
             errors.append("Event retention_hours must be positive")
 
         # Progressive feedback validation
-        if (
-            config.progressive_feedback.enabled
-            and config.progressive_feedback.update_interval <= 0
-        ):
+        if config.progressive_feedback.enabled and config.progressive_feedback.update_interval <= 0:
             errors.append("Progressive feedback update_interval must be positive")
 
         if (
             config.progressive_feedback.enabled
             and config.progressive_feedback.max_updates_per_workflow <= 0
         ):
-            errors.append(
-                "Progressive feedback max_updates_per_workflow must be positive"
-            )
+            errors.append("Progressive feedback max_updates_per_workflow must be positive")
 
         # Optimization validation
         if config.optimization.enabled:
             if not (0.0 <= config.optimization.speed_creativity_balance <= 1.0):
-                errors.append(
-                    "Optimization speed_creativity_balance must be between 0.0 and 1.0"
-                )
+                errors.append("Optimization speed_creativity_balance must be between 0.0 and 1.0")
 
         # Dependency validation
         if config.websocket.enabled and not config.events.enabled:
-            logger.warning(
-                "WebSocket enabled but events disabled - limited functionality"
-            )
+            logger.warning("WebSocket enabled but events disabled - limited functionality")
 
         if config.progressive_feedback.enabled and not config.events.enabled:
             errors.append("Progressive feedback requires events to be enabled")
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/dashboard.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/dashboard.py
index 354f2dfde..c0f77f431 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/dashboard.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/dashboard.py
@@ -89,9 +89,7 @@ class RealtimeDashboardManager:
         if ALERTING_AVAILABLE:
             try:
                 self.alert_manager = get_alert_manager()
-                self.alert_manager.add_notification_handler(
-                    self._handle_alert_notification
-                )
+                self.alert_manager.add_notification_handler(self._handle_alert_notification)
             except Exception as e:
                 logger.warning(f"Could not initialize alert manager: {e}")
 
@@ -149,9 +147,7 @@ class RealtimeDashboardManager:
         for dashboard_type in dashboard_types:
             await self._send_dashboard_snapshot(connection_id, dashboard_type)
 
-        logger.debug(
-            f"Connection {connection_id} subscribed to dashboards: {dashboard_types}"
-        )
+        logger.debug(f"Connection {connection_id} subscribed to dashboards: {dashboard_types}")
         return True
 
     async def unsubscribe_from_dashboard(
@@ -239,9 +235,7 @@ class RealtimeDashboardManager:
                 "uptime": metrics.get("uptime", 0.0),
                 "health_status": self._determine_health_status(metrics),
             },
-            metadata={
-                "update_count": self.update_counts.get(DashboardType.SYSTEM_HEALTH, 0)
-            },
+            metadata={"update_count": self.update_counts.get(DashboardType.SYSTEM_HEALTH, 0)},
         )
 
         await self._store_and_broadcast_data(dashboard_data)
@@ -271,9 +265,7 @@ class RealtimeDashboardManager:
                     }
                     for agent_id, metrics in agent_metrics.items()
                 },
-                "performance_summary": self._calculate_performance_summary(
-                    agent_metrics
-                ),
+                "performance_summary": self._calculate_performance_summary(agent_metrics),
             },
         )
 
@@ -436,9 +428,7 @@ class RealtimeDashboardManager:
                 "avg_error_rate": 0.0,
             }
 
-        response_times = [
-            m.get("response_time_avg", 0.0) for m in agent_metrics.values()
-        ]
+        response_times = [m.get("response_time_avg", 0.0) for m in agent_metrics.values()]
         throughputs = [m.get("throughput_rps", 0.0) for m in agent_metrics.values()]
         error_rates = [m.get("error_rate", 0.0) for m in agent_metrics.values()]
 
@@ -466,9 +456,7 @@ class RealtimeDashboardManager:
                     "value": alert.value,
                     "threshold": alert.threshold,
                     "started_at": alert.started_at.isoformat(),
-                    "resolved_at": (
-                        alert.resolved_at.isoformat() if alert.resolved_at else None
-                    ),
+                    "resolved_at": (alert.resolved_at.isoformat() if alert.resolved_at else None),
                     "labels": alert.labels,
                     "annotations": alert.annotations,
                 },
@@ -517,9 +505,7 @@ class RealtimeDashboardManager:
                         for alert in active_alerts[:10]  # Limit to 10 most recent
                     ],
                 },
-                metadata={
-                    "update_count": self.update_counts.get(DashboardType.ALERTS, 0)
-                },
+                metadata={"update_count": self.update_counts.get(DashboardType.ALERTS, 0)},
             )
 
             self.dashboard_data[DashboardType.ALERTS].append(dashboard_data)
@@ -549,8 +535,7 @@ class RealtimeDashboardManager:
             )
 
         status["update_counts"] = {
-            dashboard_type.value: count
-            for dashboard_type, count in self.update_counts.items()
+            dashboard_type.value: count for dashboard_type, count in self.update_counts.items()
         }
         status["last_update_times"] = {
             dashboard_type.value: timestamp
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/error_reporting.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/error_reporting.py
index fcb69250c..83a18e336 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/error_reporting.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/error_reporting.py
@@ -219,15 +219,11 @@ class ErrorReportingManager:
             # Check if we should abandon recovery
             if error_report.recovery_attempts >= self.max_recovery_attempts:
                 error_report.recovery_status = RecoveryStatus.ABANDONED
-                await self._broadcast_recovery_event(
-                    error_report, success=False, abandoned=True
-                )
+                await self._broadcast_recovery_event(error_report, success=False, abandoned=True)
             else:
                 await self._broadcast_recovery_event(error_report, success=False)
 
-        logger.info(
-            f"Recovery attempt for {error_id}: {recovery_message}, success: {success}"
-        )
+        logger.info(f"Recovery attempt for {error_id}: {recovery_message}, success: {success}")
         return True
 
     async def _broadcast_error_event(self, error_report: ErrorReport) -> None:
@@ -361,17 +357,14 @@ class ErrorReportingManager:
 
                 # Clean up old errors from history
                 self.error_history = [
-                    error
-                    for error in self.error_history
-                    if error.timestamp > retention_cutoff
+                    error for error in self.error_history if error.timestamp > retention_cutoff
                 ]
 
                 # Clean up resolved errors from active list
                 resolved_errors = [
                     error_id
                     for error_id, error in self.active_errors.items()
-                    if error.recovery_success
-                    or error.recovery_status == RecoveryStatus.ABANDONED
+                    if error.recovery_success or error.recovery_status == RecoveryStatus.ABANDONED
                 ]
 
                 for error_id in resolved_errors:
@@ -395,8 +388,7 @@ class ErrorReportingManager:
                     if (
                         current_time - error_report.timestamp > self.escalation_timeout
                         and error_report.escalation_level == 0
-                        and error_report.severity
-                        in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]
+                        and error_report.severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]
                     ):
                         error_report.escalation_level = 1
                         await self._escalate_error(error_report)
@@ -410,9 +402,7 @@ class ErrorReportingManager:
 
     async def _escalate_error(self, error_report: ErrorReport) -> None:
         """Escalate an unresolved error."""
-        logger.warning(
-            f"Escalating error {error_report.error_id}: {error_report.error_message}"
-        )
+        logger.warning(f"Escalating error {error_report.error_id}: {error_report.error_message}")
 
         # Send escalation notifications
         for handler in self.notification_handlers:
@@ -459,9 +449,7 @@ class ErrorReportingManager:
 
     def _get_recovery_success_rate(self) -> float:
         """Get recovery success rate."""
-        attempted_recoveries = [
-            e for e in self.error_history if e.recovery_attempts > 0
-        ]
+        attempted_recoveries = [e for e in self.error_history if e.recovery_attempts > 0]
         if not attempted_recoveries:
             return 0.0
 
@@ -470,9 +458,7 @@ class ErrorReportingManager:
 
     def _get_average_recovery_attempts(self) -> float:
         """Get average number of recovery attempts."""
-        attempted_recoveries = [
-            e for e in self.error_history if e.recovery_attempts > 0
-        ]
+        attempted_recoveries = [e for e in self.error_history if e.recovery_attempts > 0]
         if not attempted_recoveries:
             return 0.0
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_publisher.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_publisher.py
index 461bd5f0c..fdbf1b209 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_publisher.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_publisher.py
@@ -74,9 +74,7 @@ class EventPublisher:
     def remove_websocket_manager(self, manager: Any) -> None:
         """Remove a WebSocket manager."""
         self.websocket_managers.discard(manager)
-        logger.debug(
-            f"Removed WebSocket manager: {len(self.websocket_managers)} remaining"
-        )
+        logger.debug(f"Removed WebSocket manager: {len(self.websocket_managers)} remaining")
 
     async def publish_agent_status_event(
         self,
@@ -330,9 +328,7 @@ class EventPublisher:
 
         # Create tasks for concurrent broadcasting
         tasks = []
-        for manager in list(
-            self.websocket_managers
-        ):  # Copy to avoid modification during iteration
+        for manager in list(self.websocket_managers):  # Copy to avoid modification during iteration
             task = asyncio.create_task(self._broadcast_to_manager(manager, event))
             tasks.append(task)
 
@@ -341,9 +337,7 @@ class EventPublisher:
             results = await asyncio.gather(*tasks, return_exceptions=True)
             success_count = sum(1 for result in results if result is True)
 
-        logger.debug(
-            f"Broadcasted event to {success_count}/{total_count} WebSocket managers"
-        )
+        logger.debug(f"Broadcasted event to {success_count}/{total_count} WebSocket managers")
         return success_count == total_count
 
     async def _broadcast_to_manager(self, manager: Any, event: WebSocketEvent) -> bool:
@@ -379,9 +373,7 @@ class EventPublisher:
     async def get_recent_events(self, count: int = 10) -> list[dict[str, Any]]:
         """Get recent events from the buffer."""
         async with self.buffer_lock:
-            recent_events = (
-                self.event_buffer[-count:] if count > 0 else self.event_buffer[:]
-            )
+            recent_events = self.event_buffer[-count:] if count > 0 else self.event_buffer[:]
             return [event.model_dump() for event in recent_events]
 
     async def shutdown(self) -> None:
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_subscriber.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_subscriber.py
index 74d670630..f67b6b6bb 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_subscriber.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/event_subscriber.py
@@ -82,9 +82,7 @@ class EventSubscriber:
 
         logger.info(f"EventSubscriber stopped: {self.subscriber_id}")
 
-    async def subscribe_to_all_events(
-        self, handler: Callable[[WebSocketEvent], None]
-    ) -> None:
+    async def subscribe_to_all_events(self, handler: Callable[[WebSocketEvent], None]) -> None:
         """Subscribe to all events."""
         await self.subscribe_to_channel(f"{self.channel_prefix}:all", handler)
 
@@ -92,25 +90,19 @@ class EventSubscriber:
         self, event_type: EventType, handler: Callable[[WebSocketEvent], None]
     ) -> None:
         """Subscribe to a specific event type."""
-        await self.subscribe_to_channel(
-            f"{self.channel_prefix}:{event_type.value}", handler
-        )
+        await self.subscribe_to_channel(f"{self.channel_prefix}:{event_type.value}", handler)
 
     async def subscribe_to_agent_events(
         self, agent_id: str, handler: Callable[[WebSocketEvent], None]
     ) -> None:
         """Subscribe to events for a specific agent."""
-        await self.subscribe_to_channel(
-            f"{self.channel_prefix}:agent:{agent_id}", handler
-        )
+        await self.subscribe_to_channel(f"{self.channel_prefix}:agent:{agent_id}", handler)
 
     async def subscribe_to_user_events(
         self, user_id: str, handler: Callable[[WebSocketEvent], None]
     ) -> None:
         """Subscribe to events for a specific user."""
-        await self.subscribe_to_channel(
-            f"{self.channel_prefix}:user:{user_id}", handler
-        )
+        await self.subscribe_to_channel(f"{self.channel_prefix}:user:{user_id}", handler)
 
     async def subscribe_to_channel(
         self, channel: str, handler: Callable[[WebSocketEvent], None]
@@ -173,7 +165,7 @@ class EventSubscriber:
                     # Process the message
                     await self._process_message(message)
 
-                except asyncio.TimeoutError:
+                except TimeoutError:
                     continue
                 except Exception as e:
                     logger.error(f"Error in subscription loop: {e}")
@@ -226,9 +218,7 @@ class EventSubscriber:
             # Wait for all handlers to complete
             if handler_tasks:
                 results = await asyncio.gather(*handler_tasks, return_exceptions=True)
-                success_count = sum(
-                    1 for result in results if not isinstance(result, Exception)
-                )
+                success_count = sum(1 for result in results if not isinstance(result, Exception))
 
                 if success_count > 0:
                     self.events_processed += 1
@@ -236,9 +226,7 @@ class EventSubscriber:
                 # Log any handler errors
                 for i, result in enumerate(results):
                     if isinstance(result, Exception):
-                        logger.error(
-                            f"Handler {i} failed for channel {channel}: {result}"
-                        )
+                        logger.error(f"Handler {i} failed for channel {channel}: {result}")
                         self.events_failed += 1
 
         except Exception as e:
@@ -267,9 +255,7 @@ class EventSubscriber:
             "events_processed": self.events_processed,
             "events_failed": self.events_failed,
             "active_subscriptions": len(self.subscriptions),
-            "total_handlers": sum(
-                len(handlers) for handlers in self.subscriptions.values()
-            ),
+            "total_handlers": sum(len(handlers) for handlers in self.subscriptions.values()),
             "subscribed_channels": list(self.subscriptions.keys()),
         }
 
@@ -316,9 +302,7 @@ class EventDistributor:
     def remove_websocket_manager(self, manager: Any) -> None:
         """Remove a WebSocket manager."""
         self.websocket_managers.discard(manager)
-        logger.debug(
-            f"Removed WebSocket manager: {len(self.websocket_managers)} remaining"
-        )
+        logger.debug(f"Removed WebSocket manager: {len(self.websocket_managers)} remaining")
 
     async def start_all_subscribers(self) -> None:
         """Start all registered subscribers."""
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/message_workflow_integration.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/message_workflow_integration.py
index 7f822c0b0..af52f3059 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/message_workflow_integration.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/message_workflow_integration.py
@@ -47,9 +47,7 @@ class WorkflowAwareMessageCoordinator:
         self.workflow_agents: dict[str, set[AgentId]] = {}  # workflow_id -> agent_ids
 
         # Message processing callbacks
-        self.message_callbacks: dict[
-            str, set[Callable]
-        ] = {}  # workflow_id -> callbacks
+        self.message_callbacks: dict[str, set[Callable]] = {}  # workflow_id -> callbacks
 
         logger.info("WorkflowAwareMessageCoordinator initialized")
 
@@ -126,9 +124,7 @@ class WorkflowAwareMessageCoordinator:
 
         # Track workflow association if enabled
         if self.track_message_workflows and workflow_id and result.delivered:
-            await self._track_workflow_message(
-                workflow_id, message.message_id, sender, recipient
-            )
+            await self._track_workflow_message(workflow_id, message.message_id, sender, recipient)
 
         return result
 
@@ -141,9 +137,7 @@ class WorkflowAwareMessageCoordinator:
     ) -> list[MessageResult]:
         """Broadcast a message with workflow tracking."""
         # Send messages through existing coordinator
-        results = await self.redis_coordinator.broadcast_message(
-            sender, message, recipients
-        )
+        results = await self.redis_coordinator.broadcast_message(sender, message, recipients)
 
         # Track workflow associations if enabled
         if self.track_message_workflows and workflow_id:
@@ -162,15 +156,11 @@ class WorkflowAwareMessageCoordinator:
     ) -> ReceivedMessage | None:
         """Receive a message with workflow progress tracking."""
         # Receive message through existing coordinator
-        received_message = await self.redis_coordinator.receive(
-            agent_id, visibility_timeout
-        )
+        received_message = await self.redis_coordinator.receive(agent_id, visibility_timeout)
 
         if received_message and self.track_message_workflows:
             # Check if this message is part of a workflow
-            workflow_id = self.message_to_workflow.get(
-                received_message.message.message_id
-            )
+            workflow_id = self.message_to_workflow.get(received_message.message.message_id)
             if workflow_id:
                 await self._update_workflow_on_message_received(
                     workflow_id, received_message, agent_id
@@ -256,9 +246,7 @@ class WorkflowAwareMessageCoordinator:
             for message_id in message_ids:
                 self.message_to_workflow.pop(message_id, None)
 
-            logger.info(
-                f"Completed workflow: {workflow_id} ({'success' if success else 'failed'})"
-            )
+            logger.info(f"Completed workflow: {workflow_id} ({'success' if success else 'failed'})")
             return True
 
         except Exception as e:
@@ -367,9 +355,7 @@ class WorkflowAwareMessageCoordinator:
             {
                 "message_info": message_info,
                 "agent_id": (
-                    agent_id.model_dump()
-                    if hasattr(agent_id, "model_dump")
-                    else str(agent_id)
+                    agent_id.model_dump() if hasattr(agent_id, "model_dump") else str(agent_id)
                 ),
                 "workflow_result": workflow_result,
             },
@@ -403,9 +389,7 @@ class WorkflowAwareMessageCoordinator:
             {
                 "message_info": message_info,
                 "agent_id": (
-                    agent_id.model_dump()
-                    if hasattr(agent_id, "model_dump")
-                    else str(agent_id)
+                    agent_id.model_dump() if hasattr(agent_id, "model_dump") else str(agent_id)
                 ),
                 "failure_type": failure.value,
                 "error": error,
@@ -425,9 +409,7 @@ class WorkflowAwareMessageCoordinator:
             if payload:
                 import json
 
-                data = json.loads(
-                    payload if isinstance(payload, str) else payload.decode()
-                )
+                data = json.loads(payload if isinstance(payload, str) else payload.decode())
                 return {
                     "message_id": data.get("message", {}).get("message_id"),
                     "message_type": data.get("message", {}).get("message_type"),
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/models.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/models.py
index f40a5d7b5..1fa1c5062 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/models.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/models.py
@@ -62,9 +62,7 @@ class WebSocketEvent(BaseModel):
         default_factory=time.time, description="Unix timestamp when event was created"
     )
     source: str = Field(..., description="Source component that generated the event")
-    data: dict[str, Any] = Field(
-        default_factory=dict, description="Event-specific data payload"
-    )
+    data: dict[str, Any] = Field(default_factory=dict, description="Event-specific data payload")
 
     model_config = ConfigDict(use_enum_values=True)
 
@@ -77,23 +75,15 @@ class AgentStatusEvent(WebSocketEvent):
     agent_type: str = Field(..., description="Type of agent (ipa, wba, nga)")
     instance: str | None = Field(None, description="Agent instance identifier")
     status: AgentStatus = Field(..., description="Current agent status")
-    previous_status: AgentStatus | None = Field(
-        None, description="Previous agent status"
-    )
-    heartbeat_age: float | None = Field(
-        None, description="Age of last heartbeat in seconds"
-    )
-    metadata: dict[str, Any] = Field(
-        default_factory=dict, description="Additional agent metadata"
-    )
+    previous_status: AgentStatus | None = Field(None, description="Previous agent status")
+    heartbeat_age: float | None = Field(None, description="Age of last heartbeat in seconds")
+    metadata: dict[str, Any] = Field(default_factory=dict, description="Additional agent metadata")
 
 
 class WorkflowProgressEvent(WebSocketEvent):
     """Event for workflow progress updates."""
 
-    event_type: Literal[EventType.WORKFLOW_PROGRESS] = Field(
-        default=EventType.WORKFLOW_PROGRESS
-    )
+    event_type: Literal[EventType.WORKFLOW_PROGRESS] = Field(default=EventType.WORKFLOW_PROGRESS)
     workflow_id: str = Field(..., description="Workflow identifier")
     workflow_type: str = Field(..., description="Type of workflow")
     status: WorkflowStatus = Field(..., description="Current workflow status")
@@ -103,40 +93,26 @@ class WorkflowProgressEvent(WebSocketEvent):
     current_step: str | None = Field(None, description="Current workflow step")
     total_steps: int | None = Field(None, description="Total number of steps")
     completed_steps: int | None = Field(None, description="Number of completed steps")
-    estimated_completion: float | None = Field(
-        None, description="Estimated completion timestamp"
-    )
+    estimated_completion: float | None = Field(None, description="Estimated completion timestamp")
     user_id: str | None = Field(None, description="User associated with the workflow")
 
 
 class SystemMetricsEvent(WebSocketEvent):
     """Event for system performance metrics."""
 
-    event_type: Literal[EventType.SYSTEM_METRICS] = Field(
-        default=EventType.SYSTEM_METRICS
-    )
-    cpu_usage: float | None = Field(
-        None, ge=0.0, le=100.0, description="CPU usage percentage"
-    )
+    event_type: Literal[EventType.SYSTEM_METRICS] = Field(default=EventType.SYSTEM_METRICS)
+    cpu_usage: float | None = Field(None, ge=0.0, le=100.0, description="CPU usage percentage")
     memory_usage: float | None = Field(
         None, ge=0.0, le=100.0, description="Memory usage percentage"
     )
-    memory_usage_mb: float | None = Field(
-        None, ge=0.0, description="Memory usage in MB"
-    )
-    active_connections: int | None = Field(
-        None, ge=0, description="Number of active connections"
-    )
-    active_workflows: int | None = Field(
-        None, ge=0, description="Number of active workflows"
-    )
+    memory_usage_mb: float | None = Field(None, ge=0.0, description="Memory usage in MB")
+    active_connections: int | None = Field(None, ge=0, description="Number of active connections")
+    active_workflows: int | None = Field(None, ge=0, description="Number of active workflows")
     message_queue_size: int | None = Field(None, ge=0, description="Message queue size")
     response_time_avg: float | None = Field(
         None, ge=0.0, description="Average response time in seconds"
     )
-    error_rate: float | None = Field(
-        None, ge=0.0, le=100.0, description="Error rate percentage"
-    )
+    error_rate: float | None = Field(None, ge=0.0, le=100.0, description="Error rate percentage")
 
 
 class ProgressiveFeedbackEvent(WebSocketEvent):
@@ -149,9 +125,7 @@ class ProgressiveFeedbackEvent(WebSocketEvent):
     operation_type: str = Field(..., description="Type of operation")
     stage: str = Field(..., description="Current operation stage")
     message: str = Field(..., description="Human-readable progress message")
-    progress_percentage: float = Field(
-        ..., ge=0.0, le=100.0, description="Progress percentage"
-    )
+    progress_percentage: float = Field(..., ge=0.0, le=100.0, description="Progress percentage")
     intermediate_result: dict[str, Any] | None = Field(
         None, description="Intermediate operation result"
     )
@@ -179,17 +153,11 @@ class OptimizationEvent(WebSocketEvent):
 class ConnectionStatusEvent(WebSocketEvent):
     """Event for WebSocket connection status changes."""
 
-    event_type: Literal[EventType.CONNECTION_STATUS] = Field(
-        default=EventType.CONNECTION_STATUS
-    )
+    event_type: Literal[EventType.CONNECTION_STATUS] = Field(default=EventType.CONNECTION_STATUS)
     connection_id: str = Field(..., description="Connection identifier")
-    status: str = Field(
-        ..., description="Connection status (connected, disconnected, error)"
-    )
+    status: str = Field(..., description="Connection status (connected, disconnected, error)")
     user_id: str | None = Field(None, description="User associated with the connection")
-    client_info: dict[str, Any] = Field(
-        default_factory=dict, description="Client information"
-    )
+    client_info: dict[str, Any] = Field(default_factory=dict, description="Client information")
 
 
 class ErrorEvent(WebSocketEvent):
@@ -204,9 +172,7 @@ class ErrorEvent(WebSocketEvent):
     severity: str = Field(
         default="error", description="Error severity (info, warning, error, critical)"
     )
-    component: str | None = Field(
-        None, description="Component that generated the error"
-    )
+    component: str | None = Field(None, description="Component that generated the error")
 
 
 class HeartbeatEvent(WebSocketEvent):
@@ -214,18 +180,14 @@ class HeartbeatEvent(WebSocketEvent):
 
     event_type: Literal[EventType.HEARTBEAT] = Field(default=EventType.HEARTBEAT)
     connection_id: str = Field(..., description="Connection identifier")
-    server_timestamp: float = Field(
-        default_factory=time.time, description="Server timestamp"
-    )
+    server_timestamp: float = Field(default_factory=time.time, description="Server timestamp")
 
 
 # Event subscription models
 class EventSubscription(BaseModel):
     """Model for event subscription requests."""
 
-    event_types: list[EventType] = Field(
-        ..., description="List of event types to subscribe to"
-    )
+    event_types: list[EventType] = Field(..., description="List of event types to subscribe to")
     filters: dict[str, Any] = Field(default_factory=dict, description="Event filters")
     user_id: str | None = Field(None, description="User ID for user-specific events")
 
@@ -234,13 +196,9 @@ class EventFilter(BaseModel):
     """Model for event filtering criteria."""
 
     agent_types: list[str] | None = Field(None, description="Filter by agent types")
-    workflow_types: list[str] | None = Field(
-        None, description="Filter by workflow types"
-    )
+    workflow_types: list[str] | None = Field(None, description="Filter by workflow types")
     user_ids: list[str] | None = Field(None, description="Filter by user IDs")
-    severity_levels: list[str] | None = Field(
-        None, description="Filter by error severity levels"
-    )
+    severity_levels: list[str] | None = Field(None, description="Filter by error severity levels")
     min_progress: float | None = Field(
         None, ge=0.0, le=100.0, description="Minimum progress percentage"
     )
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/monitoring_integration.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/monitoring_integration.py
index 90cd03535..f87d5d95d 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/monitoring_integration.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/monitoring_integration.py
@@ -61,9 +61,7 @@ class MonitoringEventIntegrator:
         self.event_publisher = event_publisher
         self.system_monitor = system_monitor
         self.alert_manager = alert_manager
-        self.response_time_monitor = (
-            response_time_monitor or get_response_time_monitor()
-        )
+        self.response_time_monitor = response_time_monitor or get_response_time_monitor()
         self.performance_analytics = performance_analytics
         self.performance_alerting = performance_alerting
         self.config = config or MonitoringConfig()
@@ -79,9 +77,7 @@ class MonitoringEventIntegrator:
         # Alert handlers
         self.alert_handlers: list[Callable] = []
 
-        logger.info(
-            f"MonitoringEventIntegrator initialized, enabled: {self.config.enabled}"
-        )
+        logger.info(f"MonitoringEventIntegrator initialized, enabled: {self.config.enabled}")
 
     async def start(self) -> None:
         """Start monitoring integration."""
@@ -89,9 +85,7 @@ class MonitoringEventIntegrator:
             return
 
         if not self.event_publisher:
-            logger.warning(
-                "No event publisher available, monitoring integration disabled"
-            )
+            logger.warning("No event publisher available, monitoring integration disabled")
             return
 
         self.is_running = True
@@ -131,10 +125,7 @@ class MonitoringEventIntegrator:
                 current_time = time.time()
 
                 # Check if it's time to broadcast metrics
-                if (
-                    current_time - self.last_metrics_time
-                    >= self.config.metrics_interval
-                ):
+                if current_time - self.last_metrics_time >= self.config.metrics_interval:
                     await self._broadcast_system_metrics()
                     await self._broadcast_agent_metrics()
                     await self._broadcast_performance_metrics()
@@ -307,9 +298,7 @@ class MonitoringEventIntegrator:
                     ),
                     "sla_compliance": performance_summary.get("sla_compliance", 0.0),
                     "operation_types": performance_summary.get("operation_types", 0),
-                    "statistics_by_type": performance_summary.get(
-                        "statistics_by_type", {}
-                    ),
+                    "statistics_by_type": performance_summary.get("statistics_by_type", {}),
                 },
             )
 
@@ -375,9 +364,7 @@ class MonitoringEventIntegrator:
         except Exception as e:
             logger.error(f"Failed to broadcast performance alert: {e}")
 
-    async def broadcast_performance_analysis(
-        self, analysis_results: dict[str, Any]
-    ) -> None:
+    async def broadcast_performance_analysis(self, analysis_results: dict[str, Any]) -> None:
         """Broadcast performance analysis results."""
         if not self.event_publisher:
             return
@@ -391,15 +378,11 @@ class MonitoringEventIntegrator:
                 queue_depth=0,
                 metadata={
                     "performance_analysis": True,
-                    "analysis_timestamp": analysis_results.get(
-                        "analysis_timestamp", time.time()
-                    ),
+                    "analysis_timestamp": analysis_results.get("analysis_timestamp", time.time()),
                     "overall_health": analysis_results.get("overall_health", "unknown"),
                     "bottlenecks_count": len(analysis_results.get("bottlenecks", [])),
                     "trends_count": len(analysis_results.get("trends", [])),
-                    "recommendations_count": len(
-                        analysis_results.get("recommendations", [])
-                    ),
+                    "recommendations_count": len(analysis_results.get("recommendations", [])),
                     "bottlenecks": analysis_results.get("bottlenecks", []),
                     "trends": analysis_results.get("trends", []),
                     "recommendations": analysis_results.get("recommendations", []),
@@ -448,9 +431,7 @@ class MonitoringEventIntegrator:
                         "current_value": current_value,
                         "threshold_value": threshold_value,
                         "breach_ratio": (
-                            current_value / threshold_value
-                            if threshold_value > 0
-                            else 0
+                            current_value / threshold_value if threshold_value > 0 else 0
                         ),
                     },
                     source="performance_monitor",
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/progressive_feedback.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/progressive_feedback.py
index afe7edff1..df2cb4bcc 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/progressive_feedback.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/progressive_feedback.py
@@ -255,9 +255,7 @@ class ProgressiveFeedbackManager:
 
         # Update final status
         operation.status = "completed" if success else "failed"
-        operation.progress_percentage = (
-            100.0 if success else operation.progress_percentage
-        )
+        operation.progress_percentage = 100.0 if success else operation.progress_percentage
 
         if final_result:
             operation.intermediate_results.update(final_result)
@@ -273,9 +271,7 @@ class ProgressiveFeedbackManager:
         self.operation_update_counts.pop(operation_id, None)
         self.operation_callbacks.pop(operation_id, None)
 
-        logger.info(
-            f"Completed operation: {operation_id} ({'success' if success else 'failed'})"
-        )
+        logger.info(f"Completed operation: {operation_id} ({'success' if success else 'failed'})")
         return True
 
     async def fail_operation(
@@ -335,9 +331,7 @@ class ProgressiveFeedbackManager:
 
         return operations
 
-    async def _send_progress_event(
-        self, operation: OperationProgress, message: str
-    ) -> None:
+    async def _send_progress_event(self, operation: OperationProgress, message: str) -> None:
         """Send a progressive feedback event."""
         if not self.event_publisher:
             return
@@ -429,9 +423,7 @@ class ProgressiveFeedbackManager:
         """Get count of operations by type."""
         counts = {}
         for operation in self.active_operations.values():
-            counts[operation.operation_type] = (
-                counts.get(operation.operation_type, 0) + 1
-            )
+            counts[operation.operation_type] = counts.get(operation.operation_type, 0) + 1
         return counts
 
     def _get_operations_by_user(self) -> dict[str, int]:
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/streaming_response.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/streaming_response.py
index 95654068e..7c0382470 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/streaming_response.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/streaming_response.py
@@ -75,9 +75,7 @@ class StreamingWorkflowResponse:
 
         logger.info(f"Started streaming for workflow: {self.workflow_id}")
 
-    async def send_chunk(
-        self, data: str | dict[str, Any], chunk_type: str = "data"
-    ) -> None:
+    async def send_chunk(self, data: str | dict[str, Any], chunk_type: str = "data") -> None:
         """Send a data chunk to the stream."""
         if not self.is_streaming:
             return
@@ -212,7 +210,7 @@ class StreamingWorkflowResponse:
                     if self.completion_event.is_set() or self.error_event.is_set():
                         break
 
-                except asyncio.TimeoutError:
+                except TimeoutError:
                     # Send heartbeat
                     heartbeat_data = {
                         "type": "heartbeat",
@@ -263,9 +261,7 @@ class StreamingWorkflowResponse:
                 "intermediate_results": operation.intermediate_results,
             }
 
-            await self.stream_queue.put(
-                self._format_chunk(progress_data, "progress_update")
-            )
+            await self.stream_queue.put(self._format_chunk(progress_data, "progress_update"))
 
         except Exception as e:
             logger.error(f"Error handling progress update: {e}")
@@ -304,9 +300,7 @@ class StreamingResponseManager:
         logger.info(f"Created streaming response: {workflow_id}")
         return stream_response
 
-    def get_streaming_response(
-        self, workflow_id: str
-    ) -> StreamingWorkflowResponse | None:
+    def get_streaming_response(self, workflow_id: str) -> StreamingWorkflowResponse | None:
         """Get an existing streaming response."""
         return self.active_streams.get(workflow_id)
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/websocket_manager.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/websocket_manager.py
index ab11e8794..37babef2c 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/websocket_manager.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/websocket_manager.py
@@ -113,14 +113,10 @@ class WebSocketConnectionManager:
 
         # Configuration
         self.heartbeat_interval = float(
-            config.get(
-                "agent_orchestration.realtime.websocket.heartbeat_interval", 30.0
-            )
+            config.get("agent_orchestration.realtime.websocket.heartbeat_interval", 30.0)
         )
         self.connection_timeout = float(
-            config.get(
-                "agent_orchestration.realtime.websocket.connection_timeout", 60.0
-            )
+            config.get("agent_orchestration.realtime.websocket.connection_timeout", 60.0)
         )
         self.max_connections = int(
             config.get("agent_orchestration.realtime.websocket.max_connections", 1000)
@@ -145,9 +141,7 @@ class WebSocketConnectionManager:
         self.user_subscriptions: dict[str, set[str]] = {}  # user_id -> event_types
 
         # Connection recovery
-        self.connection_history: dict[
-            str, dict[str, Any]
-        ] = {}  # user_id -> connection_info
+        self.connection_history: dict[str, dict[str, Any]] = {}  # user_id -> connection_info
         self.recovery_enabled = bool(
             config.get("agent_orchestration.realtime.recovery.enabled", True)
         )
@@ -265,9 +259,7 @@ class WebSocketConnectionManager:
             auth_data = json.loads(auth_message)
 
             if auth_data.get("type") != "auth":
-                await self._send_error(
-                    connection, "AUTH_REQUIRED", "Authentication required"
-                )
+                await self._send_error(connection, "AUTH_REQUIRED", "Authentication required")
                 return False
 
             # Extract token from message
@@ -278,7 +270,7 @@ class WebSocketConnectionManager:
 
             return await self._authenticate_with_token(connection, token)
 
-        except asyncio.TimeoutError:
+        except TimeoutError:
             await self._send_error(connection, "AUTH_TIMEOUT", "Authentication timeout")
             return False
         except json.JSONDecodeError:
@@ -305,9 +297,7 @@ class WebSocketConnectionManager:
 
         return None
 
-    async def _authenticate_with_token(
-        self, connection: WebSocketConnection, token: str
-    ) -> bool:
+    async def _authenticate_with_token(self, connection: WebSocketConnection, token: str) -> bool:
         """Authenticate connection with JWT token using existing auth system."""
         try:
             # Import the existing JWT verification function
@@ -380,9 +370,7 @@ class WebSocketConnectionManager:
             return True
         except Exception as e:
             logger.error(f"Token authentication error: {e}")
-            await self._send_error(
-                connection, "AUTH_ERROR", "Token authentication error"
-            )
+            await self._send_error(connection, "AUTH_ERROR", "Token authentication error")
             return False
 
     async def _handle_messages(self, connection: WebSocketConnection) -> None:
@@ -423,14 +411,10 @@ class WebSocketConnectionManager:
             except WebSocketDisconnect:
                 break
             except json.JSONDecodeError:
-                await self._send_error(
-                    connection, "INVALID_JSON", "Invalid JSON message"
-                )
+                await self._send_error(connection, "INVALID_JSON", "Invalid JSON message")
             except Exception as e:
                 logger.error(f"Message handling error: {e}")
-                await self._send_error(
-                    connection, "MESSAGE_ERROR", "Error handling message"
-                )
+                await self._send_error(connection, "MESSAGE_ERROR", "Error handling message")
 
     async def _handle_subscription(
         self, connection: WebSocketConnection, data: dict[str, Any]
@@ -574,9 +558,7 @@ class WebSocketConnectionManager:
                 connection, "UNSUBSCRIPTION_ERROR", "Error processing unsubscription"
             )
 
-    async def _handle_ping(
-        self, connection: WebSocketConnection, data: dict[str, Any]
-    ) -> None:
+    async def _handle_ping(self, connection: WebSocketConnection, data: dict[str, Any]) -> None:
         """Handle ping message and respond with pong."""
         # Send pong response
         pong_event = WebSocketEvent(
@@ -591,18 +573,14 @@ class WebSocketConnectionManager:
         )
         await self._send_to_connection(connection, pong_event)
 
-    async def _handle_pong(
-        self, connection: WebSocketConnection, data: dict[str, Any]
-    ) -> None:
+    async def _handle_pong(self, connection: WebSocketConnection, data: dict[str, Any]) -> None:
         """Handle pong response from client."""
         connection.last_pong = time.time()
         connection.pong_count += 1
 
         # Reset missed pongs counter
         if connection.missed_pongs > 0:
-            logger.info(
-                f"Connection {connection.connection_id} recovered, missed pongs reset"
-            )
+            logger.info(f"Connection {connection.connection_id} recovered, missed pongs reset")
         connection.missed_pongs = 0
 
         logger.debug(f"Received pong from connection {connection.connection_id}")
@@ -621,9 +599,7 @@ class WebSocketConnectionManager:
                 )
                 return
 
-            success = await self.subscribe_connection_to_agent(
-                connection.connection_id, agent_id
-            )
+            success = await self.subscribe_connection_to_agent(connection.connection_id, agent_id)
 
             if success:
                 await self._send_to_connection(
@@ -696,9 +672,7 @@ class WebSocketConnectionManager:
             new_filters = data.get("filters", {})
 
             # Validate and apply authorized filters
-            authorized_filters = self._filter_authorized_filters(
-                connection, new_filters
-            )
+            authorized_filters = self._filter_authorized_filters(connection, new_filters)
 
             # Update connection filters
             try:
@@ -756,12 +730,8 @@ class WebSocketConnectionManager:
     def _extract_client_info(self, websocket: WebSocket) -> dict[str, Any]:
         """Extract client information from WebSocket."""
         return {
-            "client_host": (
-                getattr(websocket.client, "host", None) if websocket.client else None
-            ),
-            "client_port": (
-                getattr(websocket.client, "port", None) if websocket.client else None
-            ),
+            "client_host": (getattr(websocket.client, "host", None) if websocket.client else None),
+            "client_port": (getattr(websocket.client, "port", None) if websocket.client else None),
             "headers": dict(websocket.headers) if hasattr(websocket, "headers") else {},
         }
 
@@ -822,9 +792,7 @@ class WebSocketConnectionManager:
 
     async def _recover_connection(self, connection: WebSocketConnection) -> bool:
         """Attempt to recover a previous connection's state."""
-        if not connection.user_id or not self._can_recover_connection(
-            connection.user_id
-        ):
+        if not connection.user_id or not self._can_recover_connection(connection.user_id):
             return False
 
         try:
@@ -866,9 +834,7 @@ class WebSocketConnectionManager:
             return True
 
         except Exception as e:
-            logger.error(
-                f"Error recovering connection for user {connection.user_id}: {e}"
-            )
+            logger.error(f"Error recovering connection for user {connection.user_id}: {e}")
             return False
 
     async def _heartbeat_loop(self) -> None:
@@ -921,9 +887,7 @@ class WebSocketConnectionManager:
                             )
 
                     except Exception as e:
-                        logger.debug(
-                            f"Error sending heartbeat to {connection.connection_id}: {e}"
-                        )
+                        logger.debug(f"Error sending heartbeat to {connection.connection_id}: {e}")
                         # Connection will be cleaned up by cleanup loop
 
             except asyncio.CancelledError:
@@ -942,10 +906,7 @@ class WebSocketConnectionManager:
 
                 for connection_id, connection in self.connections.items():
                     # Check for stale connections (no heartbeat activity)
-                    if (
-                        current_time - connection.last_heartbeat
-                        > self.connection_timeout
-                    ):
+                    if current_time - connection.last_heartbeat > self.connection_timeout:
                         stale_connections.append(connection_id)
                     # Check for unhealthy connections (too many missed pongs)
                     elif connection.missed_pongs > 5:
@@ -954,10 +915,7 @@ class WebSocketConnectionManager:
                             f"Removing unhealthy connection {connection_id} (missed {connection.missed_pongs} pongs)"
                         )
                     # Check for inactive connections (no pong responses)
-                    elif (
-                        current_time - connection.last_pong
-                        > self.connection_timeout * 2
-                    ):
+                    elif current_time - connection.last_pong > self.connection_timeout * 2:
                         stale_connections.append(connection_id)
                         logger.warning(
                             f"Removing inactive connection {connection_id} (no pong for {current_time - connection.last_pong:.1f}s)"
@@ -968,15 +926,11 @@ class WebSocketConnectionManager:
                     with contextlib.suppress(Exception):
                         connection = self.connections.get(connection_id)
                         if connection:
-                            await connection.websocket.close(
-                                code=1001, reason="timeout"
-                            )
+                            await connection.websocket.close(code=1001, reason="timeout")
                     await self._remove_connection(connection_id)
 
                 if stale_connections:
-                    logger.info(
-                        f"Cleaned up {len(stale_connections)} stale connections"
-                    )
+                    logger.info(f"Cleaned up {len(stale_connections)} stale connections")
 
             except asyncio.CancelledError:
                 break
@@ -994,10 +948,7 @@ class WebSocketConnectionManager:
 
                 for user_id, history in self.connection_history.items():
                     disconnected_at = history.get("disconnected_at")
-                    if (
-                        disconnected_at
-                        and (current_time - disconnected_at) > self.recovery_timeout
-                    ):
+                    if disconnected_at and (current_time - disconnected_at) > self.recovery_timeout:
                         expired_users.append(user_id)
 
                 # Remove expired recovery history
@@ -1005,9 +956,7 @@ class WebSocketConnectionManager:
                     del self.connection_history[user_id]
 
                 if expired_users:
-                    logger.info(
-                        f"Cleaned up recovery history for {len(expired_users)} users"
-                    )
+                    logger.info(f"Cleaned up recovery history for {len(expired_users)} users")
 
             except asyncio.CancelledError:
                 break
@@ -1078,9 +1027,7 @@ class WebSocketConnectionManager:
         # Apply connection-specific filters
         return self._apply_event_filters(connection, event)
 
-    def _apply_event_filters(
-        self, connection: WebSocketConnection, event: WebSocketEvent
-    ) -> bool:
+    def _apply_event_filters(self, connection: WebSocketConnection, event: WebSocketEvent) -> bool:
         """Apply connection-specific filters to determine if event should be sent."""
         filters = connection.filters
 
@@ -1122,17 +1069,13 @@ class WebSocketConnectionManager:
 
         return True
 
-    def _is_authorized_for_agent(
-        self, connection: WebSocketConnection, agent_id: str
-    ) -> bool:
+    def _is_authorized_for_agent(self, connection: WebSocketConnection, agent_id: str) -> bool:
         """Check if connection is authorized to receive events for a specific agent."""
         # Basic authorization - all authenticated users can see all agents
         # This can be extended with role-based access control
         return connection.is_authenticated
 
-    async def subscribe_connection_to_agent(
-        self, connection_id: str, agent_id: str
-    ) -> bool:
+    async def subscribe_connection_to_agent(self, connection_id: str, agent_id: str) -> bool:
         """Subscribe a connection to events for a specific agent."""
         connection = self.connections.get(connection_id)
         if not connection or not connection.is_authenticated:
@@ -1150,9 +1093,7 @@ class WebSocketConnectionManager:
         logger.debug(f"Connection {connection_id} subscribed to agent {agent_id}")
         return True
 
-    async def unsubscribe_connection_from_agent(
-        self, connection_id: str, agent_id: str
-    ) -> bool:
+    async def unsubscribe_connection_from_agent(self, connection_id: str, agent_id: str) -> bool:
         """Unsubscribe a connection from events for a specific agent."""
         connection = self.connections.get(connection_id)
         if not connection:
@@ -1164,9 +1105,7 @@ class WebSocketConnectionManager:
         logger.debug(f"Connection {connection_id} unsubscribed from agent {agent_id}")
         return True
 
-    async def subscribe_connection_to_user_events(
-        self, connection_id: str, user_id: str
-    ) -> bool:
+    async def subscribe_connection_to_user_events(self, connection_id: str, user_id: str) -> bool:
         """Subscribe a connection to events for a specific user."""
         connection = self.connections.get(connection_id)
         if not connection or not connection.is_authenticated:
@@ -1187,9 +1126,7 @@ class WebSocketConnectionManager:
         if user_id not in connection.filters.user_ids:
             connection.filters.user_ids.append(user_id)
 
-        logger.debug(
-            f"Connection {connection_id} subscribed to user events for {user_id}"
-        )
+        logger.debug(f"Connection {connection_id} subscribed to user events for {user_id}")
         return True
 
     def get_connection_subscriptions(self, connection_id: str) -> dict[str, Any] | None:
@@ -1201,9 +1138,7 @@ class WebSocketConnectionManager:
         return {
             "event_types": list(connection.subscriptions),
             "filters": connection.filters.model_dump() if connection.filters else {},
-            "agent_subscriptions": list(
-                getattr(connection, "agent_subscriptions", set())
-            ),
+            "agent_subscriptions": list(getattr(connection, "agent_subscriptions", set())),
         }
 
     async def shutdown(self) -> None:
@@ -1260,9 +1195,7 @@ class WebSocketConnectionManager:
             await self._event_subscriber.start()
 
             # Subscribe to all events to broadcast to WebSocket clients
-            await self._event_subscriber.subscribe_to_all_events(
-                self._handle_redis_event
-            )
+            await self._event_subscriber.subscribe_to_all_events(self._handle_redis_event)
 
             logger.info("Event subscription started for WebSocket manager")
 
@@ -1338,9 +1271,7 @@ class WebSocketConnectionManager:
         logger.debug(f"User {user_id} unsubscribed from events")
         return True
 
-    async def update_user_event_filters(
-        self, user_id: str, filters: dict[str, Any]
-    ) -> bool:
+    async def update_user_event_filters(self, user_id: str, filters: dict[str, Any]) -> bool:
         """Update event filters for a user."""
         if user_id not in self.user_subscriptions:
             return False
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/workflow_progress.py b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/workflow_progress.py
index cdba46d6e..4a1f0bbd6 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/workflow_progress.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/realtime/workflow_progress.py
@@ -116,9 +116,7 @@ class WorkflowProgress:
         self.milestones.append(milestone)
         return milestone_id
 
-    def complete_milestone(
-        self, milestone_id: str, metadata: dict[str, Any] | None = None
-    ) -> bool:
+    def complete_milestone(self, milestone_id: str, metadata: dict[str, Any] | None = None) -> bool:
         """Complete a milestone."""
         for milestone in self.milestones:
             if milestone.milestone_id == milestone_id:
@@ -317,9 +315,7 @@ class WorkflowProgressTracker:
                 workflow.add_milestone(
                     name=milestone_data["name"],
                     description=milestone_data.get("description", ""),
-                    stage=WorkflowStage(
-                        milestone_data.get("stage", WorkflowStage.EXECUTING)
-                    ),
+                    stage=WorkflowStage(milestone_data.get("stage", WorkflowStage.EXECUTING)),
                     weight=milestone_data.get("weight", 1.0),
                 )
 
@@ -412,12 +408,8 @@ class WorkflowProgressTracker:
 
         # Update final status
         workflow.status = WorkflowStatus.COMPLETED if success else WorkflowStatus.FAILED
-        workflow.current_stage = (
-            WorkflowStage.COMPLETED if success else WorkflowStage.FAILED
-        )
-        workflow.progress_percentage = (
-            100.0 if success else workflow.progress_percentage
-        )
+        workflow.current_stage = WorkflowStage.COMPLETED if success else WorkflowStage.FAILED
+        workflow.progress_percentage = 100.0 if success else workflow.progress_percentage
 
         if final_metadata:
             workflow.metadata.update(final_metadata)
@@ -433,9 +425,7 @@ class WorkflowProgressTracker:
         self.active_workflows.pop(workflow_id, None)
         self.workflow_callbacks.pop(workflow_id, None)
 
-        logger.info(
-            f"Completed workflow: {workflow_id} ({'success' if success else 'failed'})"
-        )
+        logger.info(f"Completed workflow: {workflow_id} ({'success' if success else 'failed'})")
         return True
 
     async def fail_workflow(
@@ -517,9 +507,7 @@ class WorkflowProgressTracker:
         except Exception as e:
             logger.error(f"Failed to publish workflow progress event: {e}")
 
-    async def _call_workflow_callbacks(
-        self, workflow_id: str, workflow: WorkflowProgress
-    ) -> None:
+    async def _call_workflow_callbacks(self, workflow_id: str, workflow: WorkflowProgress) -> None:
         """Call all registered callbacks for a workflow."""
         callbacks = self.workflow_callbacks.get(workflow_id, set())
 
@@ -600,9 +588,7 @@ class WorkflowProgressTracker:
         """Get count of workflows by stage."""
         counts = {}
         for workflow in self.active_workflows.values():
-            counts[workflow.current_stage.value] = (
-                counts.get(workflow.current_stage.value, 0) + 1
-            )
+            counts[workflow.current_stage.value] = counts.get(workflow.current_stage.value, 0) + 1
         return counts
 
     def _get_workflows_by_status(self) -> dict[str, int]:
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/registries/redis_agent_registry.py b/packages/tta-ai-framework/src/tta_ai/orchestration/registries/redis_agent_registry.py
index 9fcd0a818..55be398de 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/registries/redis_agent_registry.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/registries/redis_agent_registry.py
@@ -74,9 +74,7 @@ class RedisAgentRegistry(AgentRegistry):
 
     # ---- Key helpers ----
     def _key(self, agent_id: AgentId) -> str:
-        return (
-            f"{self._pfx}:agents:{agent_id.type.value}:{agent_id.instance or 'default'}"
-        )
+        return f"{self._pfx}:agents:{agent_id.type.value}:{agent_id.instance or 'default'}"
 
     def _index_key(self) -> str:
         return f"{self._pfx}:agents:index"
@@ -269,9 +267,7 @@ class RedisAgentRegistry(AgentRegistry):
             await self._detect_and_publish_status_changes(agent)
 
         except Exception as e:
-            logger.warning(
-                f"Failed to update heartbeat for agent {agent.agent_id}: {e}"
-            )
+            logger.warning(f"Failed to update heartbeat for agent {agent.agent_id}: {e}")
 
     async def restore_state_if_available(self, agent: Agent) -> bool:
         """If a serialized state exists in Redis for this agent, restore it.
@@ -359,9 +355,7 @@ class RedisAgentRegistry(AgentRegistry):
                 },
             )
 
-            logger.debug(
-                f"Published agent event via EventPublisher: {event_type} for {agent_key}"
-            )
+            logger.debug(f"Published agent event via EventPublisher: {event_type} for {agent_key}")
 
         except Exception as e:
             logger.warning(f"Failed to publish agent event via EventPublisher: {e}")
@@ -424,9 +418,7 @@ class RedisAgentRegistry(AgentRegistry):
         agent_key = self._agent_status_key(agent_id)
         self._agent_status_cache.pop(agent_key, None)
 
-    async def _publish_heartbeat_event(
-        self, agent: Agent, heartbeat_age: float = 0.0
-    ) -> None:
+    async def _publish_heartbeat_event(self, agent: Agent, heartbeat_age: float = 0.0) -> None:
         """Publish heartbeat event with current status."""
         await self._publish_agent_event(
             "heartbeat",
@@ -461,9 +453,7 @@ class RedisAgentRegistry(AgentRegistry):
             logger.error(f"Failed to register capabilities for {agent_id}: {e}")
             return False
 
-    async def get_agent_capabilities(
-        self, agent_id: AgentId
-    ) -> AgentCapabilitySet | None:
+    async def get_agent_capabilities(self, agent_id: AgentId) -> AgentCapabilitySet | None:
         """Get capabilities for a specific agent."""
         try:
             cap_key = self._capability_key(agent_id)
@@ -495,9 +485,7 @@ class RedisAgentRegistry(AgentRegistry):
                         capability_set = AgentCapabilitySet(**capabilities_dict)
                         capability_sets.append(capability_set)
                     except Exception as e:
-                        logger.warning(
-                            f"Failed to parse capabilities from key {key}: {e}"
-                        )
+                        logger.warning(f"Failed to parse capabilities from key {key}: {e}")
 
             return capability_sets
 
@@ -602,9 +590,7 @@ class RedisAgentRegistry(AgentRegistry):
             all_capabilities = await self.list_all_capabilities()
 
             total_agents = len(all_capabilities)
-            total_capabilities = sum(
-                len(cap_set.capabilities) for cap_set in all_capabilities
-            )
+            total_capabilities = sum(len(cap_set.capabilities) for cap_set in all_capabilities)
             active_capabilities = sum(
                 len(cap_set.get_active_capabilities()) for cap_set in all_capabilities
             )
@@ -620,8 +606,7 @@ class RedisAgentRegistry(AgentRegistry):
             avg_load_factor = 0.0
             if total_agents > 0:
                 avg_load_factor = (
-                    sum(cap_set.load_factor for cap_set in all_capabilities)
-                    / total_agents
+                    sum(cap_set.load_factor for cap_set in all_capabilities) / total_agents
                 )
 
             return {
@@ -660,9 +645,7 @@ class RedisAgentRegistry(AgentRegistry):
             # Convert agent keys to expected capability key format
             expected_cap_keys = set()
             for bkey in agent_keys:
-                agent_key = (
-                    bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
-                )
+                agent_key = bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
                 # Convert from "ao:agents:type:instance" to "ao:capabilities:type:instance"
                 if ":agents:" in agent_key:
                     cap_key = agent_key.replace(":agents:", ":capabilities:")
@@ -671,9 +654,7 @@ class RedisAgentRegistry(AgentRegistry):
             # Find stale capability keys
             stale_keys = []
             for bkey in cap_keys:
-                cap_key = (
-                    bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
-                )
+                cap_key = bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
                 if cap_key not in expected_cap_keys:
                     stale_keys.append(cap_key)
 
@@ -685,9 +666,7 @@ class RedisAgentRegistry(AgentRegistry):
                     await self._redis.srem(self._capability_index_key(), stale_key)
                     removed_count += 1
                 except Exception as e:
-                    logger.warning(
-                        f"Failed to remove stale capability key {stale_key}: {e}"
-                    )
+                    logger.warning(f"Failed to remove stale capability key {stale_key}: {e}")
 
             if removed_count > 0:
                 logger.info(f"Cleaned up {removed_count} stale capability entries")
@@ -741,18 +720,14 @@ class RedisAgentRegistry(AgentRegistry):
             # Check for orphaned capabilities
             expected_cap_keys = set()
             for bkey in agent_keys:
-                agent_key = (
-                    bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
-                )
+                agent_key = bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
                 if ":agents:" in agent_key:
                     cap_key = agent_key.replace(":agents:", ":capabilities:")
                     expected_cap_keys.add(cap_key)
 
             orphaned_count = 0
             for bkey in cap_keys:
-                cap_key = (
-                    bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
-                )
+                cap_key = bkey.decode() if isinstance(bkey, (bytes, bytearray)) else bkey
                 if cap_key not in expected_cap_keys:
                     orphaned_count += 1
 
@@ -760,8 +735,7 @@ class RedisAgentRegistry(AgentRegistry):
                 "total_capability_entries": total_capabilities,
                 "total_agent_entries": total_agents,
                 "orphaned_capabilities": orphaned_count,
-                "capability_coverage": (total_capabilities - orphaned_count)
-                / max(1, total_agents),
+                "capability_coverage": (total_capabilities - orphaned_count) / max(1, total_agents),
                 "ttl_seconds": self._ttl,
                 "heartbeat_interval_seconds": self._hb_interval_s,
             }
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/resource_exhaustion_detector.py b/packages/tta-ai-framework/src/tta_ai/orchestration/resource_exhaustion_detector.py
index 02585ebb8..973b1b4f0 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/resource_exhaustion_detector.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/resource_exhaustion_detector.py
@@ -57,9 +57,7 @@ class ResourceThresholds:
             < self.memory_exhaustion_percent
             <= 100
         ):
-            errors.append(
-                "Memory thresholds must be: 0 < warning < critical < exhaustion <= 100"
-            )
+            errors.append("Memory thresholds must be: 0 < warning < critical < exhaustion <= 100")
 
         # Check CPU thresholds
         if not (
@@ -69,9 +67,7 @@ class ResourceThresholds:
             < self.cpu_exhaustion_percent
             <= 100
         ):
-            errors.append(
-                "CPU thresholds must be: 0 < warning < critical < exhaustion <= 100"
-            )
+            errors.append("CPU thresholds must be: 0 < warning < critical < exhaustion <= 100")
 
         # Check disk thresholds
         if not (
@@ -81,9 +77,7 @@ class ResourceThresholds:
             < self.disk_exhaustion_percent
             <= 100
         ):
-            errors.append(
-                "Disk thresholds must be: 0 < warning < critical < exhaustion <= 100"
-            )
+            errors.append("Disk thresholds must be: 0 < warning < critical < exhaustion <= 100")
 
         # Check early warning percentage
         if not (0 < self.early_warning_percent < 100):
@@ -152,12 +146,8 @@ class ResourceExhaustionDetector:
         }
 
         # Callbacks
-        self._exhaustion_callbacks: list[
-            Callable[[ResourceExhaustionEvent], Awaitable[None]]
-        ] = []
-        self._warning_callbacks: list[
-            Callable[[ResourceExhaustionEvent], Awaitable[None]]
-        ] = []
+        self._exhaustion_callbacks: list[Callable[[ResourceExhaustionEvent], Awaitable[None]]] = []
+        self._warning_callbacks: list[Callable[[ResourceExhaustionEvent], Awaitable[None]]] = []
 
         # Statistics
         self._total_checks = 0
@@ -356,9 +346,7 @@ class ResourceExhaustionDetector:
             return "decreasing"
         return "stable"
 
-    async def _handle_sustained_exhaustion(
-        self, event: ResourceExhaustionEvent
-    ) -> None:
+    async def _handle_sustained_exhaustion(self, event: ResourceExhaustionEvent) -> None:
         """Handle sustained resource exhaustion."""
         logger.error(
             f"Sustained resource exhaustion detected: {event.resource_type}",
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/resources.py b/packages/tta-ai-framework/src/tta_ai/orchestration/resources.py
index 4ed9c4b09..67f8fc413 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/resources.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/resources.py
@@ -64,13 +64,9 @@ class ResourceUsageReport:
 @dataclass
 class WorkloadMetrics:
     # Minimal fields to drive optimization
-    queue_lengths: dict[str, int] = field(
-        default_factory=dict
-    )  # key: agent_type:instance
+    queue_lengths: dict[str, int] = field(default_factory=dict)  # key: agent_type:instance
     dlq_lengths: dict[str, int] = field(default_factory=dict)
-    step_latency_ms_p50: dict[str, float] = field(
-        default_factory=dict
-    )  # key: agent_type
+    step_latency_ms_p50: dict[str, float] = field(default_factory=dict)  # key: agent_type
     step_error_rates: dict[str, float] = field(default_factory=dict)  # 0..1
 
 
@@ -102,9 +98,7 @@ class ResourceManager:
         redis_prefix: str = "ao",
         circuit_breaker_registry: Any = None,
     ) -> None:
-        self.gpu_memory_limit_fraction = float(
-            max(0.0, min(1.0, gpu_memory_limit_fraction))
-        )
+        self.gpu_memory_limit_fraction = float(max(0.0, min(1.0, gpu_memory_limit_fraction)))
         self.cpu_thread_limit = cpu_thread_limit
         self.memory_limit_bytes = memory_limit_bytes
         self.warn_cpu_percent = warn_cpu_percent
@@ -139,24 +133,18 @@ class ResourceManager:
         """
         usage = self._collect_usage()
         if usage is None:
-            return ResourceAllocation(
-                granted=True, reason="psutil unavailable; cannot evaluate"
-            )
+            return ResourceAllocation(granted=True, reason="psutil unavailable; cannot evaluate")
 
         # CPU check
         if self.cpu_thread_limit is not None and resource_requirements.cpu_threads:
             if resource_requirements.cpu_threads > self.cpu_thread_limit:
-                return ResourceAllocation(
-                    granted=False, reason="CPU thread request exceeds limit"
-                )
+                return ResourceAllocation(granted=False, reason="CPU thread request exceeds limit")
 
         # Memory check
         if resource_requirements.ram_bytes is not None:
             avail_bytes = max(0, usage.memory_total_bytes - usage.memory_used_bytes)
             if resource_requirements.ram_bytes > avail_bytes:
-                return ResourceAllocation(
-                    granted=False, reason="Insufficient RAM available"
-                )
+                return ResourceAllocation(granted=False, reason="Insufficient RAM available")
 
         # GPU check (best-effort)
         gpu_index, grant_gpu_bytes = self._evaluate_gpu_request(resource_requirements)
@@ -196,18 +184,14 @@ class ResourceManager:
         )
         self._latest_report = report
         # Emergency mode flag
-        self._emergency_active = any(
-            v.get("level") == "critical" for v in thresholds.values()
-        )
+        self._emergency_active = any(v.get("level") == "critical" for v in thresholds.values())
 
         # Check for resource exhaustion and trigger workflow error handling
         await self._check_resource_exhaustion(report)
 
         return report
 
-    async def optimize_allocation(
-        self, current_workload: WorkloadMetrics
-    ) -> OptimizationResult:
+    async def optimize_allocation(self, current_workload: WorkloadMetrics) -> OptimizationResult:
         actions: list[str] = []
         details: dict[str, Any] = {}
 
@@ -217,17 +201,13 @@ class ResourceManager:
             details["emergency"] = True
 
         # Use queue lengths to recommend instance scaling (logical recommendations)
-        hot_agents = {
-            k: v for k, v in current_workload.queue_lengths.items() if v >= 10
-        }
+        hot_agents = {k: v for k, v in current_workload.queue_lengths.items() if v >= 10}
         if hot_agents:
             actions.append("rebalance_queues")
             details["hot_agents"] = hot_agents
 
         # If step latencies degraded, suggest lowering concurrency or increasing backoff
-        slow_agents = {
-            k: v for k, v in current_workload.step_latency_ms_p50.items() if v >= 1500.0
-        }
+        slow_agents = {k: v for k, v in current_workload.step_latency_ms_p50.items() if v >= 1500.0}
         if slow_agents:
             actions.append("reduce_concurrency")
             details["slow_agents"] = slow_agents
@@ -245,9 +225,7 @@ class ResourceManager:
         with contextlib.suppress(Exception):
             loop = asyncio.get_event_loop()
             if loop.is_running():
-                self._monitoring_task = loop.create_task(
-                    self._monitor_loop(interval_seconds)
-                )
+                self._monitoring_task = loop.create_task(self._monitor_loop(interval_seconds))
             else:
                 # In synchronous contexts, run one iteration
                 loop.run_until_complete(self.monitor_usage())
@@ -268,13 +246,9 @@ class ResourceManager:
                     lvl = v.get("level")
                     val = v.get("value")
                     if lvl == "critical":
-                        logger.error(
-                            "[ResourceManager] Critical %s usage: %s%%", k, val
-                        )
+                        logger.error("[ResourceManager] Critical %s usage: %s%%", k, val)
                     elif lvl == "warning":
-                        logger.warning(
-                            "[ResourceManager] Elevated %s usage: %s%%", k, val
-                        )
+                        logger.warning("[ResourceManager] Elevated %s usage: %s%%", k, val)
             except asyncio.CancelledError:
                 break
             except Exception as e:
@@ -376,9 +350,7 @@ class ResourceManager:
                     pct = (used / total * 100.0) if total else 0.0
                     usage.gpu_utilization.append(float(pct))
 
-    def _evaluate_gpu_request(
-        self, req: ResourceRequirements
-    ) -> tuple[int | None, int]:
+    def _evaluate_gpu_request(self, req: ResourceRequirements) -> tuple[int | None, int]:
         # Best-effort: grant on device 0 if enough headroom by fraction
         with contextlib.suppress(Exception):
             import torch  # type: ignore
@@ -416,9 +388,7 @@ class ResourceManager:
         for resource, threshold_info in report.thresholds_exceeded.items():
             if threshold_info.get("level") == "critical":
                 exhaustion_detected = True
-                exhaustion_reasons.append(
-                    f"{resource}: {threshold_info.get('value', 0):.1f}%"
-                )
+                exhaustion_reasons.append(f"{resource}: {threshold_info.get('value', 0):.1f}%")
 
         if exhaustion_detected:
             self._last_exhaustion_alert = current_time
@@ -432,9 +402,7 @@ class ResourceManager:
             )
 
             # Trigger circuit breakers for resource exhaustion
-            await self._trigger_resource_exhaustion_circuit_breakers(
-                report, exhaustion_reasons
-            )
+            await self._trigger_resource_exhaustion_circuit_breakers(report, exhaustion_reasons)
 
             # Call registered callbacks
             for callback in self._resource_exhaustion_callbacks:
@@ -487,9 +455,7 @@ class ResourceManager:
         except ValueError:
             return False
 
-    async def check_resource_health_for_workflow(
-        self, workflow_name: str
-    ) -> dict[str, Any]:
+    async def check_resource_health_for_workflow(self, workflow_name: str) -> dict[str, Any]:
         """Check if resources are healthy enough to run a workflow."""
         if not self._latest_report:
             await self.monitor_usage()
@@ -501,9 +467,7 @@ class ResourceManager:
         critical_issues = []
         for resource, threshold_info in self._latest_report.thresholds_exceeded.items():
             if threshold_info.get("level") == "critical":
-                critical_issues.append(
-                    f"{resource}: {threshold_info.get('value', 0):.1f}%"
-                )
+                critical_issues.append(f"{resource}: {threshold_info.get('value', 0):.1f}%")
 
         if critical_issues:
             return {
@@ -517,9 +481,7 @@ class ResourceManager:
         warning_issues = []
         for resource, threshold_info in self._latest_report.thresholds_exceeded.items():
             if threshold_info.get("level") == "warning":
-                warning_issues.append(
-                    f"{resource}: {threshold_info.get('value', 0):.1f}%"
-                )
+                warning_issues.append(f"{resource}: {threshold_info.get('value', 0):.1f}%")
 
         return {
             "healthy": True,
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/router.py b/packages/tta-ai-framework/src/tta_ai/orchestration/router.py
index b92a2e4a5..c6794e263 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/router.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/router.py
@@ -119,17 +119,12 @@ class AgentRouter:
             )
         # Normalize: lower is better for q and h; higher is better for s
         nq = self._normalize(queues)
-        nh = [
-            min(1.0, (h / self._hb_fresh) if self._hb_fresh > 0 else 1.0)
-            for h in hb_ages
-        ]
+        nh = [min(1.0, (h / self._hb_fresh) if self._hb_fresh > 0 else 1.0) for h in hb_ages]
         ns = succs  # already in [0,1]
         # Compute weighted score: lower is better (penalize low success)
         for i, entry in enumerate(raw):
             penalty_success = 1.0 - ns[i]
-            score = (
-                (self._wq * nq[i]) + (self._wh * nh[i]) + (self._ws * penalty_success)
-            )
+            score = (self._wq * nq[i]) + (self._wh * nh[i]) + (self._ws * penalty_success)
             entry["score"] = float(score)
         # Choose best (min score)
         best = None
@@ -156,16 +151,12 @@ class AgentRouter:
             return None
         scored, best_agent = await self._score_candidates(candidates)
         if best_agent is not None:
-            return AgentId(
-                type=best_agent.agent_id.type, instance=best_agent.agent_id.instance
-            )
+            return AgentId(type=best_agent.agent_id.type, instance=best_agent.agent_id.instance)
         # Fallback: first candidate
         a0 = candidates[0]
         return AgentId(type=a0.agent_id.type, instance=a0.agent_id.instance)
 
-    async def resolve_target(
-        self, recipient: AgentId, exclude_degraded: bool = True
-    ) -> AgentId:
+    async def resolve_target(self, recipient: AgentId, exclude_degraded: bool = True) -> AgentId:
         """Return an AgentId with instance resolved to a healthy one when possible.
         Honors explicit instance when healthy; otherwise picks another healthy instance of the same type.
         """
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/service.py b/packages/tta-ai-framework/src/tta_ai/orchestration/service.py
index 6ad4561b2..b659ce1db 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/service.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/service.py
@@ -190,9 +190,7 @@ class AgentOrchestrationService:
 
             # Determine workflow type if not specified
             if workflow_type is None:
-                workflow_type = await self._determine_workflow_type(
-                    user_input, session_context
-                )
+                workflow_type = await self._determine_workflow_type(user_input, session_context)
 
             # Create orchestration request
             request = OrchestrationRequest(
@@ -228,9 +226,7 @@ class AgentOrchestrationService:
             self._error_count += 1
             logger.error(f"Error processing user input: {e}")
 
-            if isinstance(
-                e, (TherapeuticSafetyError, WorkflowExecutionError, SessionContextError)
-            ):
+            if isinstance(e, (TherapeuticSafetyError, WorkflowExecutionError, SessionContextError)):
                 raise
             raise ServiceError(f"Unexpected error during processing: {e}") from e
 
@@ -284,9 +280,7 @@ class AgentOrchestrationService:
                 logger.error(f"Workflow execution failed: {error}")
                 raise WorkflowExecutionError(f"Workflow execution failed: {error}")
 
-            logger.info(
-                f"Successfully coordinated agents for {workflow_type.value} workflow"
-            )
+            logger.info(f"Successfully coordinated agents for {workflow_type.value} workflow")
             return response, run_id, error
 
         except Exception as e:
@@ -319,9 +313,7 @@ class AgentOrchestrationService:
             }
 
             if self.crisis_intervention_manager:
-                metrics["crisis_manager"] = (
-                    self.crisis_intervention_manager.get_crisis_metrics()
-                )
+                metrics["crisis_manager"] = self.crisis_intervention_manager.get_crisis_metrics()
 
             if self.emergency_protocol_engine:
                 metrics["emergency_protocols"] = (
@@ -358,9 +350,7 @@ class AgentOrchestrationService:
         """Generate comprehensive safety report."""
         try:
             if self.safety_monitoring_dashboard:
-                return self.safety_monitoring_dashboard.get_safety_report(
-                    time_range_hours
-                )
+                return self.safety_monitoring_dashboard.get_safety_report(time_range_hours)
             return {"error": "Safety monitoring dashboard not available"}
         except Exception as e:
             logger.error(f"Error generating safety report: {e}")
@@ -381,8 +371,7 @@ class AgentOrchestrationService:
                 "request_count": self._request_count,
                 "error_count": self._error_count,
                 "error_rate": self._error_count / max(self._request_count, 1),
-                "avg_processing_time": self._total_processing_time
-                / max(self._request_count, 1),
+                "avg_processing_time": self._total_processing_time / max(self._request_count, 1),
             },
             "components": {
                 "workflow_manager": self.workflow_manager is not None,
@@ -392,13 +381,10 @@ class AgentOrchestrationService:
                 "resource_manager": self.resource_manager is not None,
                 "optimization_engine": self.optimization_engine is not None,
                 "neo4j_manager": self.neo4j_manager is not None,
-                "crisis_intervention_manager": self.crisis_intervention_manager
-                is not None,
+                "crisis_intervention_manager": self.crisis_intervention_manager is not None,
                 "emergency_protocol_engine": self.emergency_protocol_engine is not None,
-                "human_oversight_escalation": self.human_oversight_escalation
-                is not None,
-                "safety_monitoring_dashboard": self.safety_monitoring_dashboard
-                is not None,
+                "human_oversight_escalation": self.human_oversight_escalation is not None,
+                "safety_monitoring_dashboard": self.safety_monitoring_dashboard is not None,
             },
         }
 
@@ -412,9 +398,7 @@ class AgentOrchestrationService:
         try:
             # Cancel active workflows
             for session_id, run_id in self._active_workflows.items():
-                logger.info(
-                    f"Cancelling active workflow {run_id} for session {session_id}"
-                )
+                logger.info(f"Cancelling active workflow {run_id} for session {session_id}")
                 # Note: Actual cancellation would depend on WorkflowManager implementation
 
             # Clear state
@@ -440,12 +424,8 @@ class AgentOrchestrationService:
             collaborative_workflow = WorkflowDefinition(
                 workflow_type=WorkflowType.COLLABORATIVE,
                 agent_sequence=[
-                    AgentStep(
-                        agent=AgentType.IPA, name="input_processing", timeout_seconds=10
-                    ),
-                    AgentStep(
-                        agent=AgentType.WBA, name="world_building", timeout_seconds=15
-                    ),
+                    AgentStep(agent=AgentType.IPA, name="input_processing", timeout_seconds=10),
+                    AgentStep(agent=AgentType.WBA, name="world_building", timeout_seconds=15),
                     AgentStep(
                         agent=AgentType.NGA,
                         name="narrative_generation",
@@ -465,9 +445,7 @@ class AgentOrchestrationService:
             input_workflow = WorkflowDefinition(
                 workflow_type=WorkflowType.INPUT_PROCESSING,
                 agent_sequence=[
-                    AgentStep(
-                        agent=AgentType.IPA, name="input_processing", timeout_seconds=10
-                    ),
+                    AgentStep(agent=AgentType.IPA, name="input_processing", timeout_seconds=10),
                 ],
                 error_handling=ErrorHandlingStrategy.FAIL_FAST,
             )
@@ -482,9 +460,7 @@ class AgentOrchestrationService:
             world_building_workflow = WorkflowDefinition(
                 workflow_type=WorkflowType.WORLD_BUILDING,
                 agent_sequence=[
-                    AgentStep(
-                        agent=AgentType.WBA, name="world_building", timeout_seconds=15
-                    ),
+                    AgentStep(agent=AgentType.WBA, name="world_building", timeout_seconds=15),
                 ],
                 error_handling=ErrorHandlingStrategy.RETRY,
             )
@@ -512,9 +488,7 @@ class AgentOrchestrationService:
                 "narrative_generation", narrative_workflow
             )
             if not success:
-                logger.warning(
-                    f"Failed to register narrative generation workflow: {error}"
-                )
+                logger.warning(f"Failed to register narrative generation workflow: {error}")
 
             logger.info("Default workflows registered successfully")
 
@@ -575,9 +549,7 @@ class AgentOrchestrationService:
                     safety_level = validation_result.get("level", "unknown")
                     reason = validation_result.get("reason", "Safety validation failed")
                     crisis_detected = validation_result.get("crisis_detected", False)
-                    escalation_recommended = validation_result.get(
-                        "escalation_recommended", False
-                    )
+                    escalation_recommended = validation_result.get("escalation_recommended", False)
                     alternative_content = validation_result.get("alternative_content")
 
                     logger.warning(
@@ -587,13 +559,9 @@ class AgentOrchestrationService:
 
                     # For blocked content or crisis situations, raise an error with alternative
                     if safety_level == "blocked" or crisis_detected:
-                        error_message = (
-                            f"Content blocked due to safety concerns: {reason}"
-                        )
+                        error_message = f"Content blocked due to safety concerns: {reason}"
                         if alternative_content:
-                            error_message += (
-                                f"\n\nSuggested response: {alternative_content}"
-                            )
+                            error_message += f"\n\nSuggested response: {alternative_content}"
 
                         raise TherapeuticSafetyError(error_message)
 
@@ -636,9 +604,7 @@ class AgentOrchestrationService:
                 "session_count": getattr(session_context, "session_count", 0),
                 "previous_violations": getattr(session_context, "safety_violations", 0),
                 "therapeutic_session": True,
-                "previous_crisis_indicators": getattr(
-                    session_context, "crisis_indicators", False
-                ),
+                "previous_crisis_indicators": getattr(session_context, "crisis_indicators", False),
             }
 
             # Perform comprehensive validation
@@ -646,9 +612,7 @@ class AgentOrchestrationService:
 
             # Check if we should alert on this result
             should_alert = (
-                validator.should_alert(result)
-                if hasattr(validator, "should_alert")
-                else False
+                validator.should_alert(result) if hasattr(validator, "should_alert") else False
             )
 
             # Convert to expected format
@@ -681,12 +645,8 @@ class AgentOrchestrationService:
                         crisis_context = {
                             "session_id": session_context.session_id,
                             "user_id": getattr(session_context, "user_id", "unknown"),
-                            "session_count": getattr(
-                                session_context, "interaction_count", 0
-                            ),
-                            "previous_violations": getattr(
-                                session_context, "safety_violations", 0
-                            ),
+                            "session_count": getattr(session_context, "interaction_count", 0),
+                            "previous_violations": getattr(session_context, "safety_violations", 0),
                             "location": getattr(session_context, "location", "unknown"),
                         }
 
@@ -694,19 +654,14 @@ class AgentOrchestrationService:
                         assessment = self.crisis_intervention_manager.assess_crisis(
                             result, crisis_context
                         )
-                        intervention = (
-                            self.crisis_intervention_manager.initiate_intervention(
-                                assessment,
-                                session_context.session_id,
-                                crisis_context["user_id"],
-                            )
+                        intervention = self.crisis_intervention_manager.initiate_intervention(
+                            assessment,
+                            session_context.session_id,
+                            crisis_context["user_id"],
                         )
 
                         # Handle escalation if required
-                        if (
-                            assessment.escalation_required
-                            and self.human_oversight_escalation
-                        ):
+                        if assessment.escalation_required and self.human_oversight_escalation:
                             if assessment.crisis_level.value == "critical":
                                 self.human_oversight_escalation.escalate_to_emergency_services(
                                     intervention, "mental_health"
@@ -735,9 +690,7 @@ class AgentOrchestrationService:
                                 ),
                                 {
                                     "intervention_id": intervention.intervention_id,
-                                    "crisis_types": [
-                                        ct.value for ct in assessment.crisis_types
-                                    ],
+                                    "crisis_types": [ct.value for ct in assessment.crisis_types],
                                     "crisis_level": assessment.crisis_level.value,
                                 },
                             )
@@ -899,9 +852,7 @@ class AgentOrchestrationService:
         try:
             # Update context with response data
             if response.updated_context:
-                session_context.context.memory.update(
-                    response.updated_context.get("memory", {})
-                )
+                session_context.context.memory.update(response.updated_context.get("memory", {}))
                 session_context.context.world_state.update(
                     response.updated_context.get("world_state", {})
                 )
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/state_validator.py b/packages/tta-ai-framework/src/tta_ai/orchestration/state_validator.py
index 2fe179365..e338f104b 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/state_validator.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/state_validator.py
@@ -49,22 +49,16 @@ class StateValidator:
                 ):
                     k = key.decode() if isinstance(key, (bytes, bytearray)) else key
                     instances.add(k.split(":")[-1])
-                async for key in self._redis.scan_iter(
-                    match=f"{self._pfx}:reserved:{at.value}:*"
-                ):
+                async for key in self._redis.scan_iter(match=f"{self._pfx}:reserved:{at.value}:*"):
                     k = key.decode() if isinstance(key, (bytes, bytearray)) else key
                     instances.add(k.split(":")[-1])
                 # Also union with KEYS results (robustness in tests/small envs)
                 with contextlib.suppress(Exception):
-                    klist = await self._redis.keys(
-                        f"{self._pfx}:reserved_deadlines:{at.value}:*"
-                    )
+                    klist = await self._redis.keys(f"{self._pfx}:reserved_deadlines:{at.value}:*")
                     for kk in klist or []:
                         k = kk.decode() if isinstance(kk, (bytes, bytearray)) else kk
                         instances.add(k.split(":")[-1])
-                    klist2 = await self._redis.keys(
-                        f"{self._pfx}:reserved:{at.value}:*"
-                    )
+                    klist2 = await self._redis.keys(f"{self._pfx}:reserved:{at.value}:*")
                     for kk in klist2 or []:
                         k = kk.decode() if isinstance(kk, (bytes, bytearray)) else kk
                         instances.add(k.split(":")[-1])
@@ -76,32 +70,20 @@ class StateValidator:
                         from .coordinators import RedisMessageCoordinator
                         from .models import AgentId
 
-                        coord = RedisMessageCoordinator(
-                            self._redis, key_prefix=self._pfx
-                        )
+                        coord = RedisMessageCoordinator(self._redis, key_prefix=self._pfx)
                         repaired += int(
                             await coord.recover_pending(AgentId(type=at, instance=inst))
                         )
                     # Run up to two passes to be robust to immediate writes
                     for _, cutoff in enumerate(passes):
-                        cut = (
-                            cutoff
-                            if cutoff is not None
-                            else int(time.time() * 1_000_000)
-                        )
-                        zr_tokens = await self._redis.zrangebyscore(
-                            dkey, min=-1, max=cut
-                        )
+                        cut = cutoff if cutoff is not None else int(time.time() * 1_000_000)
+                        zr_tokens = await self._redis.zrangebyscore(dkey, min=-1, max=cut)
                         # Also consider tokens from reserved hash with missing or past deadlines (robust against timing)
                         extra_tokens: list = []
                         with contextlib.suppress(Exception):
                             htokens = await self._redis.hkeys(self._res_hash(at, inst))
                             for ht in htokens or []:
-                                htok = (
-                                    ht.decode()
-                                    if isinstance(ht, (bytes, bytearray))
-                                    else ht
-                                )
+                                htok = ht.decode() if isinstance(ht, (bytes, bytearray)) else ht
                                 try:
                                     dscore = await self._redis.zscore(dkey, htok)
                                 except Exception:
@@ -112,9 +94,7 @@ class StateValidator:
                         tokens_set = set()
                         for tb in zr_tokens or []:
                             tokens_set.add(
-                                tb.decode()
-                                if isinstance(tb, (bytes, bytearray))
-                                else tb
+                                tb.decode() if isinstance(tb, (bytes, bytearray)) else tb
                             )
                         for et in extra_tokens:
                             tokens_set.add(et)
@@ -122,9 +102,7 @@ class StateValidator:
                             continue
                         for tok in tokens_set:
                             try:
-                                payload = await self._redis.hget(
-                                    self._res_hash(at, inst), tok
-                                )
+                                payload = await self._redis.hget(self._res_hash(at, inst), tok)
                                 if not payload:
                                     # Token expired but payload already reclaimed elsewhere; clean up deadline and count as repaired
                                     await self._redis.zrem(dkey, tok)
@@ -133,22 +111,16 @@ class StateValidator:
                                 # Requeue to sched
                                 try:
                                     pdata = (
-                                        payload
-                                        if isinstance(payload, str)
-                                        else payload.decode()
+                                        payload if isinstance(payload, str) else payload.decode()
                                     )
                                     data = json.loads(pdata)
                                     prio = int(data.get("priority", 5))
                                     await self._redis.zadd(
                                         self._sched_key(at, inst, prio), {payload: cut}
                                     )
-                                    await self._redis.rpush(
-                                        self._queue_key(at, inst), payload
-                                    )
+                                    await self._redis.rpush(self._queue_key(at, inst), payload)
                                 except Exception:
-                                    await self._redis.rpush(
-                                        self._dlq_key(at, inst), payload
-                                    )
+                                    await self._redis.rpush(self._dlq_key(at, inst), payload)
                                 # Cleanup reservation
                                 await self._redis.hdel(self._res_hash(at, inst), tok)
                                 await self._redis.zrem(dkey, tok)
@@ -166,11 +138,7 @@ class StateValidator:
                         inst = k.split(":")[-1]
                         hkeys = await self._redis.hkeys(k)
                         for ht in hkeys or []:
-                            tok = (
-                                ht.decode()
-                                if isinstance(ht, (bytes, bytearray))
-                                else ht
-                            )
+                            tok = ht.decode() if isinstance(ht, (bytes, bytearray)) else ht
                             try:
                                 dscore = await self._redis.zscore(
                                     f"{self._pfx}:reserved_deadlines:{at.value}:{inst}",
@@ -193,13 +161,9 @@ class StateValidator:
                                             self._sched_key(at, inst, prio),
                                             {payload: now2},
                                         )
-                                        await self._redis.rpush(
-                                            self._queue_key(at, inst), payload
-                                        )
+                                        await self._redis.rpush(self._queue_key(at, inst), payload)
                                     except Exception:
-                                        await self._redis.rpush(
-                                            self._dlq_key(at, inst), payload
-                                        )
+                                        await self._redis.rpush(self._dlq_key(at, inst), payload)
                                 await self._redis.hdel(k, tok)
                                 await self._redis.zrem(
                                     f"{self._pfx}:reserved_deadlines:{at.value}:{inst}",
@@ -242,11 +206,7 @@ class StateValidator:
                                 # Derive priority without decoding by attempting JSON load; fallback to NORMAL
                                 prio = 5
                                 try:
-                                    pd = (
-                                        payload
-                                        if isinstance(payload, str)
-                                        else payload.decode()
-                                    )
+                                    pd = payload if isinstance(payload, str) else payload.decode()
                                     jd = json.loads(pd)
                                     prio = int(jd.get("priority", 5))
                                 except Exception:
@@ -259,9 +219,7 @@ class StateValidator:
                                 await self._redis.rpush(
                                     f"{self._pfx}:dlq:{at.value}:{inst}", payload
                                 )
-                        await self._redis.hdel(
-                            f"{self._pfx}:reserved:{at.value}:{inst}", tok_field
-                        )
+                        await self._redis.hdel(f"{self._pfx}:reserved:{at.value}:{inst}", tok_field)
                         await self._redis.zrem(dkey, tok_field)
                         repaired += 1
         return {"repaired": repaired, "errors": errors}
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/therapeutic_safety.py b/packages/tta-ai-framework/src/tta_ai/orchestration/therapeutic_safety.py
index 4564ebba1..e2454194c 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/therapeutic_safety.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/therapeutic_safety.py
@@ -311,9 +311,7 @@ class SafetyRuleEngine:
         with open(path, encoding="utf-8") as f:
             return json.load(f)
 
-    def evaluate(
-        self, text: str, context: dict[str, Any] | None = None
-    ) -> list[ValidationFinding]:
+    def evaluate(self, text: str, context: dict[str, Any] | None = None) -> list[ValidationFinding]:
         """Enhanced evaluation with multiple validation types and context awareness."""
         findings: list[ValidationFinding] = []
 
@@ -322,21 +320,13 @@ class SafetyRuleEngine:
 
         for rule, rx in self._compiled:
             if rule.validation_type == ValidationType.KEYWORD:
-                findings.extend(
-                    self._evaluate_keyword_rule(rule, rx, text, sentiment_score)
-                )
+                findings.extend(self._evaluate_keyword_rule(rule, rx, text, sentiment_score))
             elif rule.validation_type == ValidationType.CRISIS_DETECTION:
-                findings.extend(
-                    self._evaluate_crisis_rule(rule, text, sentiment_score, context)
-                )
+                findings.extend(self._evaluate_crisis_rule(rule, text, sentiment_score, context))
             elif rule.validation_type == ValidationType.THERAPEUTIC_BOUNDARY:
-                findings.extend(
-                    self._evaluate_therapeutic_rule(rule, rx, text, context)
-                )
+                findings.extend(self._evaluate_therapeutic_rule(rule, rx, text, context))
             elif rule.validation_type == ValidationType.SENTIMENT:
-                findings.extend(
-                    self._evaluate_sentiment_rule(rule, text, sentiment_score)
-                )
+                findings.extend(self._evaluate_sentiment_rule(rule, text, sentiment_score))
             elif rule.validation_type == ValidationType.CONTEXT_AWARE:
                 findings.extend(self._evaluate_context_rule(rule, rx, text, context))
 
@@ -662,21 +652,15 @@ class TherapeuticValidator:
         self._config = cfg
 
         # Enhanced configuration options
-        self._crisis_detection_enabled = cfg.get("crisis_detection", {}).get(
-            "enabled", True
-        )
-        self._crisis_sensitivity = cfg.get("crisis_detection", {}).get(
-            "sensitivity", 0.7
-        )
+        self._crisis_detection_enabled = cfg.get("crisis_detection", {}).get("enabled", True)
+        self._crisis_sensitivity = cfg.get("crisis_detection", {}).get("sensitivity", 0.7)
         self._escalation_threshold = cfg.get("crisis_detection", {}).get(
             "escalation_threshold", 0.9
         )
-        self._alternative_generation_enabled = cfg.get(
-            "alternative_generation", {}
-        ).get("enabled", True)
-        self._therapeutic_tone = cfg.get("alternative_generation", {}).get(
-            "therapeutic_tone", True
+        self._alternative_generation_enabled = cfg.get("alternative_generation", {}).get(
+            "enabled", True
         )
+        self._therapeutic_tone = cfg.get("alternative_generation", {}).get("therapeutic_tone", True)
 
         # Monitoring and alerting
         self._violation_count = 0
@@ -693,9 +677,7 @@ class TherapeuticValidator:
         """Enhanced text validation with comprehensive analysis."""
         audit: list[dict[str, Any]] = []
         if include_audit:
-            audit.append(
-                {"event": "validate_text.start", "text_length": len(text or "")}
-            )
+            audit.append({"event": "validate_text.start", "text_length": len(text or "")})
 
         text = text or ""
 
@@ -705,9 +687,7 @@ class TherapeuticValidator:
         # Comprehensive analysis
         overall_sentiment = self._engine._analyze_sentiment(text)
         crisis_detected = any(f.crisis_type is not None for f in findings)
-        crisis_types = list(
-            {f.crisis_type for f in findings if f.crisis_type is not None}
-        )
+        crisis_types = list({f.crisis_type for f in findings if f.crisis_type is not None})
         escalation_recommended = any(f.escalation_required for f in findings)
 
         # Determine overall level with enhanced logic
@@ -720,9 +700,7 @@ class TherapeuticValidator:
             self._violation_count += 1
 
         # Enhanced scoring based on multiple factors
-        score = self._calculate_comprehensive_score(
-            findings, overall_sentiment, crisis_detected
-        )
+        score = self._calculate_comprehensive_score(findings, overall_sentiment, crisis_detected)
 
         # Calculate therapeutic appropriateness
         therapeutic_appropriateness = self._assess_therapeutic_appropriateness(
@@ -732,9 +710,7 @@ class TherapeuticValidator:
         # Generate alternative content if needed
         alternative_content = None
         if level != SafetyLevel.SAFE and self._alternative_generation_enabled:
-            alternative_content = self._generate_therapeutic_alternative(
-                text, findings, level
-            )
+            alternative_content = self._generate_therapeutic_alternative(text, findings, level)
 
         # Monitoring flags
         monitoring_flags = self._generate_monitoring_flags(
@@ -920,9 +896,7 @@ class TherapeuticValidator:
                 "Would you like to talk about what's been weighing on you, or explore some resources that might help?"
             ),
         }
-        return alternatives.get(
-            crisis_type, self._basic_alternative(SafetyLevel.BLOCKED)
-        )
+        return alternatives.get(crisis_type, self._basic_alternative(SafetyLevel.BLOCKED))
 
     def _professional_boundary_alternative(self) -> str:
         """Generate professional boundary alternatives."""
@@ -969,9 +943,7 @@ class TherapeuticValidator:
 
     def get_monitoring_metrics(self) -> dict[str, Any]:
         """Get monitoring metrics for safety oversight."""
-        total_estimated = max(
-            10, self._violation_count * 2
-        )  # Rough estimate of total validations
+        total_estimated = max(10, self._violation_count * 2)  # Rough estimate of total validations
         return {
             "total_validations_estimated": total_estimated,
             "violation_count": self._violation_count,
@@ -997,8 +969,7 @@ class TherapeuticValidator:
         if result.escalation_recommended:
             return True
         if result.crisis_detected and any(
-            ct in [CrisisType.SUICIDAL_IDEATION, CrisisType.SELF_HARM]
-            for ct in result.crisis_types
+            ct in [CrisisType.SUICIDAL_IDEATION, CrisisType.SELF_HARM] for ct in result.crisis_types
         ):
             return True
         return bool(result.level == SafetyLevel.BLOCKED and result.score < 0.2)
@@ -1012,15 +983,13 @@ class TherapeuticValidator:
         self._crisis_detection_enabled = self._config.get("crisis_detection", {}).get(
             "enabled", True
         )
-        self._crisis_sensitivity = self._config.get("crisis_detection", {}).get(
-            "sensitivity", 0.7
-        )
+        self._crisis_sensitivity = self._config.get("crisis_detection", {}).get("sensitivity", 0.7)
         self._escalation_threshold = self._config.get("crisis_detection", {}).get(
             "escalation_threshold", 0.9
         )
-        self._alternative_generation_enabled = self._config.get(
-            "alternative_generation", {}
-        ).get("enabled", True)
+        self._alternative_generation_enabled = self._config.get("alternative_generation", {}).get(
+            "enabled", True
+        )
         self._therapeutic_tone = self._config.get("alternative_generation", {}).get(
             "therapeutic_tone", True
         )
@@ -1214,17 +1183,13 @@ class CrisisInterventionManager:
         protective_factors = self._identify_protective_factors(session_context)
 
         # Determine intervention type
-        intervention_type = self._determine_intervention_type(
-            crisis_level, validation_result
-        )
+        intervention_type = self._determine_intervention_type(crisis_level, validation_result)
 
         # Check for immediate risk
         immediate_risk = self._assess_immediate_risk(validation_result, crisis_level)
 
         # Calculate overall confidence
-        confidence = self._calculate_crisis_confidence(
-            validation_result, session_context
-        )
+        confidence = self._calculate_crisis_confidence(validation_result, session_context)
 
         return CrisisAssessment(
             crisis_level=crisis_level,
@@ -1234,8 +1199,7 @@ class CrisisInterventionManager:
             protective_factors=protective_factors,
             immediate_risk=immediate_risk,
             intervention_recommended=intervention_type,
-            escalation_required=validation_result.escalation_recommended
-            or immediate_risk,
+            escalation_required=validation_result.escalation_recommended or immediate_risk,
             assessment_timestamp=time.time(),
             context=session_context,
         )
@@ -1279,9 +1243,7 @@ class CrisisInterventionManager:
             # This would be injected in a real implementation
             # For now, we'll add a simple alert mechanism
             alert_severity = (
-                "critical"
-                if assessment.crisis_level == CrisisLevel.CRITICAL
-                else "high"
+                "critical" if assessment.crisis_level == CrisisLevel.CRITICAL else "high"
             )
             self.logger.warning(
                 f"CRISIS ALERT: {alert_severity.upper()} intervention {intervention_id} "
@@ -1302,26 +1264,17 @@ class CrisisInterventionManager:
         # Check for critical indicators - more specific criteria
         if any(ct in high_risk_types for ct in validation_result.crisis_types):
             # Critical: Very low safety score AND escalation recommended
-            if (
-                validation_result.score < 0.15
-                and validation_result.escalation_recommended
-            ):
+            if validation_result.score < 0.15 and validation_result.escalation_recommended:
                 return CrisisLevel.CRITICAL
             # High: Low safety score OR escalation recommended
-            if (
-                validation_result.score < 0.25
-                or validation_result.escalation_recommended
-            ):
+            if validation_result.score < 0.25 or validation_result.escalation_recommended:
                 return CrisisLevel.HIGH
             # Moderate: Crisis detected but not severe
             return CrisisLevel.MODERATE
 
         # Severe depression with high confidence
         if CrisisType.SEVERE_DEPRESSION in validation_result.crisis_types:
-            if (
-                validation_result.score < 0.2
-                and validation_result.escalation_recommended
-            ):
+            if validation_result.score < 0.2 and validation_result.escalation_recommended:
                 return CrisisLevel.HIGH
             return CrisisLevel.MODERATE
 
@@ -1349,20 +1302,13 @@ class CrisisInterventionManager:
             risk_factors.extend(["suicidal_ideation", "death_wish", "hopelessness"])
 
         if CrisisType.SELF_HARM in validation_result.crisis_types:
-            risk_factors.extend(
-                ["self_harm_behavior", "self_punishment", "coping_mechanism"]
-            )
+            risk_factors.extend(["self_harm_behavior", "self_punishment", "coping_mechanism"])
 
         if CrisisType.SEVERE_DEPRESSION in validation_result.crisis_types:
-            risk_factors.extend(
-                ["severe_depression", "worthlessness", "emotional_numbness"]
-            )
+            risk_factors.extend(["severe_depression", "worthlessness", "emotional_numbness"])
 
         # Sentiment-based risk factors
-        if (
-            validation_result.overall_sentiment
-            and validation_result.overall_sentiment < -0.7
-        ):
+        if validation_result.overall_sentiment and validation_result.overall_sentiment < -0.7:
             risk_factors.append("severe_negative_sentiment")
 
         # Context-based risk factors
@@ -1436,17 +1382,14 @@ class CrisisInterventionManager:
 
         # Base confidence from validation findings
         if validation_result.findings:
-            base_confidence = sum(
-                f.confidence for f in validation_result.findings
-            ) / len(validation_result.findings)
+            base_confidence = sum(f.confidence for f in validation_result.findings) / len(
+                validation_result.findings
+            )
         else:
             base_confidence = 0.5
 
         # Adjust based on sentiment
-        if (
-            validation_result.overall_sentiment
-            and validation_result.overall_sentiment < -0.5
-        ):
+        if validation_result.overall_sentiment and validation_result.overall_sentiment < -0.5:
             base_confidence += 0.1
 
         # Adjust based on therapeutic appropriateness
@@ -1467,9 +1410,7 @@ class CrisisInterventionManager:
 
         try:
             # Generate appropriate response based on crisis type
-            response_message = self._generate_crisis_response(
-                intervention.crisis_assessment
-            )
+            response_message = self._generate_crisis_response(intervention.crisis_assessment)
 
             # Record the action
             action = InterventionAction(
@@ -1543,9 +1484,7 @@ class CrisisInterventionManager:
             response_time_ms=(time.perf_counter() - start_time) * 1000,
             metadata={
                 "crisis_level": intervention.crisis_assessment.crisis_level.value,
-                "crisis_types": [
-                    ct.value for ct in intervention.crisis_assessment.crisis_types
-                ],
+                "crisis_types": [ct.value for ct in intervention.crisis_assessment.crisis_types],
                 "user_id": intervention.user_id,
                 "session_id": intervention.session_id,
             },
@@ -1577,9 +1516,7 @@ class CrisisInterventionManager:
             response_time_ms=(time.perf_counter() - start_time) * 1000,
             metadata={
                 "crisis_level": intervention.crisis_assessment.crisis_level.value,
-                "crisis_types": [
-                    ct.value for ct in intervention.crisis_assessment.crisis_types
-                ],
+                "crisis_types": [ct.value for ct in intervention.crisis_assessment.crisis_types],
                 "risk_factors": intervention.crisis_assessment.risk_factors,
                 "protective_factors": intervention.crisis_assessment.protective_factors,
             },
@@ -1689,15 +1626,11 @@ class CrisisInterventionManager:
             },
         }
 
-    def get_intervention_status(
-        self, intervention_id: str
-    ) -> CrisisIntervention | None:
+    def get_intervention_status(self, intervention_id: str) -> CrisisIntervention | None:
         """Get the status of a specific intervention."""
         return self.active_interventions.get(intervention_id)
 
-    def resolve_intervention(
-        self, intervention_id: str, resolution_notes: str = ""
-    ) -> bool:
+    def resolve_intervention(self, intervention_id: str, resolution_notes: str = "") -> bool:
         """Mark an intervention as resolved."""
         import time
 
@@ -1726,17 +1659,13 @@ class CrisisInterventionManager:
 
         # Crisis level distribution
         crisis_levels = {}
-        for intervention in (
-            list(self.active_interventions.values()) + self.intervention_history
-        ):
+        for intervention in list(self.active_interventions.values()) + self.intervention_history:
             level = intervention.crisis_assessment.crisis_level.value
             crisis_levels[level] = crisis_levels.get(level, 0) + 1
 
         # Crisis type distribution
         crisis_types = {}
-        for intervention in (
-            list(self.active_interventions.values()) + self.intervention_history
-        ):
+        for intervention in list(self.active_interventions.values()) + self.intervention_history:
             for crisis_type in intervention.crisis_assessment.crisis_types:
                 type_name = crisis_type.value
                 crisis_types[type_name] = crisis_types.get(type_name, 0) + 1
@@ -1755,9 +1684,7 @@ class CrisisInterventionManager:
 
     def _calculate_average_response_time(self) -> float:
         """Calculate average response time for interventions."""
-        all_interventions = (
-            list(self.active_interventions.values()) + self.intervention_history
-        )
+        all_interventions = list(self.active_interventions.values()) + self.intervention_history
 
         if not all_interventions:
             return 0.0
@@ -1857,9 +1784,7 @@ class EmergencyProtocolEngine:
 
                 # If a critical step fails, abort protocol
                 if step.get("critical", False) and not step_result["success"]:
-                    raise Exception(
-                        f"Critical protocol step failed: {step_result['error']}"
-                    )
+                    raise Exception(f"Critical protocol step failed: {step_result['error']}")
 
             protocol_execution["success"] = True
             self.successful_protocols += 1
@@ -1877,9 +1802,7 @@ class EmergencyProtocolEngine:
 
         finally:
             # Calculate response time
-            protocol_execution["response_time_ms"] = (
-                time.perf_counter() - start_time
-            ) * 1000
+            protocol_execution["response_time_ms"] = (time.perf_counter() - start_time) * 1000
 
             # Move to history
             self.protocol_history.append(protocol_execution.copy())
@@ -1964,9 +1887,7 @@ class EmergencyProtocolEngine:
 
         return step_result
 
-    def _generate_protocol_response(
-        self, step: dict[str, Any], context: dict[str, Any]
-    ) -> str:
+    def _generate_protocol_response(self, step: dict[str, Any], context: dict[str, Any]) -> str:
         """Generate a protocol-specific response message."""
         template = step.get("template", "")
 
@@ -1978,9 +1899,7 @@ class EmergencyProtocolEngine:
             timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
         )
 
-    def _log_protocol_event(
-        self, step: dict[str, Any], context: dict[str, Any]
-    ) -> None:
+    def _log_protocol_event(self, step: dict[str, Any], context: dict[str, Any]) -> None:
         """Log a protocol event."""
         log_level = step.get("log_level", "info").lower()
         message = step.get("message", "Protocol event")
@@ -2029,9 +1948,7 @@ class EmergencyProtocolEngine:
         }
 
         # In a real implementation, this would contact actual emergency services
-        self.logger.critical(
-            f"EMERGENCY SERVICES CONTACT: {emergency_contact['reason']}"
-        )
+        self.logger.critical(f"EMERGENCY SERVICES CONTACT: {emergency_contact['reason']}")
 
         return emergency_contact
 
@@ -2060,9 +1977,7 @@ class EmergencyProtocolEngine:
 
         return resources
 
-    def _schedule_followup(
-        self, step: dict[str, Any], context: dict[str, Any]
-    ) -> dict[str, Any]:
+    def _schedule_followup(self, step: dict[str, Any], context: dict[str, Any]) -> dict[str, Any]:
         """Schedule follow-up contact."""
         followup = {
             "type": "followup",
@@ -2079,9 +1994,7 @@ class EmergencyProtocolEngine:
 
     def get_protocol_metrics(self) -> dict[str, Any]:
         """Get comprehensive protocol execution metrics."""
-        success_rate = (
-            self.successful_protocols / max(1, self.protocols_executed)
-        ) * 100
+        success_rate = (self.successful_protocols / max(1, self.protocols_executed)) * 100
 
         # Protocol type distribution
         protocol_types = {}
@@ -2307,9 +2220,7 @@ class HumanOversightEscalation:
             "intervention_id": intervention.intervention_id,
             "escalation_type": escalation_type,
             "crisis_level": intervention.crisis_assessment.crisis_level.value,
-            "crisis_types": [
-                ct.value for ct in intervention.crisis_assessment.crisis_types
-            ],
+            "crisis_types": [ct.value for ct in intervention.crisis_assessment.crisis_types],
             "user_id": intervention.user_id,
             "session_id": intervention.session_id,
             "timestamp": time.time(),
@@ -2348,18 +2259,14 @@ class HumanOversightEscalation:
             "escalation_type": "emergency_services",
             "emergency_type": emergency_type,
             "crisis_level": intervention.crisis_assessment.crisis_level.value,
-            "crisis_types": [
-                ct.value for ct in intervention.crisis_assessment.crisis_types
-            ],
+            "crisis_types": [ct.value for ct in intervention.crisis_assessment.crisis_types],
             "user_id": intervention.user_id,
             "session_id": intervention.session_id,
             "timestamp": time.time(),
             "emergency_contacts": [],
             "status": "critical",
             "response_time_required": "immediate",
-            "location_info": intervention.crisis_assessment.context.get(
-                "location", "unknown"
-            ),
+            "location_info": intervention.crisis_assessment.context.get("location", "unknown"),
         }
 
         self.active_escalations[escalation_id] = escalation
@@ -2387,9 +2294,7 @@ class HumanOversightEscalation:
 
         for channel in channels:
             try:
-                notification_result = self._send_notification(
-                    channel, escalation, intervention
-                )
+                notification_result = self._send_notification(channel, escalation, intervention)
                 escalation["notifications_sent"].append(notification_result)
 
                 if notification_result["success"]:
@@ -2418,9 +2323,7 @@ class HumanOversightEscalation:
 
         # Filter based on configuration
         enabled_channels = self.config.get("notification_channels", {})
-        return [
-            ch for ch in channels if enabled_channels.get(ch, {}).get("enabled", False)
-        ]
+        return [ch for ch in channels if enabled_channels.get(ch, {}).get("enabled", False)]
 
     def _send_notification(
         self, channel: str, escalation: dict[str, Any], intervention: CrisisIntervention
@@ -2489,9 +2392,7 @@ class HumanOversightEscalation:
                 f"Please review and respond immediately."
             ),
             "priority": (
-                "high"
-                if escalation["crisis_level"] in ["high", "critical"]
-                else "medium"
+                "high" if escalation["crisis_level"] in ["high", "critical"] else "medium"
             ),
             "metadata": {
                 "escalation_id": escalation["escalation_id"],
@@ -2520,9 +2421,7 @@ class HumanOversightEscalation:
     ) -> dict[str, Any]:
         """Send SMS notification (placeholder implementation)."""
         # In a real implementation, this would send actual SMS messages
-        self.logger.warning(
-            f"SMS NOTIFICATION: Crisis escalation {escalation['escalation_id']}"
-        )
+        self.logger.warning(f"SMS NOTIFICATION: Crisis escalation {escalation['escalation_id']}")
         return {
             "message_id": f"sms_{escalation['escalation_id']}",
             "recipients": self.config.get("notification_channels", {})
@@ -2536,9 +2435,7 @@ class HumanOversightEscalation:
     ) -> dict[str, Any]:
         """Send phone notification (placeholder implementation)."""
         # In a real implementation, this would make actual phone calls
-        self.logger.critical(
-            f"PHONE NOTIFICATION: Crisis escalation {escalation['escalation_id']}"
-        )
+        self.logger.critical(f"PHONE NOTIFICATION: Crisis escalation {escalation['escalation_id']}")
         return {
             "message_id": f"phone_{escalation['escalation_id']}",
             "recipients": self.config.get("notification_channels", {})
@@ -2552,9 +2449,7 @@ class HumanOversightEscalation:
     ) -> dict[str, Any]:
         """Send dashboard notification (placeholder implementation)."""
         # In a real implementation, this would update a monitoring dashboard
-        self.logger.info(
-            f"DASHBOARD NOTIFICATION: Crisis escalation {escalation['escalation_id']}"
-        )
+        self.logger.info(f"DASHBOARD NOTIFICATION: Crisis escalation {escalation['escalation_id']}")
         return {
             "message_id": f"dashboard_{escalation['escalation_id']}",
             "dashboard_url": self.config.get("notification_channels", {})
@@ -2568,9 +2463,7 @@ class HumanOversightEscalation:
     ) -> dict[str, Any]:
         """Send pager notification (placeholder implementation)."""
         # In a real implementation, this would send to pager systems
-        self.logger.critical(
-            f"PAGER NOTIFICATION: Crisis escalation {escalation['escalation_id']}"
-        )
+        self.logger.critical(f"PAGER NOTIFICATION: Crisis escalation {escalation['escalation_id']}")
         return {
             "message_id": f"pager_{escalation['escalation_id']}",
             "recipients": self.config.get("notification_channels", {})
@@ -2637,15 +2530,11 @@ class HumanOversightEscalation:
         escalation["response_time"] = time.time()
         escalation["response_notes"] = response_notes
 
-        self.logger.info(
-            f"Escalation {escalation_id} acknowledged by {human_id}: {response_notes}"
-        )
+        self.logger.info(f"Escalation {escalation_id} acknowledged by {human_id}: {response_notes}")
 
         return True
 
-    def resolve_escalation(
-        self, escalation_id: str, resolution_notes: str = ""
-    ) -> bool:
+    def resolve_escalation(self, escalation_id: str, resolution_notes: str = "") -> bool:
         """Mark an escalation as resolved."""
         import time
 
@@ -2683,15 +2572,11 @@ class HumanOversightEscalation:
                 response_time = escalation["response_time"] - escalation["timestamp"]
                 response_times.append(response_time)
 
-        avg_response_time = (
-            sum(response_times) / len(response_times) if response_times else 0.0
-        )
+        avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0
 
         # Escalation type distribution
         escalation_types = {}
-        for escalation in (
-            list(self.active_escalations.values()) + self.escalation_history
-        ):
+        for escalation in list(self.active_escalations.values()) + self.escalation_history:
             esc_type = escalation.get("escalation_type", "unknown")
             escalation_types[esc_type] = escalation_types.get(esc_type, 0) + 1
 
@@ -2843,9 +2728,7 @@ class SafetyMonitoringDashboard:
                 "status": "active",
                 "active_escalations": escalation_metrics.get("active_escalations", 0),
                 "total_escalations": escalation_metrics.get("total_escalations", 0),
-                "emergency_escalations": escalation_metrics.get(
-                    "emergency_escalations", 0
-                ),
+                "emergency_escalations": escalation_metrics.get("emergency_escalations", 0),
                 "notification_success_rate": escalation_metrics.get(
                     "notification_success_rate_percent", 0
                 ),
@@ -2895,9 +2778,7 @@ class SafetyMonitoringDashboard:
                     "active_interventions": metrics.get("active_interventions", 0),
                     "total_interventions_today": metrics.get("total_interventions", 0),
                     "emergency_contacts_today": metrics.get("emergency_contacts", 0),
-                    "average_response_time_ms": metrics.get(
-                        "average_response_time_ms", 0.0
-                    ),
+                    "average_response_time_ms": metrics.get("average_response_time_ms", 0.0),
                     "success_rate_percent": metrics.get("success_rate_percent", 0.0),
                 }
             )
@@ -2924,8 +2805,7 @@ class SafetyMonitoringDashboard:
                         "session_id": intervention.session_id,
                         "crisis_level": intervention.crisis_assessment.crisis_level.value,
                         "crisis_types": [
-                            ct.value
-                            for ct in intervention.crisis_assessment.crisis_types
+                            ct.value for ct in intervention.crisis_assessment.crisis_types
                         ],
                         "created_timestamp": intervention.created_timestamp,
                         "escalation_status": intervention.escalation_status.value,
@@ -2956,9 +2836,7 @@ class SafetyMonitoringDashboard:
                         "user_id": escalation["user_id"],
                         "timestamp": escalation["timestamp"],
                         "status": escalation["status"],
-                        "notifications_sent": len(
-                            escalation.get("notifications_sent", [])
-                        ),
+                        "notifications_sent": len(escalation.get("notifications_sent", [])),
                         "response_received": escalation.get("response_received", False),
                     }
                 )
@@ -2975,9 +2853,7 @@ class SafetyMonitoringDashboard:
                         "user_id": escalation["user_id"],
                         "timestamp": escalation["timestamp"],
                         "status": escalation["status"],
-                        "notifications_sent": len(
-                            escalation.get("notifications_sent", [])
-                        ),
+                        "notifications_sent": len(escalation.get("notifications_sent", [])),
                         "response_received": escalation.get("response_received", False),
                         "resolution_time": escalation.get("resolution_time"),
                     }
@@ -2999,12 +2875,8 @@ class SafetyMonitoringDashboard:
 
         if self.crisis_manager:
             metrics = self.crisis_manager.get_crisis_metrics()
-            trends["crisis_type_distribution"] = metrics.get(
-                "crisis_type_distribution", {}
-            )
-            trends["crisis_level_distribution"] = metrics.get(
-                "crisis_level_distribution", {}
-            )
+            trends["crisis_type_distribution"] = metrics.get("crisis_type_distribution", {})
+            trends["crisis_level_distribution"] = metrics.get("crisis_level_distribution", {})
 
         # In a real implementation, this would analyze historical data
         # For now, we'll provide placeholder trend data
@@ -3041,12 +2913,8 @@ class SafetyMonitoringDashboard:
                 "average_validation_time_ms": validator_metrics.get(
                     "average_validation_time_ms", 0.0
                 ),
-                "crisis_detection_rate": validator_metrics.get(
-                    "crisis_detection_rate", 0.0
-                ),
-                "false_positive_rate": validator_metrics.get(
-                    "false_positive_rate", 0.0
-                ),
+                "crisis_detection_rate": validator_metrics.get("crisis_detection_rate", 0.0),
+                "false_positive_rate": validator_metrics.get("false_positive_rate", 0.0),
             }
 
         # Intervention performance
@@ -3054,9 +2922,7 @@ class SafetyMonitoringDashboard:
             crisis_metrics = self.crisis_manager.get_crisis_metrics()
             metrics["intervention_performance"] = {
                 "success_rate_percent": crisis_metrics.get("success_rate_percent", 0.0),
-                "average_response_time_ms": crisis_metrics.get(
-                    "average_response_time_ms", 0.0
-                ),
+                "average_response_time_ms": crisis_metrics.get("average_response_time_ms", 0.0),
                 "escalation_rate": (
                     crisis_metrics.get("escalations_triggered", 0)
                     / max(1, crisis_metrics.get("total_interventions", 1))
@@ -3085,12 +2951,8 @@ class SafetyMonitoringDashboard:
         if self.protocol_engine:
             protocol_metrics = self.protocol_engine.get_protocol_metrics()
             metrics["protocol_performance"] = {
-                "success_rate_percent": protocol_metrics.get(
-                    "success_rate_percent", 0.0
-                ),
-                "average_response_times_ms": protocol_metrics.get(
-                    "average_response_times_ms", {}
-                ),
+                "success_rate_percent": protocol_metrics.get("success_rate_percent", 0.0),
+                "average_response_times_ms": protocol_metrics.get("average_response_times_ms", {}),
             }
 
         return metrics
@@ -3182,9 +3044,7 @@ class SafetyMonitoringDashboard:
 
         return False
 
-    def resolve_alert(
-        self, alert_id: str, resolved_by: str, resolution_notes: str = ""
-    ) -> bool:
+    def resolve_alert(self, alert_id: str, resolved_by: str, resolution_notes: str = "") -> bool:
         """Resolve an alert."""
         import time
 
@@ -3199,9 +3059,7 @@ class SafetyMonitoringDashboard:
                 self.historical_data.append(alert.copy())
                 del self.alert_queue[i]
 
-                self.logger.info(
-                    f"Alert {alert_id} resolved by {resolved_by}: {resolution_notes}"
-                )
+                self.logger.info(f"Alert {alert_id} resolved by {resolved_by}: {resolution_notes}")
                 return True
 
         return False
@@ -3254,8 +3112,8 @@ class SafetyMonitoringDashboard:
         if self.escalation_system:
             escalation_metrics = self.escalation_system.get_escalation_metrics()
             report["escalation_summary"] = escalation_metrics
-            report["executive_summary"]["emergency_escalations"] = (
-                escalation_metrics.get("emergency_escalations", 0)
+            report["executive_summary"]["emergency_escalations"] = escalation_metrics.get(
+                "emergency_escalations", 0
             )
 
         if self.protocol_engine:
@@ -3435,9 +3293,7 @@ class SafetyService:
     the underlying raw JSON changes (TTL handled by provider).
     """
 
-    def __init__(
-        self, enabled: bool = False, provider: SafetyRulesProvider | None = None
-    ) -> None:
+    def __init__(self, enabled: bool = False, provider: SafetyRulesProvider | None = None) -> None:
         self._enabled = bool(enabled)
         self._provider = provider or SafetyRulesProvider(redis_client=None)
         self._last_raw: str | None = None
@@ -3475,9 +3331,7 @@ class SafetyService:
 
 
 _global_safety_service: SafetyService | None = None
-_global_safety_locked: bool = (
-    False  # When True, do not auto-refresh from env (component-managed)
-)
+_global_safety_locked: bool = False  # When True, do not auto-refresh from env (component-managed)
 
 
 def get_global_safety_service() -> SafetyService:
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/callable_registry.py b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/callable_registry.py
index 6fc6b83ac..eec86662f 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/callable_registry.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/callable_registry.py
@@ -54,6 +54,5 @@ class CallableRegistry:
         """
         with self._lock:
             return {
-                n: {v: repr(fn) for v, fn in versions.items()}
-                for n, versions in self._map.items()
+                n: {v: repr(fn) for v, fn in versions.items()} for n, versions in self._map.items()
             }
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/coordinator.py b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/coordinator.py
index ceaaf79f9..0f5fdf9c1 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/coordinator.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/coordinator.py
@@ -17,9 +17,7 @@ FactoryFn = Callable[[], Awaitable[ToolSpec]] | Callable[[], ToolSpec]
 
 
 class ToolCoordinator:
-    def __init__(
-        self, registry: RedisToolRegistry, policy: ToolPolicy | None = None
-    ) -> None:
+    def __init__(self, registry: RedisToolRegistry, policy: ToolPolicy | None = None) -> None:
         self._registry = registry
         self._policy = policy or ToolPolicy()
         self._locks: dict[str, asyncio.Lock] = {}
@@ -37,11 +35,7 @@ class ToolCoordinator:
         # Acquire a per-signature lock to avoid duplicate creation across tasks.
         async with self._lock_for(signature):
             # Build spec via factory
-            spec = (
-                await factory_fn()
-                if asyncio.iscoroutinefunction(factory_fn)
-                else factory_fn()
-            )
+            spec = await factory_fn() if asyncio.iscoroutinefunction(factory_fn) else factory_fn()
             # Validate safety and constraints before registration
             self._policy.check_safety(spec)
             now = time.time()
@@ -59,14 +53,10 @@ class ToolCoordinator:
                 spec = existing or spec
             return spec
 
-    async def run_tool(
-        self, spec: ToolSpec, fn: Callable[..., object], *args, **kwargs
-    ):
+    async def run_tool(self, spec: ToolSpec, fn: Callable[..., object], *args, **kwargs):
         """Run a tool callable with automatic metrics collection and policy timeouts."""
         timeout_ms = (
-            self._policy.get_timeout_ms()
-            if hasattr(self._policy, "get_timeout_ms")
-            else None
+            self._policy.get_timeout_ms() if hasattr(self._policy, "get_timeout_ms") else None
         )
         res = run_with_metrics(spec.name, spec.version, fn, *args, **kwargs)
         # Async path: enforce timeout via asyncio.wait_for
@@ -99,9 +89,7 @@ class ToolCoordinator:
             t.join(timeout_ms / 1000.0)
             if t.is_alive():
                 with contextlib.suppress(Exception):
-                    get_tool_metrics().record_failure(
-                        spec.name, spec.version, timeout_ms
-                    )
+                    get_tool_metrics().record_failure(spec.name, spec.version, timeout_ms)
                 # cannot kill the thread safely; document limitation
                 raise TimeoutError(
                     f"Tool '{spec.name}:{spec.version}' execution exceeded {timeout_ms} ms"
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/invocation_service.py b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/invocation_service.py
index 1c8ab5dd6..157fe8757 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/invocation_service.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/invocation_service.py
@@ -79,9 +79,7 @@ class ToolInvocationService:
         try:
             return await self._coord.run_tool(spec, callable_fn, *args, **kwargs)
         except Exception as e:
-            logger.exception(
-                "Tool invocation by spec failed: %s %s", spec.name, spec.version
-            )
+            logger.exception("Tool invocation by spec failed: %s %s", spec.name, spec.version)
             if self._on_error:
                 return self._on_error(e, spec)
             raise
@@ -106,9 +104,7 @@ class ToolInvocationService:
         try:
             return await self._coord.run_tool(spec, callable_fn, *args, **kwargs)
         except Exception as e:
-            logger.exception(
-                "Tool register+invoke failed: %s %s", spec.name, spec.version
-            )
+            logger.exception("Tool register+invoke failed: %s %s", spec.name, spec.version)
             if self._on_error:
                 return self._on_error(e, spec)
             raise
@@ -119,9 +115,7 @@ class ToolInvocationService:
     ) -> Any:
         try:
             asyncio.get_running_loop()
-            raise RuntimeError(
-                "invoke_tool_sync cannot run inside an active event loop"
-            )
+            raise RuntimeError("invoke_tool_sync cannot run inside an active event loop")
         except RuntimeError:
             return asyncio.run(self.invoke_tool(tool_name, version, arguments))
 
@@ -130,13 +124,9 @@ class ToolInvocationService:
     ) -> Any:
         try:
             asyncio.get_running_loop()
-            raise RuntimeError(
-                "invoke_tool_by_spec_sync cannot run inside an active event loop"
-            )
+            raise RuntimeError("invoke_tool_by_spec_sync cannot run inside an active event loop")
         except RuntimeError:
-            return asyncio.run(
-                self.invoke_tool_by_spec(spec, callable_fn, *args, **kwargs)
-            )
+            return asyncio.run(self.invoke_tool_by_spec(spec, callable_fn, *args, **kwargs))
 
     def register_and_invoke_sync(
         self,
@@ -148,12 +138,8 @@ class ToolInvocationService:
     ) -> Any:
         try:
             asyncio.get_running_loop()
-            raise RuntimeError(
-                "register_and_invoke_sync cannot run inside an active event loop"
-            )
+            raise RuntimeError("register_and_invoke_sync cannot run inside an active event loop")
         except RuntimeError:
             return asyncio.run(
-                self.register_and_invoke(
-                    factory_fn, signature, callable_fn, *args, **kwargs
-                )
+                self.register_and_invoke(factory_fn, signature, callable_fn, *args, **kwargs)
             )
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/metrics.py b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/metrics.py
index 69b2f1f95..41b8df7c8 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/metrics.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/metrics.py
@@ -142,9 +142,7 @@ def tool_exec_context(name: str, version: str) -> Generator[None, None, None]:
         raise
 
 
-def run_with_metrics(
-    name: str, version: str, fn: Callable[..., Any], *args, **kwargs
-) -> Any:
+def run_with_metrics(name: str, version: str, fn: Callable[..., Any], *args, **kwargs) -> Any:
     """Run function and record metrics; supports sync and async return."""
     wrapped = tool_execution(name, version)(fn)
     return wrapped(*args, **kwargs)
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/policy_config.py b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/policy_config.py
index 6c899fc6f..ef48604d3 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/policy_config.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/policy_config.py
@@ -118,9 +118,7 @@ def load_tool_policy_config() -> ToolPolicyConfig:
     # 2b) Env boolean flags
     for env_key, field_name in _ENV_BOOL_KEYS.items():
         if env_key in os.environ:
-            base[field_name] = _parse_bool(
-                os.environ.get(env_key), base.get(field_name, True)
-            )
+            base[field_name] = _parse_bool(os.environ.get(env_key), base.get(field_name, True))
 
     # 2c) Env integer options
     for env_key, field_name in _ENV_INT_KEYS.items():
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/redis_tool_registry.py b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/redis_tool_registry.py
index 948f85239..66554c608 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/tools/redis_tool_registry.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/tools/redis_tool_registry.py
@@ -174,9 +174,7 @@ class RedisToolRegistry:
         return await self._cache.stats()
 
     async def deprecate_tool(self, name: str, version: str) -> None:
-        await self._redis.set(
-            self._status_key(name, version), ToolStatus.DEPRECATED.value
-        )
+        await self._redis.set(self._status_key(name, version), ToolStatus.DEPRECATED.value)
         key = self._key(name, version)
         cached = await self._cache.get(key)
         if cached:
@@ -214,9 +212,7 @@ class RedisToolRegistry:
             status = data.get("status", ToolStatus.ACTIVE.value)
             if status == ToolStatus.ACTIVE.value and (now - last) > max_idle_seconds:
                 # soft deprecate then remove index to allow GC by external retention policies
-                await self._redis.set(
-                    self._status_key(nm, ver), ToolStatus.DEPRECATED.value
-                )
+                await self._redis.set(self._status_key(nm, ver), ToolStatus.DEPRECATED.value)
                 await self._redis.srem(self._idx, f"{nm}:{ver}")
                 removed += 1
         return removed
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/unified_orchestrator.py b/packages/tta-ai-framework/src/tta_ai/orchestration/unified_orchestrator.py
index 37091e995..b928765b2 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/unified_orchestrator.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/unified_orchestrator.py
@@ -135,17 +135,13 @@ class UnifiedAgentOrchestrator:
         self.retry_config = retry_config or RetryConfig()
 
         # Initialize adapters
-        self.ipa_adapter = IPAAdapter(
-            fallback_to_mock=True, retry_config=self.retry_config
-        )
+        self.ipa_adapter = IPAAdapter(fallback_to_mock=True, retry_config=self.retry_config)
         self.wba_adapter = WBAAdapter(
             neo4j_manager=neo4j_manager,
             fallback_to_mock=True,
             retry_config=self.retry_config,
         )
-        self.nga_adapter = NGAAdapter(
-            fallback_to_mock=True, retry_config=self.retry_config
-        )
+        self.nga_adapter = NGAAdapter(fallback_to_mock=True, retry_config=self.retry_config)
 
         # Redis client for state persistence
         self.redis: aioredis.Redis | None = None
@@ -265,9 +261,7 @@ class UnifiedAgentOrchestrator:
                 "safety_level": state.safety_level.value,
             }
 
-    async def _process_input_phase(
-        self, state: OrchestrationState
-    ) -> OrchestrationState:
+    async def _process_input_phase(self, state: OrchestrationState) -> OrchestrationState:
         """Process input through IPA."""
         try:
             # Validate input safety
@@ -287,9 +281,7 @@ class UnifiedAgentOrchestrator:
             logger.error(f"Input processing phase error: {e}")
             raise
 
-    async def _process_world_building_phase(
-        self, state: OrchestrationState
-    ) -> OrchestrationState:
+    async def _process_world_building_phase(self, state: OrchestrationState) -> OrchestrationState:
         """Process world updates through WBA."""
         try:
             # Extract intent and entities from IPA result
@@ -324,9 +316,7 @@ class UnifiedAgentOrchestrator:
             logger.error(f"World building phase error: {e}")
             raise
 
-    async def _process_narrative_phase(
-        self, state: OrchestrationState
-    ) -> OrchestrationState:
+    async def _process_narrative_phase(self, state: OrchestrationState) -> OrchestrationState:
         """Generate narrative response through NGA."""
         try:
             # Build narrative generation prompt
@@ -441,9 +431,7 @@ class UnifiedAgentOrchestrator:
             logger.error(f"Failed to retrieve state: {e}")
             return None
 
-    async def get_session_latest_workflow(
-        self, session_id: str
-    ) -> OrchestrationState | None:
+    async def get_session_latest_workflow(self, session_id: str) -> OrchestrationState | None:
         """Get the latest workflow for a session."""
         if not self.redis:
             return None
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_manager.py b/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_manager.py
index 2c830ea20..096f58d6b 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_manager.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_manager.py
@@ -73,9 +73,7 @@ class WorkflowRunState(BaseModel):
 class WorkflowManager:
     """Registers and executes workflows with basic validation and state tracking."""
 
-    def __init__(
-        self, circuit_breaker_registry: CircuitBreakerRegistry | None = None
-    ) -> None:
+    def __init__(self, circuit_breaker_registry: CircuitBreakerRegistry | None = None) -> None:
         self._workflows: dict[str, WorkflowDefinition] = {}
         self._runs: dict[str, WorkflowRunState] = {}
         self._lg_builder = LangGraphWorkflowBuilder()
@@ -253,9 +251,7 @@ class WorkflowManager:
             duration_ms = (t1 - t0) * 1000.0
             # Record performance per agent type key
             with contextlib.suppress(Exception):
-                self._aggregator.record(
-                    step.agent.value, duration_ms, success=(error is None)
-                )
+                self._aggregator.record(step.agent.value, duration_ms, success=(error is None))
             result.ended_at = _utc_now()
         return result
 
@@ -313,9 +309,7 @@ class WorkflowManager:
                 ),
                 "graph_response": graph_response,
             },
-            performance_metrics=(
-                self._aggregator.get_metrics() if self._aggregator else {}
-            ),
+            performance_metrics=(self._aggregator.get_metrics() if self._aggregator else {}),
             therapeutic_validation=agg_safety,
         )
 
@@ -377,9 +371,7 @@ class WorkflowManager:
                     logger.warning(
                         f"Circuit breaker open for workflow {name}, attempting degraded execution"
                     )
-                    return self._execute_degraded_workflow(
-                        name, request, context, metadata
-                    )
+                    return self._execute_degraded_workflow(name, request, context, metadata)
             except Exception as e:
                 logger.warning(f"Circuit breaker check failed for workflow {name}: {e}")
                 # Continue with normal execution if circuit breaker check fails
@@ -415,12 +407,8 @@ class WorkflowManager:
                     if response:
                         return response, run_id, None
                 except CircuitBreakerOpenError:
-                    logger.warning(
-                        f"Circuit breaker opened during workflow {name} execution"
-                    )
-                    return self._execute_degraded_workflow(
-                        name, request, context, metadata
-                    )
+                    logger.warning(f"Circuit breaker opened during workflow {name} execution")
+                    return self._execute_degraded_workflow(name, request, context, metadata)
                 except Exception as e:
                     logger.error(f"Workflow {name} execution failed: {e}")
                     run_state.status = WorkflowRunStatus.FAILED
@@ -441,16 +429,12 @@ class WorkflowManager:
         return None, run_id, "Workflow execution completed without response"
 
     # ---- Circuit breaker management ----
-    async def get_circuit_breaker_status(
-        self, workflow_name: str
-    ) -> dict[str, Any] | None:
+    async def get_circuit_breaker_status(self, workflow_name: str) -> dict[str, Any] | None:
         """Get circuit breaker status for a workflow."""
         if not self._circuit_breaker_registry:
             return None
 
-        circuit_breaker = await self._circuit_breaker_registry.get(
-            f"workflow:{workflow_name}"
-        )
+        circuit_breaker = await self._circuit_breaker_registry.get(f"workflow:{workflow_name}")
         if circuit_breaker:
             return await circuit_breaker.get_metrics()
         return None
@@ -460,9 +444,7 @@ class WorkflowManager:
         if not self._circuit_breaker_registry:
             return False
 
-        circuit_breaker = await self._circuit_breaker_registry.get(
-            f"workflow:{workflow_name}"
-        )
+        circuit_breaker = await self._circuit_breaker_registry.get(f"workflow:{workflow_name}")
         if circuit_breaker:
             await circuit_breaker.reset()
             logger.info(f"Reset circuit breaker for workflow {workflow_name}")
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_monitor.py b/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_monitor.py
index 4317b4fab..f9e8c737c 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_monitor.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_monitor.py
@@ -247,9 +247,7 @@ class WorkflowMonitor:
         if not raw:
             return None
         try:
-            return self._from_dump(
-                json.loads(raw if isinstance(raw, str) else raw.decode())
-            )
+            return self._from_dump(json.loads(raw if isinstance(raw, str) else raw.decode()))
         except Exception:
             return None
 
diff --git a/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_transaction.py b/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_transaction.py
index 68b180f75..7af74d81d 100644
--- a/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_transaction.py
+++ b/packages/tta-ai-framework/src/tta_ai/orchestration/workflow_transaction.py
@@ -53,9 +53,7 @@ class WorkflowTransaction:
             tx.savepoints.append(Savepoint(name=name, created_at=created_at))
         await self._persist(tx)
 
-    async def add_cleanup(
-        self, run_id: str, savepoint: str, *, kind: str, value: str
-    ) -> None:
+    async def add_cleanup(self, run_id: str, savepoint: str, *, kind: str, value: str) -> None:
         tx = await self._load(run_id) or TxState(run_id=run_id)
         items = tx.cleanup.setdefault(savepoint, [])
         # idempotent: avoid duplicate (kind,value)
@@ -115,9 +113,7 @@ class WorkflowTransaction:
         if not raw:
             return None
         try:
-            return self._from_dump(
-                json.loads(raw if isinstance(raw, str) else raw.decode())
-            )
+            return self._from_dump(json.loads(raw if isinstance(raw, str) else raw.decode()))
         except Exception:
             return None
 
@@ -125,9 +121,7 @@ class WorkflowTransaction:
     def _dump(self, tx: TxState) -> dict[str, Any]:
         return {
             "run_id": tx.run_id,
-            "savepoints": [
-                {"name": s.name, "created_at": s.created_at} for s in tx.savepoints
-            ],
+            "savepoints": [{"name": s.name, "created_at": s.created_at} for s in tx.savepoints],
             "cleanup": {
                 sp: [{"kind": c.kind, "value": c.value, "done": c.done} for c in items]
                 for sp, items in tx.cleanup.items()
@@ -138,9 +132,7 @@ class WorkflowTransaction:
         tx = TxState(run_id=d.get("run_id"))
         for s in d.get("savepoints", []) or []:
             tx.savepoints.append(
-                Savepoint(
-                    name=s.get("name"), created_at=float(s.get("created_at") or 0)
-                )
+                Savepoint(name=s.get("name"), created_at=float(s.get("created_at") or 0))
             )
         for sp, items in (d.get("cleanup") or {}).items():
             tx.cleanup[sp] = [
diff --git a/packages/tta-ai-framework/src/tta_ai/prompts/__init__.py b/packages/tta-ai-framework/src/tta_ai/prompts/__init__.py
index 3770557c5..542b11d97 100644
--- a/packages/tta-ai-framework/src/tta_ai/prompts/__init__.py
+++ b/packages/tta-ai-framework/src/tta_ai/prompts/__init__.py
@@ -8,4 +8,3 @@ performance tracking, and A/B testing capabilities.
 from .prompt_registry import PromptMetrics, PromptRegistry, PromptTemplate
 
 __all__ = ["PromptRegistry", "PromptMetrics", "PromptTemplate"]
-
diff --git a/packages/tta-ai-framework/src/tta_ai/prompts/prompt_registry.py b/packages/tta-ai-framework/src/tta_ai/prompts/prompt_registry.py
index 8d0b2bebf..c02cc4153 100644
--- a/packages/tta-ai-framework/src/tta_ai/prompts/prompt_registry.py
+++ b/packages/tta-ai-framework/src/tta_ai/prompts/prompt_registry.py
@@ -182,9 +182,7 @@ class PromptRegistry:
         self.registry_file = prompts_dir / "registry.yaml"
 
         # Loaded prompts and metrics
-        self.prompts: dict[
-            str, dict[str, PromptTemplate]
-        ] = {}  # {prompt_id: {version: template}}
+        self.prompts: dict[str, dict[str, PromptTemplate]] = {}  # {prompt_id: {version: template}}
         self.active_versions: dict[str, str] = {}  # {prompt_id: active_version}
         self.metrics: dict[str, PromptMetrics] = {}  # {prompt_id:version: metrics}
 
@@ -280,9 +278,7 @@ class PromptRegistry:
             raise ValueError(f"No active version found for prompt '{prompt_id}'")
         return version
 
-    def render_prompt(
-        self, prompt_id: str, version: str | None = None, **kwargs: Any
-    ) -> str:
+    def render_prompt(self, prompt_id: str, version: str | None = None, **kwargs: Any) -> str:
         """
         Render a prompt with variables.
 
@@ -340,9 +336,7 @@ class PromptRegistry:
 
         return self.metrics[cache_key]
 
-    def get_baseline_scores(
-        self, prompt_id: str, version: str | None = None
-    ) -> dict[str, Any]:
+    def get_baseline_scores(self, prompt_id: str, version: str | None = None) -> dict[str, Any]:
         """Get baseline performance scores for a prompt."""
         template = self.load_prompt(prompt_id, version)
         return template.performance_baseline
@@ -366,6 +360,4 @@ class PromptRegistry:
 
     def export_metrics(self) -> dict[str, Any]:
         """Export all metrics as a dictionary."""
-        return {
-            cache_key: metrics.to_dict() for cache_key, metrics in self.metrics.items()
-        }
+        return {cache_key: metrics.to_dict() for cache_key, metrics in self.metrics.items()}
diff --git a/packages/tta-ai-framework/src/tta_ai/prompts/registry.yaml b/packages/tta-ai-framework/src/tta_ai/prompts/registry.yaml
index 3e4dbc831..9111d118d 100644
--- a/packages/tta-ai-framework/src/tta_ai/prompts/registry.yaml
+++ b/packages/tta-ai-framework/src/tta_ai/prompts/registry.yaml
@@ -26,4 +26,3 @@ prompts:
           avg_latency_ms: 1200
           quality_score: 8.2
           cost_per_call_usd: 0.0005
-
diff --git a/packages/tta-narrative-engine/src/tta_narrative/coherence/causal_validator.py b/packages/tta-narrative-engine/src/tta_narrative/coherence/causal_validator.py
index 964949235..928cdf0b9 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/coherence/causal_validator.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/coherence/causal_validator.py
@@ -52,15 +52,9 @@ class CausalValidator:
             result.detected_issues.extend(issues)
             result.causal_consistency = self._calculate_causal_consistency_score(issues)
             result.consistency_score = result.causal_consistency
-            result.is_valid = result.causal_consistency >= self.config.get(
-                "causal_threshold", 0.7
-            )
-            result.suggested_corrections = await self._generate_causal_corrections(
-                issues
-            )
-            logger.debug(
-                f"Causal validation completed: score={result.causal_consistency:.2f}"
-            )
+            result.is_valid = result.causal_consistency >= self.config.get("causal_threshold", 0.7)
+            result.suggested_corrections = await self._generate_causal_corrections(issues)
+            logger.debug(f"Causal validation completed: score={result.causal_consistency:.2f}")
             return result
         except Exception as e:
             logger.error(f"Error validating causal logic: {e}")
@@ -127,9 +121,7 @@ class CausalValidator:
     ) -> list[ConsistencyIssue]:
         issues: list[ConsistencyIssue] = []
         try:
-            causal_relationships = await self._extract_causal_relationships(
-                narrative_branch
-            )
+            causal_relationships = await self._extract_causal_relationships(narrative_branch)
             for relationship in causal_relationships:
                 issues.extend(await self._validate_causal_relationship(relationship))
             return issues
@@ -169,9 +161,7 @@ class CausalValidator:
     ) -> list[ConsistencyIssue]:
         issues: list[ConsistencyIssue] = []
         try:
-            issues.extend(
-                await self._check_consequence_proportionality(narrative_branch)
-            )
+            issues.extend(await self._check_consequence_proportionality(narrative_branch))
             issues.extend(await self._check_consequence_timing(narrative_branch))
             issues.extend(await self._check_consequence_believability(narrative_branch))
             return issues
@@ -186,9 +176,7 @@ class CausalValidator:
                 )
             ]
 
-    def _calculate_causal_consistency_score(
-        self, issues: list[ConsistencyIssue]
-    ) -> float:
+    def _calculate_causal_consistency_score(self, issues: list[ConsistencyIssue]) -> float:
         if not issues:
             return 1.0
         total_penalty = 0.0
@@ -203,9 +191,7 @@ class CausalValidator:
         max_penalty = len(issues) * 1.0
         return max(0.0, 1.0 - (total_penalty / max_penalty))
 
-    async def _generate_causal_corrections(
-        self, issues: list[ConsistencyIssue]
-    ) -> list[str]:
+    async def _generate_causal_corrections(self, issues: list[ConsistencyIssue]) -> list[str]:
         return [f"Review causal relationship: {issue.description}" for issue in issues]
 
     # Placeholders for detailed causal analysis
diff --git a/packages/tta-narrative-engine/src/tta_narrative/coherence/coherence_validator.py b/packages/tta-narrative-engine/src/tta_narrative/coherence/coherence_validator.py
index 7e022ada1..76c898d60 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/coherence/coherence_validator.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/coherence/coherence_validator.py
@@ -36,19 +36,13 @@ class CoherenceValidator:
         # Validation thresholds
         self.consistency_threshold = config.get("consistency_threshold", 0.7)
         self.lore_compliance_threshold = config.get("lore_compliance_threshold", 0.8)
-        self.character_consistency_threshold = config.get(
-            "character_consistency_threshold", 0.75
-        )
+        self.character_consistency_threshold = config.get("character_consistency_threshold", 0.75)
 
         logger.info("CoherenceValidator initialized")
 
-    async def validate_narrative_consistency(
-        self, content: NarrativeContent
-    ) -> ValidationResult:
+    async def validate_narrative_consistency(self, content: NarrativeContent) -> ValidationResult:
         try:
-            logger.debug(
-                f"Validating narrative consistency for content {content.content_id}"
-            )
+            logger.debug(f"Validating narrative consistency for content {content.content_id}")
 
             result = ValidationResult(
                 is_valid=True,
@@ -87,9 +81,7 @@ class CoherenceValidator:
                 and result.character_consistency >= self.character_consistency_threshold
             )
 
-            result.suggested_corrections = await self._generate_corrections(
-                result.detected_issues
-            )
+            result.suggested_corrections = await self._generate_corrections(result.detected_issues)
             logger.debug(
                 f"Validation completed: score={result.consistency_score:.2f}, valid={result.is_valid}"
             )
@@ -109,9 +101,7 @@ class CoherenceValidator:
                 ],
             )
 
-    async def _validate_lore_compliance(
-        self, content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _validate_lore_compliance(self, content: NarrativeContent) -> list[ConsistencyIssue]:
         issues: list[ConsistencyIssue] = []
         try:
             for character in content.characters:
@@ -126,17 +116,13 @@ class CoherenceValidator:
                 location_lore = self._get_location_lore(location)
                 if location_lore:
                     issues.extend(
-                        await self._check_location_lore_compliance(
-                            content, location, location_lore
-                        )
+                        await self._check_location_lore_compliance(content, location, location_lore)
                     )
             for theme in content.themes:
                 theme_lore = self._get_theme_lore(theme)
                 if theme_lore:
                     issues.extend(
-                        await self._check_theme_lore_compliance(
-                            content, theme, theme_lore
-                        )
+                        await self._check_theme_lore_compliance(content, theme, theme_lore)
                     )
             logger.debug(f"Found {len(issues)} lore compliance issues")
             return issues
@@ -170,19 +156,13 @@ class CoherenceValidator:
                     )
                     continue
                 issues.extend(
-                    await self._check_personality_consistency(
-                        content, character, character_profile
-                    )
+                    await self._check_personality_consistency(content, character, character_profile)
                 )
                 issues.extend(
-                    await self._check_dialogue_consistency(
-                        content, character, character_profile
-                    )
+                    await self._check_dialogue_consistency(content, character, character_profile)
                 )
                 issues.extend(
-                    await self._check_behavioral_consistency(
-                        content, character, character_profile
-                    )
+                    await self._check_behavioral_consistency(content, character, character_profile)
                 )
             logger.debug(f"Found {len(issues)} character consistency issues")
             return issues
@@ -197,9 +177,7 @@ class CoherenceValidator:
                 )
             ]
 
-    async def _validate_world_rules(
-        self, content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _validate_world_rules(self, content: NarrativeContent) -> list[ConsistencyIssue]:
         issues: list[ConsistencyIssue] = []
         try:
             issues.extend(await self._check_physics_rules(content))
@@ -340,19 +318,13 @@ class CoherenceValidator:
         return []
 
     # World rule checks (placeholders)
-    async def _check_physics_rules(
-        self, _content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _check_physics_rules(self, _content: NarrativeContent) -> list[ConsistencyIssue]:
         return []
 
-    async def _check_supernatural_rules(
-        self, _content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _check_supernatural_rules(self, _content: NarrativeContent) -> list[ConsistencyIssue]:
         return []
 
-    async def _check_social_rules(
-        self, _content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _check_social_rules(self, _content: NarrativeContent) -> list[ConsistencyIssue]:
         return []
 
     async def _check_technological_rules(
@@ -361,9 +333,7 @@ class CoherenceValidator:
         return []
 
     # Therapeutic checks (placeholders)
-    async def _check_harmful_content(
-        self, _content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _check_harmful_content(self, _content: NarrativeContent) -> list[ConsistencyIssue]:
         return []
 
     async def _check_therapeutic_concepts(
@@ -371,9 +341,7 @@ class CoherenceValidator:
     ) -> list[ConsistencyIssue]:
         return []
 
-    async def _check_emotional_safety(
-        self, _content: NarrativeContent
-    ) -> list[ConsistencyIssue]:
+    async def _check_emotional_safety(self, _content: NarrativeContent) -> list[ConsistencyIssue]:
         return []
 
     async def _check_therapeutic_progression(
@@ -385,15 +353,11 @@ class CoherenceValidator:
     def _calculate_lore_compliance_score(self, issues: list[ConsistencyIssue]) -> float:
         if not issues:
             return 1.0
-        total_penalty = sum(
-            SEVERITY_WEIGHTS_LORE.get(issue.severity.name, 0.5) for issue in issues
-        )
+        total_penalty = sum(SEVERITY_WEIGHTS_LORE.get(issue.severity.name, 0.5) for issue in issues)
         max_penalty = len(issues) * 1.0
         return max(0.0, 1.0 - (total_penalty / max_penalty))
 
-    def _calculate_character_consistency_score(
-        self, issues: list[ConsistencyIssue]
-    ) -> float:
+    def _calculate_character_consistency_score(self, issues: list[ConsistencyIssue]) -> float:
         if not issues:
             return 1.0
         total_penalty = 0.0
@@ -403,14 +367,11 @@ class CoherenceValidator:
         max_penalty = len(issues) * 1.0
         return max(0.0, 1.0 - (total_penalty / max_penalty))
 
-    def _calculate_therapeutic_alignment_score(
-        self, issues: list[ConsistencyIssue]
-    ) -> float:
+    def _calculate_therapeutic_alignment_score(self, issues: list[ConsistencyIssue]) -> float:
         if not issues:
             return 1.0
         total_penalty = sum(
-            SEVERITY_WEIGHTS_THERAPEUTIC.get(issue.severity.name, 0.5)
-            for issue in issues
+            SEVERITY_WEIGHTS_THERAPEUTIC.get(issue.severity.name, 0.5) for issue in issues
         )
         max_penalty = len(issues) * 1.0
         return max(0.0, 1.0 - (total_penalty / max_penalty))
diff --git a/packages/tta-narrative-engine/src/tta_narrative/coherence/contradiction_detector.py b/packages/tta-narrative-engine/src/tta_narrative/coherence/contradiction_detector.py
index f8262b562..21ec56e2a 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/coherence/contradiction_detector.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/coherence/contradiction_detector.py
@@ -52,34 +52,24 @@ class ContradictionDetector:
             List of detected contradictions
         """
         try:
-            logger.debug(
-                f"Detecting contradictions across {len(content_history)} content pieces"
-            )
+            logger.debug(f"Detecting contradictions across {len(content_history)} content pieces")
 
             contradictions: list[Contradiction] = []
 
             # Detect direct contradictions
-            direct_contradictions = await self._detect_direct_contradictions(
-                content_history
-            )
+            direct_contradictions = await self._detect_direct_contradictions(content_history)
             contradictions.extend(direct_contradictions)
 
             # Detect implicit contradictions
-            implicit_contradictions = await self._detect_implicit_contradictions(
-                content_history
-            )
+            implicit_contradictions = await self._detect_implicit_contradictions(content_history)
             contradictions.extend(implicit_contradictions)
 
             # Detect temporal contradictions
-            temporal_contradictions = await self._detect_temporal_contradictions(
-                content_history
-            )
+            temporal_contradictions = await self._detect_temporal_contradictions(content_history)
             contradictions.extend(temporal_contradictions)
 
             # Detect causal contradictions
-            causal_contradictions = await self._detect_causal_contradictions(
-                content_history
-            )
+            causal_contradictions = await self._detect_causal_contradictions(content_history)
             contradictions.extend(causal_contradictions)
 
             logger.info(f"Detected {len(contradictions)} contradictions")
@@ -183,9 +173,7 @@ class ContradictionDetector:
                 for j in range(i + 1, len(content_history)):
                     content1 = content_history[i]
                     content2 = content_history[j]
-                    direct_conflicts = await self._find_direct_conflicts(
-                        content1, content2
-                    )
+                    direct_conflicts = await self._find_direct_conflicts(content1, content2)
                     contradictions.extend(direct_conflicts)
             return contradictions
         except Exception as e:
@@ -202,9 +190,7 @@ class ContradictionDetector:
                 for j in range(i + 1, len(content_history)):
                     content1 = content_history[i]
                     content2 = content_history[j]
-                    implicit_conflicts = await self._find_implicit_conflicts(
-                        content1, content2
-                    )
+                    implicit_conflicts = await self._find_implicit_conflicts(content1, content2)
                     contradictions.extend(implicit_conflicts)
             return contradictions
         except Exception as e:
@@ -222,9 +208,7 @@ class ContradictionDetector:
                 for j in range(i + 1, len(temporal_events)):
                     event1 = temporal_events[i]
                     event2 = temporal_events[j]
-                    temporal_conflicts = await self._find_temporal_conflicts(
-                        event1, event2
-                    )
+                    temporal_conflicts = await self._find_temporal_conflicts(event1, event2)
                     contradictions.extend(temporal_conflicts)
             return contradictions
         except Exception as e:
diff --git a/packages/tta-narrative-engine/src/tta_narrative/generation/complexity_adapter.py b/packages/tta-narrative-engine/src/tta_narrative/generation/complexity_adapter.py
index d37de97e5..90359a1ec 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/generation/complexity_adapter.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/generation/complexity_adapter.py
@@ -53,9 +53,7 @@ class NarrativeComplexityAdapter:
         self.adaptation_rules: dict[
             EmotionalState, dict[ComplexityDimension, AdaptationStrategy]
         ] = {}
-        self.difficulty_mappings: dict[
-            DifficultyLevel, dict[ComplexityDimension, float]
-        ] = {}
+        self.difficulty_mappings: dict[DifficultyLevel, dict[ComplexityDimension, float]] = {}
 
         logger.info("NarrativeComplexityAdapter initialized")
 
@@ -73,9 +71,7 @@ class NarrativeComplexityAdapter:
             logger.error(f"NarrativeComplexityAdapter initialization failed: {e}")
             return False
 
-    async def adapt_scene_complexity(
-        self, scene: Scene, session_state: SessionState
-    ) -> Scene:
+    async def adapt_scene_complexity(self, scene: Scene, session_state: SessionState) -> Scene:
         """
         Adapt scene complexity based on current session state.
 
@@ -105,18 +101,14 @@ class NarrativeComplexityAdapter:
                     adapted_scene, dimension, strategy, session_state
                 )
 
-            logger.info(
-                f"Adapted scene complexity using strategies: {adaptation_strategies}"
-            )
+            logger.info(f"Adapted scene complexity using strategies: {adaptation_strategies}")
             return adapted_scene
 
         except Exception as e:
             logger.error(f"Failed to adapt scene complexity: {e}")
             return scene
 
-    async def simplify_for_intervention(
-        self, scene: Scene, session_state: SessionState
-    ) -> Scene:
+    async def simplify_for_intervention(self, scene: Scene, session_state: SessionState) -> Scene:
         """
         Simplify scene complexity for therapeutic interventions.
 
@@ -143,22 +135,16 @@ class NarrativeComplexityAdapter:
 
             # Ensure intervention-appropriate characteristics
             simplified_scene.difficulty_level = DifficultyLevel.GENTLE
-            simplified_scene.estimated_duration = min(
-                simplified_scene.estimated_duration, 600
-            )
+            simplified_scene.estimated_duration = min(simplified_scene.estimated_duration, 600)
 
-            logger.info(
-                f"Simplified scene for intervention: {simplified_scene.scene_id}"
-            )
+            logger.info(f"Simplified scene for intervention: {simplified_scene.scene_id}")
             return simplified_scene
 
         except Exception as e:
             logger.error(f"Failed to simplify scene for intervention: {e}")
             return scene
 
-    async def assess_scene_complexity(
-        self, scene: Scene
-    ) -> dict[ComplexityDimension, float]:
+    async def assess_scene_complexity(self, scene: Scene) -> dict[ComplexityDimension, float]:
         """
         Assess the current complexity level of a scene across all dimensions.
 
@@ -343,9 +329,7 @@ class NarrativeComplexityAdapter:
         }
 
     # Core Analysis and Adaptation Methods
-    async def _analyze_complexity_needs(
-        self, session_state: SessionState
-    ) -> dict[str, Any]:
+    async def _analyze_complexity_needs(self, session_state: SessionState) -> dict[str, Any]:
         """Analyze current session state to determine complexity needs."""
         return {
             "emotional_state": session_state.emotional_state,
@@ -353,9 +337,7 @@ class NarrativeComplexityAdapter:
             "session_duration": len(session_state.choice_history) * 300,  # Estimate
             "therapeutic_progress": session_state.therapeutic_context.progress_markers,
             "recent_performance": self._analyze_recent_performance(session_state),
-            "cognitive_load_indicators": self._assess_cognitive_load_indicators(
-                session_state
-            ),
+            "cognitive_load_indicators": self._assess_cognitive_load_indicators(session_state),
         }
 
     async def _determine_adaptation_strategies(
@@ -430,14 +412,10 @@ class NarrativeComplexityAdapter:
         recent_choices = session_state.choice_history[-3:]
 
         # Calculate average therapeutic value
-        total_value = sum(
-            choice.get("therapeutic_value", 0.5) for choice in recent_choices
-        )
+        total_value = sum(choice.get("therapeutic_value", 0.5) for choice in recent_choices)
         return total_value / len(recent_choices)
 
-    def _assess_cognitive_load_indicators(
-        self, session_state: SessionState
-    ) -> dict[str, Any]:
+    def _assess_cognitive_load_indicators(self, session_state: SessionState) -> dict[str, Any]:
         """Assess indicators of cognitive load from session state."""
         indicators = {
             "choice_time_average": 30,  # Default assumption
@@ -455,9 +433,7 @@ class NarrativeComplexityAdapter:
         return indicators
 
     # Dimension-Specific Adaptation Methods
-    async def _adapt_cognitive_load(
-        self, scene: Scene, strategy: AdaptationStrategy
-    ) -> Scene:
+    async def _adapt_cognitive_load(self, scene: Scene, strategy: AdaptationStrategy) -> Scene:
         """Adapt cognitive load of the scene."""
         if strategy in (
             AdaptationStrategy.SIMPLIFY,
@@ -478,9 +454,7 @@ class NarrativeComplexityAdapter:
 
         return scene
 
-    async def _adapt_emotional_intensity(
-        self, scene: Scene, strategy: AdaptationStrategy
-    ) -> Scene:
+    async def _adapt_emotional_intensity(self, scene: Scene, strategy: AdaptationStrategy) -> Scene:
         """Adapt emotional intensity of the scene."""
         if strategy in (
             AdaptationStrategy.SIMPLIFY,
@@ -490,9 +464,7 @@ class NarrativeComplexityAdapter:
             scene.emotional_tone = "deeply_calming"
 
             # Add calming elements
-            calming_addition = (
-                " A profound sense of peace and safety permeates this space."
-            )
+            calming_addition = " A profound sense of peace and safety permeates this space."
             scene.narrative_content += calming_addition
 
         elif strategy == AdaptationStrategy.INCREASE:
@@ -501,9 +473,7 @@ class NarrativeComplexityAdapter:
 
         return scene
 
-    async def _adapt_choice_complexity(
-        self, scene: Scene, strategy: AdaptationStrategy
-    ) -> Scene:
+    async def _adapt_choice_complexity(self, scene: Scene, strategy: AdaptationStrategy) -> Scene:
         """Adapt choice complexity (this affects future choice generation)."""
         # Store complexity preference for choice generation
         if not hasattr(scene, "choice_complexity_preference"):
@@ -519,9 +489,7 @@ class NarrativeComplexityAdapter:
 
         return scene
 
-    async def _adapt_therapeutic_depth(
-        self, scene: Scene, strategy: AdaptationStrategy
-    ) -> Scene:
+    async def _adapt_therapeutic_depth(self, scene: Scene, strategy: AdaptationStrategy) -> Scene:
         """Adapt therapeutic depth of the scene."""
         if strategy in (
             AdaptationStrategy.SIMPLIFY,
@@ -540,9 +508,7 @@ class NarrativeComplexityAdapter:
 
         return scene
 
-    async def _adapt_narrative_length(
-        self, scene: Scene, strategy: AdaptationStrategy
-    ) -> Scene:
+    async def _adapt_narrative_length(self, scene: Scene, strategy: AdaptationStrategy) -> Scene:
         """Adapt narrative length."""
         if strategy in (
             AdaptationStrategy.SIMPLIFY,
@@ -558,14 +524,14 @@ class NarrativeComplexityAdapter:
 
         elif strategy == AdaptationStrategy.INCREASE:
             # Extend narrative with additional detail
-            scene.narrative_content += " Take time to fully absorb and appreciate all the nuances of this experience."
+            scene.narrative_content += (
+                " Take time to fully absorb and appreciate all the nuances of this experience."
+            )
             scene.estimated_duration = min(scene.estimated_duration + 120, 600)
 
         return scene
 
-    async def _adapt_vocabulary_level(
-        self, scene: Scene, strategy: AdaptationStrategy
-    ) -> Scene:
+    async def _adapt_vocabulary_level(self, scene: Scene, strategy: AdaptationStrategy) -> Scene:
         """Adapt vocabulary complexity."""
         if strategy in (
             AdaptationStrategy.SIMPLIFY,
@@ -603,7 +569,9 @@ class NarrativeComplexityAdapter:
 
         elif strategy == AdaptationStrategy.INCREASE:
             # Add more abstract therapeutic concepts
-            scene.narrative_content += " Consider the deeper meanings and connections that emerge from this experience."
+            scene.narrative_content += (
+                " Consider the deeper meanings and connections that emerge from this experience."
+            )
 
         return scene
 
@@ -639,15 +607,9 @@ class NarrativeComplexityAdapter:
 
         content_lower = scene.narrative_content.lower()
 
-        high_count = sum(
-            1 for word in intensity_keywords["high"] if word in content_lower
-        )
-        medium_count = sum(
-            1 for word in intensity_keywords["medium"] if word in content_lower
-        )
-        low_count = sum(
-            1 for word in intensity_keywords["low"] if word in content_lower
-        )
+        high_count = sum(1 for word in intensity_keywords["high"] if word in content_lower)
+        medium_count = sum(1 for word in intensity_keywords["medium"] if word in content_lower)
+        low_count = sum(1 for word in intensity_keywords["low"] if word in content_lower)
 
         if high_count > 0:
             return 0.8
@@ -699,9 +661,7 @@ class NarrativeComplexityAdapter:
         ]
 
         content_lower = scene.narrative_content.lower()
-        keyword_count = sum(
-            1 for keyword in therapeutic_keywords if keyword in content_lower
-        )
+        keyword_count = sum(1 for keyword in therapeutic_keywords if keyword in content_lower)
         depth_score += min(keyword_count * 0.1, 0.6)
 
         return min(depth_score, 1.0)
@@ -776,12 +736,8 @@ class NarrativeComplexityAdapter:
 
         content_lower = scene.narrative_content.lower()
 
-        abstract_count = sum(
-            1 for concept in abstract_concepts if concept in content_lower
-        )
-        concrete_count = sum(
-            1 for concept in concrete_concepts if concept in content_lower
-        )
+        abstract_count = sum(1 for concept in abstract_concepts if concept in content_lower)
+        concrete_count = sum(1 for concept in concrete_concepts if concept in content_lower)
 
         if abstract_count + concrete_count == 0:
             return 0.5  # Neutral
diff --git a/packages/tta-narrative-engine/src/tta_narrative/generation/engine.py b/packages/tta-narrative-engine/src/tta_narrative/generation/engine.py
index 8c14d8aae..302dcdb90 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/generation/engine.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/generation/engine.py
@@ -39,9 +39,7 @@ class NarrativeEngine:
     narrative experiences.
     """
 
-    def __init__(
-        self, db_manager: Neo4jGameplayManager, config: dict[str, Any] | None = None
-    ):
+    def __init__(self, db_manager: Neo4jGameplayManager, config: dict[str, Any] | None = None):
         self.db_manager = db_manager
         self.config = config or {}
 
@@ -90,9 +88,7 @@ class NarrativeEngine:
             Generated opening scene or None if generation failed
         """
         try:
-            logger.info(
-                f"Generating opening scene for session {session_state.session_id}"
-            )
+            logger.info(f"Generating opening scene for session {session_state.session_id}")
 
             # Determine therapeutic context and goals
             therapeutic_context = session_state.therapeutic_context
@@ -120,14 +116,10 @@ class NarrativeEngine:
                 )
 
                 # Adapt complexity based on user profile
-                scene = await self.complexity_adapter.adapt_scene_complexity(
-                    scene, session_state
-                )
+                scene = await self.complexity_adapter.adapt_scene_complexity(scene, session_state)
 
                 # Apply immersion enhancements
-                scene = await self.immersion_manager.enhance_scene_immersion(
-                    scene, session_state
-                )
+                scene = await self.immersion_manager.enhance_scene_immersion(scene, session_state)
 
                 # Store scene in database and cache
                 await self.db_manager.create_scene(scene)
@@ -168,14 +160,10 @@ class NarrativeEngine:
             )
 
             # Determine scene type based on narrative flow
-            scene_type = await self._determine_next_scene_type(
-                session_state, narrative_direction
-            )
+            scene_type = await self._determine_next_scene_type(session_state, narrative_direction)
 
             # Check pacing and adjust if needed
-            pacing_adjustment = await self.pacing_controller.analyze_session_pacing(
-                session_state
-            )
+            pacing_adjustment = await self.pacing_controller.analyze_session_pacing(session_state)
 
             # Generate scene parameters
             scene_params = await self._determine_scene_parameters(
@@ -200,14 +188,10 @@ class NarrativeEngine:
                 )
 
                 # Adapt complexity
-                scene = await self.complexity_adapter.adapt_scene_complexity(
-                    scene, session_state
-                )
+                scene = await self.complexity_adapter.adapt_scene_complexity(scene, session_state)
 
                 # Enhance immersion
-                scene = await self.immersion_manager.enhance_scene_immersion(
-                    scene, session_state
-                )
+                scene = await self.immersion_manager.enhance_scene_immersion(scene, session_state)
 
                 # Apply pacing adjustments
                 scene = await self.pacing_controller.apply_pacing_adjustments(
@@ -268,10 +252,8 @@ class NarrativeEngine:
                 adapted_scene = await self._apply_crisis_adaptations(adapted_scene)
 
             # Re-enhance with therapeutic storytelling
-            adapted_scene = (
-                await self.therapeutic_storyteller.enhance_scene_with_therapy(
-                    adapted_scene, session_state.therapeutic_context
-                )
+            adapted_scene = await self.therapeutic_storyteller.enhance_scene_with_therapy(
+                adapted_scene, session_state.therapeutic_context
             )
 
             # Update immersion for new emotional context
@@ -300,9 +282,7 @@ class NarrativeEngine:
             Generated intervention scene or None if generation failed
         """
         try:
-            logger.info(
-                f"Generating therapeutic intervention scene: {intervention_type}"
-            )
+            logger.info(f"Generating therapeutic intervention scene: {intervention_type}")
 
             # Generate intervention-specific scene
             scene = await self.scene_generator.generate_intervention_scene(
@@ -348,9 +328,7 @@ class NarrativeEngine:
             "setting": "peaceful_garden",  # Default safe setting
             "atmosphere": "welcoming",
             "safety_level": "high",
-            "therapeutic_elements": therapeutic_context.primary_goals[
-                :2
-            ],  # Limit to top 2 goals
+            "therapeutic_elements": therapeutic_context.primary_goals[:2],  # Limit to top 2 goals
         }
 
         # Adjust based on emotional state
diff --git a/packages/tta-narrative-engine/src/tta_narrative/generation/immersion_manager.py b/packages/tta-narrative-engine/src/tta_narrative/generation/immersion_manager.py
index 758fa59b3..5918f92ab 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/generation/immersion_manager.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/generation/immersion_manager.py
@@ -70,9 +70,7 @@ class ImmersionManager:
             logger.error(f"ImmersionManager initialization failed: {e}")
             return False
 
-    async def enhance_scene_immersion(
-        self, scene: Scene, session_state: SessionState
-    ) -> Scene:
+    async def enhance_scene_immersion(self, scene: Scene, session_state: SessionState) -> Scene:
         """
         Enhance scene immersion based on therapeutic needs and user state.
 
@@ -129,9 +127,7 @@ class ImmersionManager:
                 "sensory_richness": await self._assess_sensory_richness(scene),
                 "atmospheric_depth": await self._assess_atmospheric_depth(scene),
                 "emotional_engagement": await self._assess_emotional_engagement(scene),
-                "interactive_potential": await self._assess_interactive_potential(
-                    scene
-                ),
+                "interactive_potential": await self._assess_interactive_potential(scene),
                 "personal_relevance": await self._assess_personal_relevance(scene),
                 "continuity_strength": await self._assess_continuity_strength(scene),
             }
@@ -370,9 +366,7 @@ class ImmersionManager:
         scene.narrative_content += f"\n\n{character_enhancement}"
         return scene
 
-    async def _apply_emotional_resonance(
-        self, scene: Scene, session_state: SessionState
-    ) -> Scene:
+    async def _apply_emotional_resonance(self, scene: Scene, session_state: SessionState) -> Scene:
         """Enhance emotional resonance based on current emotional state."""
         emotional_state = session_state.emotional_state
 
@@ -399,9 +393,7 @@ class ImmersionManager:
         scene.narrative_content += f"\n\n{interactive_enhancement}"
         return scene
 
-    async def _apply_continuity_weaving(
-        self, scene: Scene, session_state: SessionState
-    ) -> Scene:
+    async def _apply_continuity_weaving(self, scene: Scene, session_state: SessionState) -> Scene:
         """Weave continuity elements from previous experiences."""
         if session_state.choice_history:
             continuity_enhancement = (
@@ -414,9 +406,7 @@ class ImmersionManager:
 
         return scene
 
-    async def _apply_personal_connection(
-        self, scene: Scene, session_state: SessionState
-    ) -> Scene:
+    async def _apply_personal_connection(self, scene: Scene, session_state: SessionState) -> Scene:
         """Enhance personal connection and relevance."""
         therapeutic_goals = session_state.therapeutic_context.primary_goals
 
@@ -492,9 +482,7 @@ class ImmersionManager:
         ]
 
         content_lower = scene.narrative_content.lower()
-        atmospheric_count = sum(
-            1 for keyword in atmospheric_keywords if keyword in content_lower
-        )
+        atmospheric_count = sum(1 for keyword in atmospheric_keywords if keyword in content_lower)
 
         # Also check for descriptive adjectives that create atmosphere
         descriptive_keywords = [
@@ -510,9 +498,7 @@ class ImmersionManager:
             "healing",
         ]
 
-        descriptive_count = sum(
-            1 for keyword in descriptive_keywords if keyword in content_lower
-        )
+        descriptive_count = sum(1 for keyword in descriptive_keywords if keyword in content_lower)
 
         total_atmospheric = atmospheric_count + descriptive_count
         word_count = len(scene.narrative_content.split())
@@ -541,9 +527,7 @@ class ImmersionManager:
         ]
 
         content_lower = scene.narrative_content.lower()
-        emotional_count = sum(
-            1 for keyword in emotional_keywords if keyword in content_lower
-        )
+        emotional_count = sum(1 for keyword in emotional_keywords if keyword in content_lower)
 
         # Check for emotional tone indicators
         tone_indicators = [
@@ -556,13 +540,9 @@ class ImmersionManager:
             "resonates with",
         ]
 
-        tone_count = sum(
-            1 for indicator in tone_indicators if indicator in content_lower
-        )
+        tone_count = sum(1 for indicator in tone_indicators if indicator in content_lower)
 
-        total_emotional = emotional_count + (
-            tone_count * 2
-        )  # Weight tone indicators more
+        total_emotional = emotional_count + (tone_count * 2)  # Weight tone indicators more
         word_count = len(scene.narrative_content.split())
 
         if word_count == 0:
@@ -588,9 +568,7 @@ class ImmersionManager:
         ]
 
         content_lower = scene.narrative_content.lower()
-        interactive_count = sum(
-            1 for keyword in interactive_keywords if keyword in content_lower
-        )
+        interactive_count = sum(1 for keyword in interactive_keywords if keyword in content_lower)
 
         # Check for direct invitations to interact
         invitation_phrases = [
@@ -603,9 +581,7 @@ class ImmersionManager:
             "how will you",
         ]
 
-        invitation_count = sum(
-            1 for phrase in invitation_phrases if phrase in content_lower
-        )
+        invitation_count = sum(1 for phrase in invitation_phrases if phrase in content_lower)
 
         total_interactive = interactive_count + (invitation_count * 2)
         word_count = len(scene.narrative_content.split())
@@ -633,9 +609,7 @@ class ImmersionManager:
         ]
 
         content_lower = scene.narrative_content.lower()
-        personal_count = sum(
-            1 for keyword in personal_keywords if keyword in content_lower
-        )
+        personal_count = sum(1 for keyword in personal_keywords if keyword in content_lower)
 
         # Check for therapeutic relevance
         therapeutic_keywords = [
@@ -650,9 +624,7 @@ class ImmersionManager:
             "understanding",
         ]
 
-        therapeutic_count = sum(
-            1 for keyword in therapeutic_keywords if keyword in content_lower
-        )
+        therapeutic_count = sum(1 for keyword in therapeutic_keywords if keyword in content_lower)
 
         total_relevance = personal_count + therapeutic_count
         word_count = len(scene.narrative_content.split())
@@ -680,9 +652,7 @@ class ImmersionManager:
         ]
 
         content_lower = scene.narrative_content.lower()
-        continuity_count = sum(
-            1 for keyword in continuity_keywords if keyword in content_lower
-        )
+        continuity_count = sum(1 for keyword in continuity_keywords if keyword in content_lower)
 
         # Check for explicit continuity references
         continuity_phrases = [
@@ -695,9 +665,7 @@ class ImmersionManager:
             "your progress",
         ]
 
-        phrase_count = sum(
-            1 for phrase in continuity_phrases if phrase in content_lower
-        )
+        phrase_count = sum(1 for phrase in continuity_phrases if phrase in content_lower)
 
         total_continuity = continuity_count + (phrase_count * 2)
         word_count = len(scene.narrative_content.split())
diff --git a/packages/tta-narrative-engine/src/tta_narrative/generation/pacing_controller.py b/packages/tta-narrative-engine/src/tta_narrative/generation/pacing_controller.py
index 3c34f6ea9..c13f084d2 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/generation/pacing_controller.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/generation/pacing_controller.py
@@ -57,15 +57,11 @@ class PacingController:
         self.config = config or {}
 
         # Pacing configuration
-        self.pacing_rules: dict[
-            SessionPhase, dict[PacingDimension, PacingStrategy]
-        ] = {}
+        self.pacing_rules: dict[SessionPhase, dict[PacingDimension, PacingStrategy]] = {}
         self.emotional_pacing_adjustments: dict[
             EmotionalState, dict[PacingDimension, PacingStrategy]
         ] = {}
-        self.optimal_durations: dict[
-            SceneType, tuple[int, int]
-        ] = {}  # (min, max) seconds
+        self.optimal_durations: dict[SceneType, tuple[int, int]] = {}  # (min, max) seconds
         self.fatigue_thresholds: dict[str, int] = {}
 
         logger.info("PacingController initialized")
@@ -85,9 +81,7 @@ class PacingController:
             logger.error(f"PacingController initialization failed: {e}")
             return False
 
-    async def analyze_session_pacing(
-        self, session_state: SessionState
-    ) -> dict[str, Any]:
+    async def analyze_session_pacing(self, session_state: SessionState) -> dict[str, Any]:
         """
         Analyze current session pacing and recommend adjustments.
 
@@ -98,9 +92,7 @@ class PacingController:
             Dictionary with pacing analysis and recommendations
         """
         try:
-            logger.info(
-                f"Analyzing session pacing for session {session_state.session_id}"
-            )
+            logger.info(f"Analyzing session pacing for session {session_state.session_id}")
 
             # Determine current session phase
             current_phase = await self._determine_session_phase(session_state)
@@ -128,13 +120,10 @@ class PacingController:
                 == PacingStrategy.ACCELERATE,
                 "needs_deceleration": adjustments.get("overall_strategy")
                 == PacingStrategy.DECELERATE,
-                "needs_pause": adjustments.get("overall_strategy")
-                == PacingStrategy.PAUSE,
+                "needs_pause": adjustments.get("overall_strategy") == PacingStrategy.PAUSE,
             }
 
-            logger.info(
-                f"Session pacing analysis completed: {adjustments.get('overall_strategy')}"
-            )
+            logger.info(f"Session pacing analysis completed: {adjustments.get('overall_strategy')}")
             return analysis
 
         except Exception as e:
@@ -168,9 +157,7 @@ class PacingController:
                     )
 
             # Apply overall pacing strategy
-            overall_strategy = adjustments.get(
-                "overall_strategy", PacingStrategy.MAINTAIN
-            )
+            overall_strategy = adjustments.get("overall_strategy", PacingStrategy.MAINTAIN)
             adjusted_scene = await self._apply_overall_pacing_strategy(
                 adjusted_scene, overall_strategy
             )
@@ -182,9 +169,7 @@ class PacingController:
             logger.error(f"Failed to apply pacing adjustments: {e}")
             return scene
 
-    async def optimize_scene_duration(
-        self, scene: Scene, session_context: dict[str, Any]
-    ) -> Scene:
+    async def optimize_scene_duration(self, scene: Scene, session_context: dict[str, Any]) -> Scene:
         """
         Optimize scene duration based on type and context.
 
@@ -202,9 +187,7 @@ class PacingController:
 
             # Adjust based on session context
             fatigue_level = session_context.get("fatigue_level", 0.0)
-            emotional_state = session_context.get(
-                "emotional_state", EmotionalState.CALM
-            )
+            emotional_state = session_context.get("emotional_state", EmotionalState.CALM)
 
             # Calculate optimal duration
             if fatigue_level > 0.7:
@@ -334,9 +317,7 @@ class PacingController:
         }
 
     # Analysis Methods
-    async def _determine_session_phase(
-        self, session_state: SessionState
-    ) -> SessionPhase:
+    async def _determine_session_phase(self, session_state: SessionState) -> SessionPhase:
         """Determine the current phase of the therapeutic session."""
         choice_count = len(session_state.choice_history)
         session_duration = choice_count * 300  # Estimate based on choices
@@ -351,9 +332,7 @@ class PacingController:
             return SessionPhase.INTEGRATION
         return SessionPhase.CLOSURE
 
-    async def _calculate_pacing_metrics(
-        self, session_state: SessionState
-    ) -> dict[str, float]:
+    async def _calculate_pacing_metrics(self, session_state: SessionState) -> dict[str, float]:
         """Calculate current pacing metrics for the session."""
         choice_count = len(session_state.choice_history)
         estimated_duration = choice_count * 300  # Rough estimate
@@ -362,9 +341,7 @@ class PacingController:
             "session_duration": estimated_duration,
             "choice_frequency": choice_count
             / max(estimated_duration / 60, 1),  # Choices per minute
-            "therapeutic_intensity": self._calculate_therapeutic_intensity(
-                session_state
-            ),
+            "therapeutic_intensity": self._calculate_therapeutic_intensity(session_state),
             "cognitive_load": self._calculate_cognitive_load(session_state),
             "emotional_engagement": self._calculate_emotional_engagement(session_state),
             "narrative_momentum": self._calculate_narrative_momentum(session_state),
@@ -380,17 +357,14 @@ class PacingController:
 
         if estimated_duration > self.fatigue_thresholds["session_duration"]:
             duration_fatigue = min(
-                (estimated_duration - self.fatigue_thresholds["session_duration"])
-                / 1800,
+                (estimated_duration - self.fatigue_thresholds["session_duration"]) / 1800,
                 1.0,
             )
             fatigue_score += duration_fatigue * 0.4
 
         # Choice count fatigue
         if choice_count > self.fatigue_thresholds["choice_count"]:
-            choice_fatigue = min(
-                (choice_count - self.fatigue_thresholds["choice_count"]) / 10, 1.0
-            )
+            choice_fatigue = min((choice_count - self.fatigue_thresholds["choice_count"]) / 10, 1.0)
             fatigue_score += choice_fatigue * 0.3
 
         # Emotional state fatigue
@@ -414,14 +388,10 @@ class PacingController:
         base_adjustments = self.pacing_rules.get(current_phase, {}).copy()
 
         # Apply emotional state adjustments
-        emotional_adjustments = self.emotional_pacing_adjustments.get(
-            emotional_state, {}
-        )
+        emotional_adjustments = self.emotional_pacing_adjustments.get(emotional_state, {})
         for dimension, strategy in emotional_adjustments.items():
             if strategy in [PacingStrategy.PAUSE, PacingStrategy.RESET]:
-                base_adjustments[dimension] = (
-                    strategy  # Override with more urgent strategies
-                )
+                base_adjustments[dimension] = strategy  # Override with more urgent strategies
 
         # Apply fatigue adjustments
         if fatigue_level > 0.7:
@@ -470,9 +440,9 @@ class PacingController:
 
         # Recent choices indicate current load
         recent_choices = session_state.choice_history[-3:]
-        avg_complexity = sum(
-            choice.get("complexity", 0.5) for choice in recent_choices
-        ) / len(recent_choices)
+        avg_complexity = sum(choice.get("complexity", 0.5) for choice in recent_choices) / len(
+            recent_choices
+        )
 
         return min(avg_complexity, 1.0)
 
@@ -524,9 +494,7 @@ class PacingController:
             return await self._adjust_interaction_frequency(scene, strategy)
         return scene
 
-    async def _apply_overall_pacing_strategy(
-        self, scene: Scene, strategy: PacingStrategy
-    ) -> Scene:
+    async def _apply_overall_pacing_strategy(self, scene: Scene, strategy: PacingStrategy) -> Scene:
         """Apply overall pacing strategy to the scene."""
         if strategy == PacingStrategy.ACCELERATE:
             scene.narrative_content += " The energy of this moment invites you to engage more fully and explore with greater depth."
@@ -545,44 +513,30 @@ class PacingController:
 
         return scene
 
-    async def _adjust_narrative_flow(
-        self, scene: Scene, strategy: PacingStrategy
-    ) -> Scene:
+    async def _adjust_narrative_flow(self, scene: Scene, strategy: PacingStrategy) -> Scene:
         """Adjust narrative flow pacing."""
         if strategy == PacingStrategy.ACCELERATE:
-            scene.narrative_content += (
-                " The story moves forward with engaging momentum."
-            )
+            scene.narrative_content += " The story moves forward with engaging momentum."
         elif strategy == PacingStrategy.DECELERATE:
-            scene.narrative_content += (
-                " The narrative unfolds at a gentle, contemplative pace."
-            )
+            scene.narrative_content += " The narrative unfolds at a gentle, contemplative pace."
         elif strategy == PacingStrategy.PAUSE:
-            scene.narrative_content += (
-                " The story pauses here, giving you space to reflect."
-            )
+            scene.narrative_content += " The story pauses here, giving you space to reflect."
 
         return scene
 
-    async def _adjust_therapeutic_intensity(
-        self, scene: Scene, strategy: PacingStrategy
-    ) -> Scene:
+    async def _adjust_therapeutic_intensity(self, scene: Scene, strategy: PacingStrategy) -> Scene:
         """Adjust therapeutic intensity pacing."""
         if strategy == PacingStrategy.ACCELERATE:
             if "self_awareness" not in scene.therapeutic_focus:
                 scene.therapeutic_focus.append("self_awareness")
         elif strategy == PacingStrategy.DECELERATE:
-            scene.therapeutic_focus = scene.therapeutic_focus[
-                :2
-            ]  # Limit therapeutic focus
+            scene.therapeutic_focus = scene.therapeutic_focus[:2]  # Limit therapeutic focus
         elif strategy == PacingStrategy.RESET:
             scene.therapeutic_focus = ["safety", "grounding"]  # Reset to basics
 
         return scene
 
-    async def _adjust_cognitive_load(
-        self, scene: Scene, strategy: PacingStrategy
-    ) -> Scene:
+    async def _adjust_cognitive_load(self, scene: Scene, strategy: PacingStrategy) -> Scene:
         """Adjust cognitive load pacing."""
         if strategy in (PacingStrategy.DECELERATE, PacingStrategy.PAUSE):
             # Simplify content
@@ -593,9 +547,7 @@ class PacingController:
 
         return scene
 
-    async def _adjust_emotional_pacing(
-        self, scene: Scene, strategy: PacingStrategy
-    ) -> Scene:
+    async def _adjust_emotional_pacing(self, scene: Scene, strategy: PacingStrategy) -> Scene:
         """Adjust emotional pacing."""
         if strategy == PacingStrategy.DECELERATE:
             scene.emotional_tone = "gentle"
@@ -606,9 +558,7 @@ class PacingController:
 
         return scene
 
-    async def _adjust_interaction_frequency(
-        self, scene: Scene, strategy: PacingStrategy
-    ) -> Scene:
+    async def _adjust_interaction_frequency(self, scene: Scene, strategy: PacingStrategy) -> Scene:
         """Adjust interaction frequency (affects future choice generation)."""
         # Store preference for choice generation
         if not hasattr(scene, "interaction_frequency_preference"):
diff --git a/packages/tta-narrative-engine/src/tta_narrative/generation/scene_generator.py b/packages/tta-narrative-engine/src/tta_narrative/generation/scene_generator.py
index 1611cac7d..a6b94a7ac 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/generation/scene_generator.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/generation/scene_generator.py
@@ -76,16 +76,12 @@ class SceneGenerator:
             Generated Scene object or None if generation failed
         """
         try:
-            logger.info(
-                f"Generating {scene_type} scene with focus: {therapeutic_focus}"
-            )
+            logger.info(f"Generating {scene_type} scene with focus: {therapeutic_focus}")
 
             # Select appropriate template
             template = await self._select_scene_template(scene_type, therapeutic_focus)
             if not template:
-                logger.warning(
-                    f"No template found for {scene_type} with focus {therapeutic_focus}"
-                )
+                logger.warning(f"No template found for {scene_type} with focus {therapeutic_focus}")
                 return None
 
             # Generate scene content
@@ -455,11 +451,7 @@ class SceneGenerator:
         # Select setting
         setting_key = kwargs.get(
             "setting",
-            (
-                template.setting_options[0]
-                if template.setting_options
-                else "peaceful_garden"
-            ),
+            (template.setting_options[0] if template.setting_options else "peaceful_garden"),
         )
         setting = self.therapeutic_settings.get(
             setting_key, self.therapeutic_settings["peaceful_garden"]
@@ -467,9 +459,7 @@ class SceneGenerator:
 
         # Select narrative pattern
         pattern_key = (
-            template.narrative_patterns[0]
-            if template.narrative_patterns
-            else "welcoming_guide"
+            template.narrative_patterns[0] if template.narrative_patterns else "welcoming_guide"
         )
         pattern = self.narrative_patterns.get(
             pattern_key, self.narrative_patterns["welcoming_guide"]
@@ -737,6 +727,4 @@ class SceneGenerator:
             },
         }
 
-        return intervention_content.get(
-            intervention_type, intervention_content["mindfulness"]
-        )
+        return intervention_content.get(intervention_type, intervention_content["mindfulness"])
diff --git a/packages/tta-narrative-engine/src/tta_narrative/generation/therapeutic_storyteller.py b/packages/tta-narrative-engine/src/tta_narrative/generation/therapeutic_storyteller.py
index 3058eb802..ca351afd8 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/generation/therapeutic_storyteller.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/generation/therapeutic_storyteller.py
@@ -50,9 +50,7 @@ class TherapeuticStoryteller:
         self.config = config or {}
 
         # Therapeutic integration mappings
-        self.approach_techniques: dict[
-            TherapeuticApproach, list[StorytellingTechnique]
-        ] = {}
+        self.approach_techniques: dict[TherapeuticApproach, list[StorytellingTechnique]] = {}
         self.metaphor_library: dict[str, dict[str, Any]] = {}
         self.character_archetypes: dict[str, dict[str, Any]] = {}
         self.therapeutic_narratives: dict[str, dict[str, Any]] = {}
@@ -91,9 +89,7 @@ class TherapeuticStoryteller:
             logger.info(f"Enhancing scene {scene.scene_id} with therapeutic elements")
 
             # Determine primary therapeutic approach
-            primary_approach = await self._determine_therapeutic_approach(
-                therapeutic_context
-            )
+            primary_approach = await self._determine_therapeutic_approach(therapeutic_context)
 
             # Select appropriate storytelling techniques
             techniques = await self._select_storytelling_techniques(
@@ -109,13 +105,9 @@ class TherapeuticStoryteller:
                 )
 
             # Add therapeutic depth without being clinical
-            enhanced_scene = await self._add_therapeutic_depth(
-                enhanced_scene, therapeutic_context
-            )
+            enhanced_scene = await self._add_therapeutic_depth(enhanced_scene, therapeutic_context)
 
-            logger.info(
-                f"Enhanced scene with therapeutic elements using {primary_approach}"
-            )
+            logger.info(f"Enhanced scene with therapeutic elements using {primary_approach}")
             return enhanced_scene
 
         except Exception as e:
@@ -160,9 +152,7 @@ class TherapeuticStoryteller:
                 enhanced_scene.narrative_content += f"\n\n{metaphors}"
 
             # Enhance with character guidance if appropriate
-            if intervention_type not in [
-                "crisis_support"
-            ]:  # Crisis support should be direct
+            if intervention_type not in ["crisis_support"]:  # Crisis support should be direct
                 guidance = await self._add_therapeutic_guidance(intervention_type)
                 if guidance:
                     enhanced_scene.narrative_content += f"\n\n{guidance}"
@@ -198,9 +188,7 @@ class TherapeuticStoryteller:
                 return None
 
             # Adapt metaphor for emotional state and difficulty
-            return await self._adapt_metaphor(
-                base_metaphor, emotional_state, difficulty_level
-            )
+            return await self._adapt_metaphor(base_metaphor, emotional_state, difficulty_level)
 
         except Exception as e:
             logger.error(f"Failed to create therapeutic metaphor: {e}")
@@ -323,20 +311,11 @@ class TherapeuticStoryteller:
         primary_goals = therapeutic_context.primary_goals
 
         # Map goals to approaches
-        if (
-            "mindfulness" in primary_goals
-            or "present_moment_awareness" in primary_goals
-        ):
+        if "mindfulness" in primary_goals or "present_moment_awareness" in primary_goals:
             return TherapeuticApproach.MINDFULNESS
-        if (
-            "cognitive_reframing" in primary_goals
-            or "thought_patterns" in primary_goals
-        ):
+        if "cognitive_reframing" in primary_goals or "thought_patterns" in primary_goals:
             return TherapeuticApproach.CBT
-        if (
-            "emotional_regulation" in primary_goals
-            or "distress_tolerance" in primary_goals
-        ):
+        if "emotional_regulation" in primary_goals or "distress_tolerance" in primary_goals:
             return TherapeuticApproach.DBT
         if "values" in primary_goals or "acceptance" in primary_goals:
             return TherapeuticApproach.ACT
@@ -356,18 +335,14 @@ class TherapeuticStoryteller:
         if scene_type == "introduction":
             selected.append(StorytellingTechnique.CHARACTER_MODELING)
         elif scene_type == "therapeutic":
-            selected.extend(
-                [StorytellingTechnique.METAPHOR, StorytellingTechnique.GUIDED_IMAGERY]
-            )
+            selected.extend([StorytellingTechnique.METAPHOR, StorytellingTechnique.GUIDED_IMAGERY])
         elif scene_type == "challenge":
             selected.append(StorytellingTechnique.EXPERIENTIAL_LEARNING)
         else:
             selected.append(StorytellingTechnique.REFLECTIVE_DIALOGUE)
 
         # Ensure selected techniques are available for the approach
-        return [tech for tech in selected if tech in available_techniques][
-            :2
-        ]  # Limit to 2
+        return [tech for tech in selected if tech in available_techniques][:2]  # Limit to 2
 
     async def _apply_storytelling_technique(
         self,
@@ -471,9 +446,13 @@ class TherapeuticStoryteller:
         elif "depression" in therapeutic_context.primary_goals:
             depth_content = "A gentle warmth fills this space, reminding you of your inherent worth and potential."
         elif "trauma" in therapeutic_context.primary_goals:
-            depth_content = "This space holds you with complete safety, honoring your strength and resilience."
+            depth_content = (
+                "This space holds you with complete safety, honoring your strength and resilience."
+            )
         else:
-            depth_content = "This environment supports your natural capacity for growth and healing."
+            depth_content = (
+                "This environment supports your natural capacity for growth and healing."
+            )
 
         scene.narrative_content += f"\n\n{depth_content}"
         return scene
@@ -546,15 +525,17 @@ class TherapeuticStoryteller:
         elif guidance_style == "structured_practice":
             addition = f"Here, you can practice {elements[0]} through {elements[1]} in a structured, supportive way."
         elif guidance_style == "direct_support":
-            addition = f"You are surrounded by {', '.join(elements)} in this completely safe environment."
+            addition = (
+                f"You are surrounded by {', '.join(elements)} in this completely safe environment."
+            )
         else:
-            addition = f"This experience offers you {', '.join(elements[:2])} through gentle practice."
+            addition = (
+                f"This experience offers you {', '.join(elements[:2])} through gentle practice."
+            )
 
         return f"{base_content}\n\n{addition}"
 
-    async def _select_intervention_metaphors(
-        self, intervention_type: str
-    ) -> str | None:
+    async def _select_intervention_metaphors(self, intervention_type: str) -> str | None:
         """Select appropriate metaphors for intervention types."""
         metaphors = {
             "mindfulness": "Like a gentle observer watching clouds pass through the sky, you can observe your thoughts and feelings with peaceful awareness.",
diff --git a/packages/tta-narrative-engine/src/tta_narrative/orchestration/causal_graph.py b/packages/tta-narrative-engine/src/tta_narrative/orchestration/causal_graph.py
index e30a54534..fdf0b6ab7 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/orchestration/causal_graph.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/orchestration/causal_graph.py
@@ -14,9 +14,7 @@ def detect_simple_cycles(graph: dict[str, set[str]]) -> list[str]:
     issues: list[str] = []
     for src, dsts in graph.items():
         issues.extend(
-            f"Cycle between {src} and {dst}"
-            for dst in dsts
-            if dst in graph and src in graph[dst]
+            f"Cycle between {src} and {dst}" for dst in dsts if dst in graph and src in graph[dst]
         )
     return issues
 
diff --git a/packages/tta-narrative-engine/src/tta_narrative/orchestration/impact_analysis.py b/packages/tta-narrative-engine/src/tta_narrative/orchestration/impact_analysis.py
index e411f7914..30356e6b4 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/orchestration/impact_analysis.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/orchestration/impact_analysis.py
@@ -25,11 +25,7 @@ def calculate_base_magnitude(choice: PlayerChoice, scale: NarrativeScale) -> flo
         NarrativeScale.EPIC_TERM: 0.1,
     }
     base = 0.5
-    choice_type = (
-        choice.metadata.get("choice_type", "dialogue")
-        if choice.metadata
-        else "dialogue"
-    )
+    choice_type = choice.metadata.get("choice_type", "dialogue") if choice.metadata else "dialogue"
     if choice_type == "major_decision":
         base *= 1.5
     elif choice_type == "character_interaction":
@@ -39,16 +35,12 @@ def calculate_base_magnitude(choice: PlayerChoice, scale: NarrativeScale) -> flo
     return min(1.0, base * scale_multipliers.get(scale, 0.5))
 
 
-def identify_affected_elements(
-    choice: PlayerChoice, scale: NarrativeScale
-) -> list[str]:
+def identify_affected_elements(choice: PlayerChoice, scale: NarrativeScale) -> list[str]:
     elements: list[str] = []
     if scale == NarrativeScale.SHORT_TERM:
         elements.extend(["current_scene", "immediate_dialogue", "character_mood"])
     elif scale == NarrativeScale.MEDIUM_TERM:
-        elements.extend(
-            ["character_relationships", "personal_growth", "skill_development"]
-        )
+        elements.extend(["character_relationships", "personal_growth", "skill_development"])
     elif scale == NarrativeScale.LONG_TERM:
         elements.extend(["world_state", "faction_relationships", "major_plot_threads"])
     elif scale == NarrativeScale.EPIC_TERM:
@@ -66,9 +58,7 @@ def calculate_causal_strength(choice: PlayerChoice, scale: NarrativeScale) -> fl
         strength *= 1.2
     if choice.metadata and "risk_level" in choice.metadata:
         with contextlib.suppress(Exception):
-            strength *= 1.1 + 0.1 * min(
-                1.0, max(0.0, float(choice.metadata["risk_level"]))
-            )
+            strength *= 1.1 + 0.1 * min(1.0, max(0.0, float(choice.metadata["risk_level"])))
     if scale == NarrativeScale.SHORT_TERM:
         strength *= 1.3
     elif scale == NarrativeScale.MEDIUM_TERM:
@@ -99,9 +89,7 @@ def calculate_confidence_score(choice: PlayerChoice, _scale: NarrativeScale) ->
         confidence *= 1.2
     if choice.metadata and "ambiguity" in choice.metadata:
         with contextlib.suppress(Exception):
-            confidence *= (
-                1.0 - min(1.0, max(0.0, float(choice.metadata["ambiguity"]))) * 0.5
-            )
+            confidence *= 1.0 - min(1.0, max(0.0, float(choice.metadata["ambiguity"]))) * 0.5
     return min(1.0, max(0.0, confidence))
 
 
@@ -124,9 +112,7 @@ def create_narrative_event(
         causal_links={},
         description=f"Event from choice {choice.choice_id}",
         participants=[
-            choice.metadata.get("character_name", "player")
-            if choice.metadata
-            else "player"
+            choice.metadata.get("character_name", "player") if choice.metadata else "player"
         ],
         metadata={
             "choice_id": choice.choice_id,
@@ -147,9 +133,7 @@ def evaluate_cross_scale_influences(
                 if other != scale:
                     for ev in other_events:
                         ev.causal_links.setdefault("cross_scale", 0.0)
-                        ev.causal_links["cross_scale"] = max(
-                            ev.causal_links["cross_scale"], 0.2
-                        )
+                        ev.causal_links["cross_scale"] = max(ev.causal_links["cross_scale"], 0.2)
 
 
 __all__ = [
diff --git a/packages/tta-narrative-engine/src/tta_narrative/orchestration/scale_manager.py b/packages/tta-narrative-engine/src/tta_narrative/orchestration/scale_manager.py
index 26951adfd..fd49aed4e 100644
--- a/packages/tta-narrative-engine/src/tta_narrative/orchestration/scale_manager.py
+++ b/packages/tta-narrative-engine/src/tta_narrative/orchestration/scale_manager.py
@@ -80,25 +80,19 @@ class ScaleManager:
         self, choice: PlayerChoice, scales: list[NarrativeScale]
     ) -> dict[NarrativeScale, ImpactAssessment]:
         try:
-            logger.debug(
-                f"Evaluating choice impact across scales: {[s.value for s in scales]}"
-            )
+            logger.debug(f"Evaluating choice impact across scales: {[s.value for s in scales]}")
             impact_assessments: dict[NarrativeScale, ImpactAssessment] = {}
             for scale in scales:
                 assessment = await self._assess_scale_impact(choice, scale)
                 impact_assessments[scale] = assessment
                 if assessment.magnitude > 0.3:
-                    event = await self._create_narrative_event(
-                        choice, scale, assessment
-                    )
+                    event = await self._create_narrative_event(choice, scale, assessment)
                     self.active_events[scale].append(event)
             await self._evaluate_cross_scale_influences(impact_assessments)
             return impact_assessments
         except Exception as e:
             logger.error(f"Error evaluating choice impact: {e}")
-            return {
-                scale: ImpactAssessment(scale=scale, magnitude=0.0) for scale in scales
-            }
+            return {scale: ImpactAssessment(scale=scale, magnitude=0.0) for scale in scales}
 
     async def maintain_causal_relationships(self, session_id: str) -> bool:
         try:
@@ -113,15 +107,11 @@ class ScaleManager:
             logger.error(f"Error maintaining causal relationships: {e}")
             return False
 
-    async def resolve_scale_conflicts(
-        self, conflicts: list[ScaleConflict]
-    ) -> list[Resolution]:
+    async def resolve_scale_conflicts(self, conflicts: list[ScaleConflict]) -> list[Resolution]:
         try:
             logger.info(f"Resolving {len(conflicts)} scale conflicts")
             resolutions: list[Resolution] = []
-            sorted_conflicts = sorted(
-                conflicts, key=lambda c: (c.resolution_priority, -c.severity)
-            )
+            sorted_conflicts = sorted(conflicts, key=lambda c: (c.resolution_priority, -c.severity))
             for conflict in sorted_conflicts:
                 resolution = await self._generate_conflict_resolution(conflict)
                 if resolution:
@@ -148,9 +138,7 @@ class ScaleManager:
     def get_scale_window(self, scale: NarrativeScale) -> int:
         return self.scale_windows.get(scale, 300)
 
-    def get_active_events(
-        self, scale: NarrativeScale | None = None
-    ) -> list[NarrativeEvent]:
+    def get_active_events(self, scale: NarrativeScale | None = None) -> list[NarrativeEvent]:
         if scale:
             return self.active_events.get(scale, [])
         all_events: list[NarrativeEvent] = []
@@ -178,9 +166,7 @@ class ScaleManager:
             temporal_decay=temporal_decay,
         )
 
-    def _calculate_base_magnitude(
-        self, choice: PlayerChoice, scale: NarrativeScale
-    ) -> float:
+    def _calculate_base_magnitude(self, choice: PlayerChoice, scale: NarrativeScale) -> float:
         scale_multipliers = {
             NarrativeScale.SHORT_TERM: 0.8,
             NarrativeScale.MEDIUM_TERM: 0.5,
@@ -189,9 +175,7 @@ class ScaleManager:
         }
         base = 0.5
         choice_type = (
-            choice.metadata.get("choice_type", "dialogue")
-            if choice.metadata
-            else "dialogue"
+            choice.metadata.get("choice_type", "dialogue") if choice.metadata else "dialogue"
         )
         if choice_type == "major_decision":
             base *= 1.5
@@ -208,13 +192,9 @@ class ScaleManager:
         if scale == NarrativeScale.SHORT_TERM:
             elements.extend(["current_scene", "immediate_dialogue", "character_mood"])
         elif scale == NarrativeScale.MEDIUM_TERM:
-            elements.extend(
-                ["character_relationships", "personal_growth", "skill_development"]
-            )
+            elements.extend(["character_relationships", "personal_growth", "skill_development"])
         elif scale == NarrativeScale.LONG_TERM:
-            elements.extend(
-                ["world_state", "faction_relationships", "major_plot_threads"]
-            )
+            elements.extend(["world_state", "faction_relationships", "major_plot_threads"])
         elif scale == NarrativeScale.EPIC_TERM:
             elements.extend(["generational_legacy", "world_history", "cultural_impact"])
         if choice.metadata and "character_name" in choice.metadata:
@@ -235,9 +215,7 @@ class ScaleManager:
         # moved to impact_analysis.assess_therapeutic_alignment
         return assess_therapeutic_alignment(choice, scale)
 
-    def _calculate_confidence_score(
-        self, choice: PlayerChoice, scale: NarrativeScale
-    ) -> float:
+    def _calculate_confidence_score(self, choice: PlayerChoice, scale: NarrativeScale) -> float:
         # moved to impact_analysis.calculate_confidence_score
         return calculate_confidence_score(choice, scale)
 
@@ -282,9 +260,7 @@ class ScaleManager:
         for scale, events in self.active_events.items():
             window = self.get_scale_window(scale)
             cutoff_time = cutoff_now - window
-            self.active_events[scale] = [
-                e for e in events if e.timestamp.timestamp() > cutoff_time
-            ]
+            self.active_events[scale] = [e for e in events if e.timestamp.timestamp() > cutoff_time]
 
     async def _detect_temporal_conflicts(self) -> list[ScaleConflict]:
         all_events = self.get_active_events()
@@ -302,9 +278,7 @@ class ScaleManager:
         all_events = self.get_active_events()
         return detect_therapeutic_conflicts(all_events)
 
-    async def _generate_conflict_resolution(
-        self, conflict: ScaleConflict
-    ) -> Resolution | None:
+    async def _generate_conflict_resolution(self, conflict: ScaleConflict) -> Resolution | None:
         return build_simple_resolution(conflict)
 
     async def _implement_resolution(self, resolution: Resolution) -> None:
diff --git a/scripts/dev_with_recovery.py b/scripts/dev_with_recovery.py
index 14fbfc5b5..1b806fa7b 100644
--- a/scripts/dev_with_recovery.py
+++ b/scripts/dev_with_recovery.py
@@ -12,26 +12,20 @@ Usage:
     python scripts/dev_with_recovery.py check-all
 """
 
+import logging
 import subprocess
 import sys
-import logging
 from pathlib import Path
 from typing import NoReturn
 
 # Add scripts/primitives to path
 sys.path.insert(0, str(Path(__file__).parent / "primitives"))
 
-from error_recovery import (
-    with_retry,
-    RetryConfig,
-    ErrorCategory,
-    classify_error
-)
+from error_recovery import RetryConfig, classify_error, with_retry
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
 
@@ -40,21 +34,23 @@ logger = logging.getLogger(__name__)
 # Development Commands with Error Recovery
 # ============================================================================
 
+
 @with_retry(RetryConfig(max_retries=2, base_delay=1.0))
 def run_linting() -> str:
     """Run linting with retry on transient failures."""
     logger.info("Running Ruff linter...")
     result = subprocess.run(
         ["uvx", "ruff", "check", "src/", "tests/"],
+        check=False,
         capture_output=True,
-        text=True
+        text=True,
     )
-    
+
     if result.returncode != 0:
         # Linting errors are permanent, not transient
         # But we still wrap in retry in case uvx itself has issues
         raise RuntimeError(f"Linting failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Linting passed")
     return result.stdout
 
@@ -65,13 +61,14 @@ def run_linting_fix() -> str:
     logger.info("Running Ruff linter with auto-fix...")
     result = subprocess.run(
         ["uvx", "ruff", "check", "--fix", "src/", "tests/"],
+        check=False,
         capture_output=True,
-        text=True
+        text=True,
     )
-    
+
     if result.returncode != 0:
         raise RuntimeError(f"Linting with auto-fix failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Linting fixes applied")
     return result.stdout
 
@@ -82,13 +79,14 @@ def run_formatting() -> str:
     logger.info("Formatting code with Ruff...")
     result = subprocess.run(
         ["uvx", "ruff", "format", "src/", "tests/"],
+        check=False,
         capture_output=True,
-        text=True
+        text=True,
     )
-    
+
     if result.returncode != 0:
         raise RuntimeError(f"Formatting failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Code formatted")
     return result.stdout
 
@@ -99,13 +97,14 @@ def run_format_check() -> str:
     logger.info("Checking code formatting...")
     result = subprocess.run(
         ["uvx", "ruff", "format", "--check", "src/", "tests/"],
+        check=False,
         capture_output=True,
-        text=True
+        text=True,
     )
-    
+
     if result.returncode != 0:
         raise RuntimeError(f"Format check failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Format check passed")
     return result.stdout
 
@@ -115,14 +114,12 @@ def run_type_checking() -> str:
     """Run type checking with retry on transient failures."""
     logger.info("Running Pyright type checker...")
     result = subprocess.run(
-        ["uvx", "pyright", "src/"],
-        capture_output=True,
-        text=True
+        ["uvx", "pyright", "src/"], check=False, capture_output=True, text=True
     )
-    
+
     if result.returncode != 0:
         raise RuntimeError(f"Type checking failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Type checking passed")
     return result.stdout
 
@@ -131,20 +128,16 @@ def run_type_checking() -> str:
 def run_tests(args: list[str] | None = None) -> str:
     """Run tests with retry on transient failures."""
     logger.info("Running tests...")
-    
+
     cmd = ["uvx", "pytest", "tests/"]
     if args:
         cmd.extend(args)
-    
-    result = subprocess.run(
-        cmd,
-        capture_output=True,
-        text=True
-    )
-    
+
+    result = subprocess.run(cmd, check=False, capture_output=True, text=True)
+
     if result.returncode != 0:
         raise RuntimeError(f"Tests failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Tests passed")
     return result.stdout
 
@@ -154,14 +147,22 @@ def run_tests_with_coverage() -> str:
     """Run tests with coverage and retry on transient failures."""
     logger.info("Running tests with coverage...")
     result = subprocess.run(
-        ["uvx", "pytest", "tests/", "--cov=src", "--cov-report=html", "--cov-report=term"],
+        [
+            "uvx",
+            "pytest",
+            "tests/",
+            "--cov=src",
+            "--cov-report=html",
+            "--cov-report=term",
+        ],
+        check=False,
         capture_output=True,
-        text=True
+        text=True,
     )
-    
+
     if result.returncode != 0:
         raise RuntimeError(f"Tests with coverage failed:\n{result.stdout}")
-    
+
     logger.info("âœ“ Tests with coverage passed")
     logger.info("Coverage report: htmlcov/index.html")
     return result.stdout
@@ -171,15 +172,11 @@ def run_tests_with_coverage() -> str:
 def install_dependencies() -> str:
     """Install dependencies with aggressive retry on network failures."""
     logger.info("Installing dependencies...")
-    result = subprocess.run(
-        ["uv", "sync"],
-        capture_output=True,
-        text=True
-    )
-    
+    result = subprocess.run(["uv", "sync"], check=False, capture_output=True, text=True)
+
     if result.returncode != 0:
         raise RuntimeError(f"Dependency installation failed:\n{result.stderr}")
-    
+
     logger.info("âœ“ Dependencies installed")
     return result.stdout
 
@@ -188,12 +185,13 @@ def install_dependencies() -> str:
 # Composite Commands
 # ============================================================================
 
+
 def cmd_quality() -> bool:
     """Run quality checks (lint + format-check)."""
     logger.info("=" * 60)
     logger.info("Running quality checks...")
     logger.info("=" * 60)
-    
+
     try:
         run_linting()
         run_format_check()
@@ -209,7 +207,7 @@ def cmd_quality_fix() -> bool:
     logger.info("=" * 60)
     logger.info("Running quality fixes...")
     logger.info("=" * 60)
-    
+
     try:
         run_linting_fix()
         run_formatting()
@@ -225,7 +223,7 @@ def cmd_check_all() -> bool:
     logger.info("=" * 60)
     logger.info("Running full validation...")
     logger.info("=" * 60)
-    
+
     try:
         run_linting()
         run_format_check()
@@ -243,7 +241,7 @@ def cmd_dev_check() -> bool:
     logger.info("=" * 60)
     logger.info("Running quick dev check...")
     logger.info("=" * 60)
-    
+
     try:
         run_linting_fix()
         run_formatting()
@@ -260,7 +258,7 @@ def cmd_setup() -> bool:
     logger.info("=" * 60)
     logger.info("Setting up development environment...")
     logger.info("=" * 60)
-    
+
     try:
         install_dependencies()
         logger.info("\nâœ“ Development environment ready!\n")
@@ -274,6 +272,7 @@ def cmd_setup() -> bool:
 # CLI
 # ============================================================================
 
+
 def print_usage() -> None:
     """Print usage information."""
     print("""
@@ -289,7 +288,7 @@ Commands:
   typecheck         Run type checking
   test              Run tests
   test-cov          Run tests with coverage
-  
+
   quality           Run quality checks (lint + format-check)
   quality-fix       Run quality fixes (lint-fix + format)
   check-all         Run full validation (quality + typecheck + test)
@@ -305,9 +304,9 @@ def main() -> NoReturn:
     if len(sys.argv) < 2:
         print_usage()
         sys.exit(1)
-    
+
     command = sys.argv[1]
-    
+
     commands = {
         "lint": lambda: run_linting() and True,
         "lint-fix": lambda: run_linting_fix() and True,
@@ -322,12 +321,12 @@ def main() -> NoReturn:
         "dev-check": cmd_dev_check,
         "setup": cmd_setup,
     }
-    
+
     if command not in commands:
         logger.error(f"Unknown command: {command}")
         print_usage()
         sys.exit(1)
-    
+
     try:
         success = commands[command]()
         sys.exit(0 if success else 1)
@@ -342,4 +341,3 @@ def main() -> NoReturn:
 
 if __name__ == "__main__":
     main()
-
diff --git a/scripts/diagnose_agents.py b/scripts/diagnose_agents.py
index ee689de43..03598c52d 100644
--- a/scripts/diagnose_agents.py
+++ b/scripts/diagnose_agents.py
@@ -10,7 +10,6 @@ Tests and validates the agent orchestration system:
 """
 
 import asyncio
-import json
 import logging
 import os
 import sys
@@ -79,9 +78,7 @@ async def test_neo4j_connection():
         neo4j_user = os.getenv("NEO4J_USER", "neo4j")
         neo4j_password = os.getenv("NEO4J_PASSWORD", "tta_dev_password_2024")
 
-        driver = AsyncGraphDatabase.driver(
-            neo4j_uri, auth=(neo4j_user, neo4j_password)
-        )
+        driver = AsyncGraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
 
         await driver.verify_connectivity()
         logger.info("âœ… Neo4j connection successful")
@@ -301,9 +298,8 @@ async def run_diagnostics():
     if all_passed:
         logger.info("ðŸŽ‰ All diagnostics passed!")
         return 0
-    else:
-        logger.error("âš ï¸  Some diagnostics failed. Check logs above.")
-        return 1
+    logger.error("âš ï¸  Some diagnostics failed. Check logs above.")
+    return 1
 
 
 if __name__ == "__main__":
diff --git a/scripts/gemini-cli-env.sh b/scripts/gemini-cli-env.sh
index 189a51cec..490c6503c 100755
--- a/scripts/gemini-cli-env.sh
+++ b/scripts/gemini-cli-env.sh
@@ -35,4 +35,3 @@ echo "  âœ“ mcp-neo4j (env vars configured)"
 echo "  âœ“ mcp-redis (env vars configured)"
 echo ""
 echo "Usage: gemini \"Your prompt here\""
-
diff --git a/scripts/process-performance-metrics.js b/scripts/process-performance-metrics.js
index 56c8f8157..129233127 100644
--- a/scripts/process-performance-metrics.js
+++ b/scripts/process-performance-metrics.js
@@ -277,4 +277,3 @@ function main() {
 }
 
 main();
-
diff --git a/scripts/send-slack-notification.js b/scripts/send-slack-notification.js
index 84bc0cd62..bf556cbcc 100644
--- a/scripts/send-slack-notification.js
+++ b/scripts/send-slack-notification.js
@@ -228,4 +228,3 @@ async function main() {
 }
 
 main();
-
diff --git a/scripts/test-logout-flow.sh b/scripts/test-logout-flow.sh
index ad7c32e1e..f5a13a19f 100755
--- a/scripts/test-logout-flow.sh
+++ b/scripts/test-logout-flow.sh
@@ -156,4 +156,3 @@ main() {
 
 # Run main function
 main "$@"
-
diff --git a/src/agent_orchestration/admin/recover.py b/src/agent_orchestration/admin/recover.py
index 8bea9ddfd..bd483b08c 100644
--- a/src/agent_orchestration/admin/recover.py
+++ b/src/agent_orchestration/admin/recover.py
@@ -3,7 +3,6 @@ from __future__ import annotations
 import asyncio
 
 from redis.asyncio import Redis
-
 from tta_ai.orchestration.coordinators import RedisMessageCoordinator
 from tta_ai.orchestration.models import AgentId, AgentType
 
diff --git a/src/agent_orchestration/langgraph_orchestrator.py b/src/agent_orchestration/langgraph_orchestrator.py
index 8a47ad838..fcd0eaa29 100644
--- a/src/agent_orchestration/langgraph_orchestrator.py
+++ b/src/agent_orchestration/langgraph_orchestrator.py
@@ -16,7 +16,6 @@ import redis.asyncio as aioredis
 from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
 from langchain_openai import ChatOpenAI
 from langgraph.graph import END, StateGraph
-
 from tta_ai.prompts import PromptRegistry
 
 from .unified_orchestrator import UnifiedAgentOrchestrator
@@ -271,7 +270,9 @@ class LangGraphAgentOrchestrator:
                 tokens=len(response.content.split()),  # Approximate
                 latency_ms=latency_ms,
                 cost_usd=0.0002,  # Approximate based on model
-                quality_score=8.5 if assessment["safety_level"] in ["safe", "concern"] else 7.0,
+                quality_score=8.5
+                if assessment["safety_level"] in ["safe", "concern"]
+                else 7.0,
             )
 
         except Exception as e:
@@ -337,7 +338,9 @@ class LangGraphAgentOrchestrator:
         start_time = time.time()
         try:
             # Prepare context
-            intent = state.get("ipa_result", {}).get("routing", {}).get("intent", "unknown")
+            intent = (
+                state.get("ipa_result", {}).get("routing", {}).get("intent", "unknown")
+            )
             world_context = json.dumps(state["world_context"], indent=2)[:200]
 
             # Use prompt registry for narrative generation
diff --git a/src/components/agent_orchestration_component.py b/src/components/agent_orchestration_component.py
index 4353b9b00..3b13ce859 100644
--- a/src/components/agent_orchestration_component.py
+++ b/src/components/agent_orchestration_component.py
@@ -2243,7 +2243,6 @@ class AgentOrchestrationComponent(Component):
                 if not sp:
                     sp = "start"
                 import redis.asyncio as aioredis
-
                 from tta_ai.orchestration.workflow_transaction import (
                     WorkflowTransaction,
                 )
diff --git a/src/components/gameplay_loop/base.py b/src/components/gameplay_loop/base.py
index 6e91208c1..8f610c81d 100644
--- a/src/components/gameplay_loop/base.py
+++ b/src/components/gameplay_loop/base.py
@@ -16,6 +16,7 @@ from enum import Enum
 from typing import Any
 
 from tta_ai.orchestration.service import AgentOrchestrationService
+
 from src.components.base import Component
 
 logger = logging.getLogger(__name__)
diff --git a/testing/comprehensive_validation/context_engineering_validator.py b/testing/comprehensive_validation/context_engineering_validator.py
index bb6850056..60f6b293f 100644
--- a/testing/comprehensive_validation/context_engineering_validator.py
+++ b/testing/comprehensive_validation/context_engineering_validator.py
@@ -22,8 +22,9 @@ from dataclasses import dataclass, field
 from enum import Enum
 from typing import Any
 
-from enhanced_preference_ai_server import EnhancedContext, PlayerPreferences
 from tta_ai.orchestration.models import AgentType
+
+from enhanced_preference_ai_server import EnhancedContext, PlayerPreferences
 from src.components.therapeutic_systems_enhanced.therapeutic_integration_system import (
     TherapeuticIntegrationSystem,
 )
@@ -299,14 +300,13 @@ class ContextEngineeringValidator:
         # Generate agent-specific context
         if agent_type == AgentType.NGA:
             return await self._generate_narrative_context(enhanced_context, scenario)
-        elif agent_type == AgentType.IPA:
+        if agent_type == AgentType.IPA:
             return await self._generate_therapeutic_context(enhanced_context, scenario)
-        elif agent_type == AgentType.WBA:
+        if agent_type == AgentType.WBA:
             return await self._generate_worldbuilding_context(
                 enhanced_context, scenario
             )
-        else:
-            raise ValueError(f"Unsupported agent type: {agent_type}")
+        raise ValueError(f"Unsupported agent type: {agent_type}")
 
     async def _generate_narrative_context(
         self, enhanced_context: EnhancedContext, scenario: TestScenario
@@ -579,8 +579,7 @@ class ContextEngineeringValidator:
                 # Check if it's mentioned in safety context (acceptable)
                 if "avoid" in context_str or "trigger" in context_str:
                     continue  # Acceptable mention in safety context
-                else:
-                    score -= 2.0  # Penalty for inappropriate trigger mention
+                score -= 2.0  # Penalty for inappropriate trigger mention
 
         # Check for safety protocols
         if "safety" in context_str:
@@ -588,10 +587,11 @@ class ContextEngineeringValidator:
 
         # Check intensity level compliance
         intensity = scenario.player_preferences.intensity_level
-        if intensity == "gentle" and "gentle" in context_str:
-            score += 0.3
-        elif intensity == "high" and (
-            "challenge" in context_str or "intensive" in context_str
+        if (
+            intensity == "gentle"
+            and "gentle" in context_str
+            or intensity == "high"
+            and ("challenge" in context_str or "intensive" in context_str)
         ):
             score += 0.3
 
diff --git a/testing/comprehensive_validation/end_to_end_journey_validator.py b/testing/comprehensive_validation/end_to_end_journey_validator.py
index 31f86aa25..7a713e388 100644
--- a/testing/comprehensive_validation/end_to_end_journey_validator.py
+++ b/testing/comprehensive_validation/end_to_end_journey_validator.py
@@ -410,36 +410,35 @@ class EndToEndJourneyValidator:
 
         if stage == JourneyStage.ONBOARDING:
             return await self._execute_onboarding_stage(scenario, journey_context)
-        elif stage == JourneyStage.PREFERENCE_SELECTION:
+        if stage == JourneyStage.PREFERENCE_SELECTION:
             return await self._execute_preference_selection_stage(
                 scenario, journey_context
             )
-        elif stage == JourneyStage.CHARACTER_CREATION:
+        if stage == JourneyStage.CHARACTER_CREATION:
             return await self._execute_character_creation_stage(
                 scenario, journey_context
             )
-        elif stage == JourneyStage.WORLD_INITIALIZATION:
+        if stage == JourneyStage.WORLD_INITIALIZATION:
             return await self._execute_world_initialization_stage(
                 scenario, journey_context
             )
-        elif stage == JourneyStage.NARRATIVE_GENERATION:
+        if stage == JourneyStage.NARRATIVE_GENERATION:
             return await self._execute_narrative_generation_stage(
                 scenario, journey_context
             )
-        elif stage == JourneyStage.THERAPEUTIC_INTERACTION:
+        if stage == JourneyStage.THERAPEUTIC_INTERACTION:
             return await self._execute_therapeutic_interaction_stage(
                 scenario, journey_context
             )
-        elif stage == JourneyStage.SESSION_PERSISTENCE:
+        if stage == JourneyStage.SESSION_PERSISTENCE:
             return await self._execute_session_persistence_stage(
                 scenario, journey_context
             )
-        elif stage == JourneyStage.JOURNEY_COMPLETION:
+        if stage == JourneyStage.JOURNEY_COMPLETION:
             return await self._execute_journey_completion_stage(
                 scenario, journey_context
             )
-        else:
-            return {"success": False, "error": f"Unknown stage: {stage}"}
+        return {"success": False, "error": f"Unknown stage: {stage}"}
 
     async def _execute_onboarding_stage(
         self, scenario: UserJourneyScenario, journey_context: dict[str, Any]
diff --git a/testing/comprehensive_validation/multi_agent_collaboration_validator.py b/testing/comprehensive_validation/multi_agent_collaboration_validator.py
index 7f54066a1..c24649d5e 100644
--- a/testing/comprehensive_validation/multi_agent_collaboration_validator.py
+++ b/testing/comprehensive_validation/multi_agent_collaboration_validator.py
@@ -24,6 +24,7 @@ from typing import Any
 from tta_ai.orchestration.models import AgentType
 from tta_ai.orchestration.service import AgentOrchestrationService
 from tta_ai.orchestration.workflow_manager import WorkflowManager
+
 from src.ai_components.langgraph_integration import TherapeuticWorkflowManager
 
 logger = logging.getLogger(__name__)
diff --git a/tests/agent_orchestration/test_agent_orchestration_service.py b/tests/agent_orchestration/test_agent_orchestration_service.py
index 9d5683895..11f327e47 100644
--- a/tests/agent_orchestration/test_agent_orchestration_service.py
+++ b/tests/agent_orchestration/test_agent_orchestration_service.py
@@ -10,7 +10,6 @@ import time
 from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
-
 from tta_ai.orchestration import (
     AgentContext,
     AgentId,
diff --git a/tests/agent_orchestration/test_agent_orchestration_service_integration.py b/tests/agent_orchestration/test_agent_orchestration_service_integration.py
index 685e41ed9..2f9385c4d 100644
--- a/tests/agent_orchestration/test_agent_orchestration_service_integration.py
+++ b/tests/agent_orchestration/test_agent_orchestration_service_integration.py
@@ -10,7 +10,6 @@ from unittest.mock import Mock, patch
 import pytest
 import pytest_asyncio
 import redis.asyncio as aioredis
-
 from tta_ai.orchestration import (
     AgentContext,
     AgentId,
diff --git a/tests/agent_orchestration/test_agent_router_selection.py b/tests/agent_orchestration/test_agent_router_selection.py
index 9483a445c..36de24a0c 100644
--- a/tests/agent_orchestration/test_agent_router_selection.py
+++ b/tests/agent_orchestration/test_agent_router_selection.py
@@ -1,5 +1,4 @@
 import pytest
-
 from tta_ai.orchestration.models import AgentId, AgentType
 from tta_ai.orchestration.router import AgentRouter
 
diff --git a/tests/agent_orchestration/test_agent_state_restore_on_register.py b/tests/agent_orchestration/test_agent_state_restore_on_register.py
index 7d306630a..ec070d3e5 100644
--- a/tests/agent_orchestration/test_agent_state_restore_on_register.py
+++ b/tests/agent_orchestration/test_agent_state_restore_on_register.py
@@ -2,7 +2,6 @@ import asyncio
 import json
 
 import pytest
-
 from tta_ai.orchestration.proxies import InputProcessorAgentProxy
 from tta_ai.orchestration.registries import RedisAgentRegistry
 
diff --git a/tests/agent_orchestration/test_agents_and_proxies.py b/tests/agent_orchestration/test_agents_and_proxies.py
index 4e8ec2b4c..c8de2a4c6 100644
--- a/tests/agent_orchestration/test_agents_and_proxies.py
+++ b/tests/agent_orchestration/test_agents_and_proxies.py
@@ -1,7 +1,6 @@
 import asyncio
 
 import pytest
-
 from tta_ai.orchestration import (
     AgentRegistry,
     AgentType,
diff --git a/tests/agent_orchestration/test_callable_registry_and_execute_endpoint.py b/tests/agent_orchestration/test_callable_registry_and_execute_endpoint.py
index 05cdf17a9..e70b337e1 100644
--- a/tests/agent_orchestration/test_callable_registry_and_execute_endpoint.py
+++ b/tests/agent_orchestration/test_callable_registry_and_execute_endpoint.py
@@ -3,9 +3,9 @@ import os
 
 import pytest
 from starlette.testclient import TestClient
-
 from tta_ai.orchestration.tools.metrics import get_tool_metrics
 from tta_ai.orchestration.tools.models import ToolSpec
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/test_capability_system_integration.py b/tests/agent_orchestration/test_capability_system_integration.py
index 82a10b898..06473f086 100644
--- a/tests/agent_orchestration/test_capability_system_integration.py
+++ b/tests/agent_orchestration/test_capability_system_integration.py
@@ -9,7 +9,6 @@ import asyncio
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.api.diagnostics import DiagnosticsAPI
 from tta_ai.orchestration.capabilities.auto_discovery import (
     AutoDiscoveryManager,
diff --git a/tests/agent_orchestration/test_circuit_breaker_integration.py b/tests/agent_orchestration/test_circuit_breaker_integration.py
index 105d2249a..c6d0c5dbe 100644
--- a/tests/agent_orchestration/test_circuit_breaker_integration.py
+++ b/tests/agent_orchestration/test_circuit_breaker_integration.py
@@ -8,7 +8,6 @@ metrics collection, and integration with workflow error handling.
 import asyncio
 
 import pytest
-
 from tta_ai.orchestration.circuit_breaker import (
     CircuitBreaker,
     CircuitBreakerConfig,
diff --git a/tests/agent_orchestration/test_crisis_detection_scenarios.py b/tests/agent_orchestration/test_crisis_detection_scenarios.py
index 66ac779d7..446643ffa 100644
--- a/tests/agent_orchestration/test_crisis_detection_scenarios.py
+++ b/tests/agent_orchestration/test_crisis_detection_scenarios.py
@@ -6,7 +6,6 @@ appropriate sensitivity and specificity for crisis intervention.
 """
 
 import pytest
-
 from tta_ai.orchestration.therapeutic_safety import (
     CrisisType,
     TherapeuticValidator,
@@ -164,14 +163,14 @@ class TestRealisticCrisisScenarios:
         for description, text, expected_crisis_types in complex_scenarios:
             result = self.validator.validate_text(text)
 
-            assert (
-                result.crisis_detected
-            ), f"Crisis not detected for {description}: {text}"
+            assert result.crisis_detected, (
+                f"Crisis not detected for {description}: {text}"
+            )
 
             for expected_type in expected_crisis_types:
-                assert (
-                    expected_type in result.crisis_types
-                ), f"Expected {expected_type.value} not found in {result.crisis_types} for: {text}"
+                assert expected_type in result.crisis_types, (
+                    f"Expected {expected_type.value} not found in {result.crisis_types} for: {text}"
+                )
 
     def test_false_positive_prevention(self):
         """Test prevention of false positives with challenging scenarios."""
@@ -211,9 +210,9 @@ class TestRealisticCrisisScenarios:
 
         # Allow some false positives for safety, but not too many
         false_positive_rate = false_positive_count / total_scenarios
-        assert (
-            false_positive_rate < 0.3
-        ), f"Too many false positives: {false_positive_rate:.2%} ({false_positive_count}/{total_scenarios})"
+        assert false_positive_rate < 0.3, (
+            f"Too many false positives: {false_positive_rate:.2%} ({false_positive_count}/{total_scenarios})"
+        )
 
     def test_sensitivity_and_specificity(self):
         """Test overall sensitivity and specificity of crisis detection."""
@@ -287,21 +286,21 @@ class TestRealisticCrisisScenarios:
             result = self.validator.validate_text(text)
 
             assert result.crisis_detected, f"Crisis not detected for: {text}"
-            assert (
-                expected_crisis in result.crisis_types
-            ), f"Wrong crisis type for: {text}"
-            assert (
-                result.alternative_content is not None
-            ), f"No alternative provided for: {text}"
+            assert expected_crisis in result.crisis_types, (
+                f"Wrong crisis type for: {text}"
+            )
+            assert result.alternative_content is not None, (
+                f"No alternative provided for: {text}"
+            )
 
             # Check that alternative contains appropriate keywords
             alternative_lower = result.alternative_content.lower()
             keyword_found = any(
                 keyword in alternative_lower for keyword in expected_keywords
             )
-            assert (
-                keyword_found
-            ), f"Alternative doesn't contain expected keywords {expected_keywords} for: {text}"
+            assert keyword_found, (
+                f"Alternative doesn't contain expected keywords {expected_keywords} for: {text}"
+            )
 
     def _test_crisis_scenarios(
         self, scenarios: list[tuple[str, str, bool]], crisis_type: CrisisType
@@ -311,12 +310,12 @@ class TestRealisticCrisisScenarios:
             result = self.validator.validate_text(text)
 
             if should_detect:
-                assert (
-                    result.crisis_detected
-                ), f"Crisis not detected for {description}: {text}"
-                assert (
-                    crisis_type in result.crisis_types
-                ), f"Expected {crisis_type.value} not found for {description}: {text}"
+                assert result.crisis_detected, (
+                    f"Crisis not detected for {description}: {text}"
+                )
+                assert crisis_type in result.crisis_types, (
+                    f"Expected {crisis_type.value} not found for {description}: {text}"
+                )
             # Note: We don't assert false for should_detect=False scenarios
             # because some medium-risk scenarios might legitimately trigger detection
 
@@ -389,9 +388,9 @@ class TestCrisisInterventionMechanisms:
                 has_supportive = any(
                     indicator in alternative for indicator in supportive_indicators
                 )
-                assert (
-                    has_supportive
-                ), f"Alternative lacks supportive language: {alternative}"
+                assert has_supportive, (
+                    f"Alternative lacks supportive language: {alternative}"
+                )
 
                 # Should not be dismissive or harsh
                 # Note: Some of these might appear in therapeutic context, so we check overall tone
diff --git a/tests/agent_orchestration/test_crisis_intervention_system.py b/tests/agent_orchestration/test_crisis_intervention_system.py
index f769fe872..c9ef1b2ef 100644
--- a/tests/agent_orchestration/test_crisis_intervention_system.py
+++ b/tests/agent_orchestration/test_crisis_intervention_system.py
@@ -12,7 +12,6 @@ Tests all crisis intervention components including:
 from unittest.mock import Mock
 
 import pytest
-
 from tta_ai.orchestration.service import AgentOrchestrationService
 from tta_ai.orchestration.therapeutic_safety import (
     CrisisInterventionManager,
diff --git a/tests/agent_orchestration/test_end_to_end_validation.py b/tests/agent_orchestration/test_end_to_end_validation.py
index 33d478134..5b2573c68 100644
--- a/tests/agent_orchestration/test_end_to_end_validation.py
+++ b/tests/agent_orchestration/test_end_to_end_validation.py
@@ -12,7 +12,6 @@ from unittest.mock import AsyncMock, Mock
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.models import AgentType
 from tta_ai.orchestration.performance.optimization import IntelligentAgentCoordinator
 from tta_ai.orchestration.performance.response_time_monitor import (
diff --git a/tests/agent_orchestration/test_end_to_end_workflows.py b/tests/agent_orchestration/test_end_to_end_workflows.py
index 708fe7ece..f531622ae 100644
--- a/tests/agent_orchestration/test_end_to_end_workflows.py
+++ b/tests/agent_orchestration/test_end_to_end_workflows.py
@@ -11,7 +11,6 @@ from unittest.mock import patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     AgentStep,
     AgentType,
@@ -20,6 +19,7 @@ from tta_ai.orchestration import (
     WorkflowManager,
     WorkflowType,
 )
+
 from tests.agent_orchestration.test_multi_agent_workflow_integration import (
     IntegrationTestHelper,
     WorkflowStateVerifier,
diff --git a/tests/agent_orchestration/test_enhanced_therapeutic_safety.py b/tests/agent_orchestration/test_enhanced_therapeutic_safety.py
index 962788591..fd050f7b8 100644
--- a/tests/agent_orchestration/test_enhanced_therapeutic_safety.py
+++ b/tests/agent_orchestration/test_enhanced_therapeutic_safety.py
@@ -5,7 +5,6 @@ Tests all validation algorithms, safety rules, crisis detection, and edge cases.
 """
 
 import pytest
-
 from tta_ai.orchestration.therapeutic_safety import (
     CrisisType,
     SafetyLevel,
@@ -114,9 +113,9 @@ class TestValidationAlgorithms:
             result = self.validator.validate_text(text)
 
             assert result.crisis_detected, f"Crisis not detected for: {text}"
-            assert (
-                expected_crisis in result.crisis_types
-            ), f"Wrong crisis type for: {text}"
+            assert expected_crisis in result.crisis_types, (
+                f"Wrong crisis type for: {text}"
+            )
             assert (
                 result.level == SafetyLevel.BLOCKED
                 or result.level == SafetyLevel.WARNING
@@ -206,9 +205,9 @@ class TestCrisisDetection:
             result = self.validator.validate_text(phrase)
 
             # Should not trigger crisis detection
-            assert (
-                not result.crisis_detected or len(result.crisis_types) == 0
-            ), f"False positive for: {phrase}"
+            assert not result.crisis_detected or len(result.crisis_types) == 0, (
+                f"False positive for: {phrase}"
+            )
 
 
 class TestAlternativeGeneration:
@@ -254,9 +253,9 @@ class TestAlternativeGeneration:
 
                 # Should contain supportive language
                 supportive_words = ["support", "help", "care", "here", "understand"]
-                assert any(
-                    word in alt for word in supportive_words
-                ), f"No supportive language in: {alt}"
+                assert any(word in alt for word in supportive_words), (
+                    f"No supportive language in: {alt}"
+                )
 
                 # Should not contain harsh language
                 # Note: "can't" might appear in therapeutic context, so we check for overall tone
diff --git a/tests/agent_orchestration/test_error_handling_recovery.py b/tests/agent_orchestration/test_error_handling_recovery.py
index 900f72e6f..1a441f4a9 100644
--- a/tests/agent_orchestration/test_error_handling_recovery.py
+++ b/tests/agent_orchestration/test_error_handling_recovery.py
@@ -11,7 +11,6 @@ from unittest.mock import patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     AgentStep,
     AgentType,
@@ -23,6 +22,7 @@ from tta_ai.orchestration import (
     WorkflowType,
     WorldBuilderAgentProxy,
 )
+
 from tests.agent_orchestration.test_multi_agent_workflow_integration import (
     IntegrationTestHelper,
 )
diff --git a/tests/agent_orchestration/test_integration_runner.py b/tests/agent_orchestration/test_integration_runner.py
index 939bf1f12..25aaa78be 100644
--- a/tests/agent_orchestration/test_integration_runner.py
+++ b/tests/agent_orchestration/test_integration_runner.py
@@ -111,9 +111,9 @@ def test_test_data_fixtures():
     assert isinstance(inputs, list), "Sample inputs should be a list"
     assert len(inputs) > 0, "Sample inputs should not be empty"
     assert "text" in inputs[0], "Sample input should have 'text' field"
-    assert (
-        "expected_intent" in inputs[0]
-    ), "Sample input should have 'expected_intent' field"
+    assert "expected_intent" in inputs[0], (
+        "Sample input should have 'expected_intent' field"
+    )
 
     print("âœ“ Test data fixtures are properly structured")
 
@@ -154,9 +154,9 @@ def test_performance_metrics_utility():
     workflow_stats = stats["workflow_stats"]
     assert workflow_stats["count"] == 3, "Incorrect workflow count"
     assert workflow_stats["avg_time"] > 0, "Invalid average time"
-    assert (
-        workflow_stats["max_time"] >= workflow_stats["min_time"]
-    ), "Invalid min/max times"
+    assert workflow_stats["max_time"] >= workflow_stats["min_time"], (
+        "Invalid min/max times"
+    )
 
     # Verify agent statistics
     agent_stats = stats["agent_stats"]
@@ -178,6 +178,7 @@ def test_workflow_state_verifier():
     from tta_ai.orchestration import (
         AgentType,
     )
+
     from tests.agent_orchestration.test_multi_agent_workflow_integration import (
         WorkflowStateVerifier,
     )
@@ -213,9 +214,9 @@ def test_workflow_state_verifier():
     persistence_checks = verifier.verify_state_persistence(initial_state, final_state)
     assert persistence_checks["session_id_preserved"], "Session ID not preserved"
     assert persistence_checks["player_id_preserved"], "Player ID not preserved"
-    assert persistence_checks[
-        "therapeutic_context_maintained"
-    ], "Therapeutic context not maintained"
+    assert persistence_checks["therapeutic_context_maintained"], (
+        "Therapeutic context not maintained"
+    )
     assert persistence_checks["game_state_updated"], "Game state not updated"
 
     # Test response aggregation verification
@@ -228,9 +229,9 @@ def test_workflow_state_verifier():
     aggregation_checks = verifier.verify_response_aggregation(responses)
     assert aggregation_checks["all_agents_responded"], "Not all agents responded"
     assert aggregation_checks["responses_have_content"], "Some responses lack content"
-    assert aggregation_checks[
-        "therapeutic_validation_present"
-    ], "Therapeutic validation missing"
+    assert aggregation_checks["therapeutic_validation_present"], (
+        "Therapeutic validation missing"
+    )
 
     print("âœ“ Workflow state verifier working correctly")
 
diff --git a/tests/agent_orchestration/test_langgraph_orchestrator.py b/tests/agent_orchestration/test_langgraph_orchestrator.py
index 5a28bd23a..e83859a27 100644
--- a/tests/agent_orchestration/test_langgraph_orchestrator.py
+++ b/tests/agent_orchestration/test_langgraph_orchestrator.py
@@ -8,7 +8,6 @@ from unittest.mock import AsyncMock, MagicMock, patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.langgraph_orchestrator import (
     AgentWorkflowState,
     LangGraphAgentOrchestrator,
diff --git a/tests/agent_orchestration/test_metrics_and_startup_recovery.py b/tests/agent_orchestration/test_metrics_and_startup_recovery.py
index cde34f7af..3078ee1ac 100644
--- a/tests/agent_orchestration/test_metrics_and_startup_recovery.py
+++ b/tests/agent_orchestration/test_metrics_and_startup_recovery.py
@@ -2,7 +2,6 @@ import asyncio
 import uuid
 
 import pytest
-
 from tta_ai.orchestration import (
     AgentId,
     AgentMessage,
diff --git a/tests/agent_orchestration/test_models.py b/tests/agent_orchestration/test_models.py
index 8cd9bf714..9bb867325 100644
--- a/tests/agent_orchestration/test_models.py
+++ b/tests/agent_orchestration/test_models.py
@@ -1,5 +1,4 @@
 import pytest
-
 from tta_ai.orchestration import (
     AgentId,
     AgentMessage,
diff --git a/tests/agent_orchestration/test_multi_agent_workflow_integration.py b/tests/agent_orchestration/test_multi_agent_workflow_integration.py
index 3fe6d0dbc..74ee48d4e 100644
--- a/tests/agent_orchestration/test_multi_agent_workflow_integration.py
+++ b/tests/agent_orchestration/test_multi_agent_workflow_integration.py
@@ -19,7 +19,6 @@ from unittest.mock import AsyncMock, Mock
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     AgentMessage,
     AgentRegistry,
diff --git a/tests/agent_orchestration/test_optimization_integration.py b/tests/agent_orchestration/test_optimization_integration.py
index 2d60e06dd..40a770c6b 100644
--- a/tests/agent_orchestration/test_optimization_integration.py
+++ b/tests/agent_orchestration/test_optimization_integration.py
@@ -9,7 +9,6 @@ import asyncio
 from unittest.mock import AsyncMock, Mock
 
 import pytest
-
 from tta_ai.orchestration.optimization import (
     OptimizationEngine,
     OptimizationStrategy,
diff --git a/tests/agent_orchestration/test_performance_concurrency.py b/tests/agent_orchestration/test_performance_concurrency.py
index a3610c8b2..1798ee3bd 100644
--- a/tests/agent_orchestration/test_performance_concurrency.py
+++ b/tests/agent_orchestration/test_performance_concurrency.py
@@ -12,7 +12,6 @@ from unittest.mock import patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     AgentStep,
     AgentType,
@@ -25,6 +24,7 @@ from tta_ai.orchestration import (
     WorkflowType,
     WorldBuilderAgentProxy,
 )
+
 from tests.agent_orchestration.test_multi_agent_workflow_integration import (
     IntegrationTestHelper,
     PerformanceMetrics,
diff --git a/tests/agent_orchestration/test_performance_validation.py b/tests/agent_orchestration/test_performance_validation.py
index 291104c46..5a476aad2 100644
--- a/tests/agent_orchestration/test_performance_validation.py
+++ b/tests/agent_orchestration/test_performance_validation.py
@@ -11,7 +11,6 @@ import time
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.models import AgentType
 from tta_ai.orchestration.performance.alerting import PerformanceAlerting
 from tta_ai.orchestration.performance.analytics import PerformanceAnalytics
diff --git a/tests/agent_orchestration/test_policy_live_reload_and_diagnostics.py b/tests/agent_orchestration/test_policy_live_reload_and_diagnostics.py
index b916009aa..4b0879669 100644
--- a/tests/agent_orchestration/test_policy_live_reload_and_diagnostics.py
+++ b/tests/agent_orchestration/test_policy_live_reload_and_diagnostics.py
@@ -5,8 +5,8 @@ import time
 
 import pytest
 from starlette.testclient import TestClient
-
 from tta_ai.orchestration.tools.policy_config import redact_policy_config_dict
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/test_real_agent_communication.py b/tests/agent_orchestration/test_real_agent_communication.py
index 50ce323fd..86faf2812 100644
--- a/tests/agent_orchestration/test_real_agent_communication.py
+++ b/tests/agent_orchestration/test_real_agent_communication.py
@@ -11,7 +11,6 @@ import time
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     InputProcessorAgentProxy,
     NarrativeGeneratorAgentProxy,
@@ -420,9 +419,9 @@ class TestCompleteWorkflowChains:
 
             # Should have supportive language in therapeutic scenarios
             if nga_result.get("source") == "real_nga":
-                assert (
-                    has_supportive_language
-                ), f"Missing supportive language in {scenario['name']}"
+                assert has_supportive_language, (
+                    f"Missing supportive language in {scenario['name']}"
+                )
 
         return results
 
diff --git a/tests/agent_orchestration/test_real_agent_error_scenarios.py b/tests/agent_orchestration/test_real_agent_error_scenarios.py
index b5ece3a0c..c28d303fa 100644
--- a/tests/agent_orchestration/test_real_agent_error_scenarios.py
+++ b/tests/agent_orchestration/test_real_agent_error_scenarios.py
@@ -11,7 +11,6 @@ from unittest.mock import patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     InputProcessorAgentProxy,
     NarrativeGeneratorAgentProxy,
@@ -358,15 +357,15 @@ class TestRealAgentErrorScenarios:
         fallback_rate = fallback_count / total_requests
 
         # Should maintain reasonable success rate even under pressure
-        assert (
-            success_rate > 0.5
-        ), f"Success rate too low under resource pressure: {success_rate}"
+        assert success_rate > 0.5, (
+            f"Success rate too low under resource pressure: {success_rate}"
+        )
 
         # Should complete within reasonable time
         total_time = end_time - start_time
-        assert (
-            total_time < 60.0
-        ), f"Resource exhaustion test took too long: {total_time}s"
+        assert total_time < 60.0, (
+            f"Resource exhaustion test took too long: {total_time}s"
+        )
 
         return {
             "total_requests": total_requests,
diff --git a/tests/agent_orchestration/test_real_agent_performance.py b/tests/agent_orchestration/test_real_agent_performance.py
index 47611d4a9..43f476cf9 100644
--- a/tests/agent_orchestration/test_real_agent_performance.py
+++ b/tests/agent_orchestration/test_real_agent_performance.py
@@ -13,7 +13,6 @@ from typing import Any
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     InputProcessorAgentProxy,
     NarrativeGeneratorAgentProxy,
@@ -125,12 +124,16 @@ class TestRealAgentPerformance:
             p95_latency=(
                 statistics.quantiles(latencies, n=20)[18]
                 if len(latencies) >= 20
-                else max(latencies) if latencies else 0
+                else max(latencies)
+                if latencies
+                else 0
             ),
             p99_latency=(
                 statistics.quantiles(latencies, n=100)[98]
                 if len(latencies) >= 100
-                else max(latencies) if latencies else 0
+                else max(latencies)
+                if latencies
+                else 0
             ),
             throughput_rps=num_requests / total_time if total_time > 0 else 0,
             error_rate=failed / num_requests if num_requests > 0 else 0,
@@ -147,12 +150,12 @@ class TestRealAgentPerformance:
 
         # Performance assertions
         assert metrics.error_rate < 0.1, f"Error rate too high: {metrics.error_rate}"
-        assert (
-            metrics.average_latency < 5000
-        ), f"Average latency too high: {metrics.average_latency}ms"
-        assert (
-            metrics.throughput_rps > 1
-        ), f"Throughput too low: {metrics.throughput_rps} RPS"
+        assert metrics.average_latency < 5000, (
+            f"Average latency too high: {metrics.average_latency}ms"
+        )
+        assert metrics.throughput_rps > 1, (
+            f"Throughput too low: {metrics.throughput_rps} RPS"
+        )
 
         return metrics
 
@@ -167,12 +170,12 @@ class TestRealAgentPerformance:
 
         # Performance assertions
         assert metrics.error_rate < 0.1, f"Error rate too high: {metrics.error_rate}"
-        assert (
-            metrics.average_latency < 8000
-        ), f"Average latency too high: {metrics.average_latency}ms"
-        assert (
-            metrics.throughput_rps > 0.5
-        ), f"Throughput too low: {metrics.throughput_rps} RPS"
+        assert metrics.average_latency < 8000, (
+            f"Average latency too high: {metrics.average_latency}ms"
+        )
+        assert metrics.throughput_rps > 0.5, (
+            f"Throughput too low: {metrics.throughput_rps} RPS"
+        )
 
         return metrics
 
@@ -193,12 +196,12 @@ class TestRealAgentPerformance:
 
         # Performance assertions (NGA typically slower due to content generation)
         assert metrics.error_rate < 0.15, f"Error rate too high: {metrics.error_rate}"
-        assert (
-            metrics.average_latency < 15000
-        ), f"Average latency too high: {metrics.average_latency}ms"
-        assert (
-            metrics.throughput_rps > 0.2
-        ), f"Throughput too low: {metrics.throughput_rps} RPS"
+        assert metrics.average_latency < 15000, (
+            f"Average latency too high: {metrics.average_latency}ms"
+        )
+        assert metrics.throughput_rps > 0.2, (
+            f"Throughput too low: {metrics.throughput_rps} RPS"
+        )
 
         return metrics
 
@@ -295,12 +298,12 @@ class TestRealAgentPerformance:
 
         # Verify throughput scaling
         for concurrency, result in throughput_results.items():
-            assert (
-                result["workflows_per_second"] > 0
-            ), f"No throughput at concurrency {concurrency}"
-            assert (
-                result["successful_workflows"] == concurrency
-            ), f"Not all workflows succeeded at concurrency {concurrency}"
+            assert result["workflows_per_second"] > 0, (
+                f"No throughput at concurrency {concurrency}"
+            )
+            assert result["successful_workflows"] == concurrency, (
+                f"Not all workflows succeeded at concurrency {concurrency}"
+            )
 
         return throughput_results
 
@@ -350,18 +353,18 @@ class TestRealAgentPerformance:
         success_rate = successful_count / request_count if request_count > 0 else 0
 
         # Performance assertions for sustained load
-        assert (
-            success_rate > 0.8
-        ), f"Success rate too low under sustained load: {success_rate}"
-        assert (
-            actual_rps >= target_rps * 0.8
-        ), f"Actual RPS too low: {actual_rps} (target: {target_rps})"
+        assert success_rate > 0.8, (
+            f"Success rate too low under sustained load: {success_rate}"
+        )
+        assert actual_rps >= target_rps * 0.8, (
+            f"Actual RPS too low: {actual_rps} (target: {target_rps})"
+        )
 
         if latencies:
             avg_latency = statistics.mean(latencies)
-            assert (
-                avg_latency < 10000
-            ), f"Average latency too high under sustained load: {avg_latency}ms"
+            assert avg_latency < 10000, (
+                f"Average latency too high under sustained load: {avg_latency}ms"
+            )
 
         return {
             "duration": actual_duration,
@@ -374,7 +377,9 @@ class TestRealAgentPerformance:
             "p95_latency": (
                 statistics.quantiles(latencies, n=20)[18]
                 if len(latencies) >= 20
-                else max(latencies) if latencies else 0
+                else max(latencies)
+                if latencies
+                else 0
             ),
         }
 
@@ -412,9 +417,9 @@ class TestRealAgentPerformance:
                 memory_increase = current_memory - baseline_memory
 
                 # Memory should not grow excessively
-                assert (
-                    memory_increase < 500
-                ), f"Memory usage increased too much: {memory_increase}MB"
+                assert memory_increase < 500, (
+                    f"Memory usage increased too much: {memory_increase}MB"
+                )
 
         # Final memory check
         final_memory = process.memory_info().rss / 1024 / 1024  # MB
diff --git a/tests/agent_orchestration/test_realtime_integration.py b/tests/agent_orchestration/test_realtime_integration.py
index c695a87cc..7e0a2abc0 100644
--- a/tests/agent_orchestration/test_realtime_integration.py
+++ b/tests/agent_orchestration/test_realtime_integration.py
@@ -8,7 +8,6 @@ progressive feedback, and workflow progress tracking.
 from unittest.mock import AsyncMock, Mock
 
 import pytest
-
 from tta_ai.orchestration.realtime.event_publisher import EventPublisher
 from tta_ai.orchestration.realtime.models import (
     AgentStatus,
diff --git a/tests/agent_orchestration/test_redis_agent_registry.py b/tests/agent_orchestration/test_redis_agent_registry.py
index 264846368..dc39075da 100644
--- a/tests/agent_orchestration/test_redis_agent_registry.py
+++ b/tests/agent_orchestration/test_redis_agent_registry.py
@@ -2,7 +2,6 @@ import asyncio
 import json
 
 import pytest
-
 from tta_ai.orchestration.proxies import InputProcessorAgentProxy
 from tta_ai.orchestration.registries import RedisAgentRegistry
 
diff --git a/tests/agent_orchestration/test_redis_agent_registry_config_and_deregister.py b/tests/agent_orchestration/test_redis_agent_registry_config_and_deregister.py
index 89b5d964d..11fdfb10d 100644
--- a/tests/agent_orchestration/test_redis_agent_registry_config_and_deregister.py
+++ b/tests/agent_orchestration/test_redis_agent_registry_config_and_deregister.py
@@ -2,7 +2,6 @@ import asyncio
 import json
 
 import pytest
-
 from tta_ai.orchestration.proxies import InputProcessorAgentProxy
 from tta_ai.orchestration.registries import RedisAgentRegistry
 
diff --git a/tests/agent_orchestration/test_redis_event_validation.py b/tests/agent_orchestration/test_redis_event_validation.py
index 1242497a6..8a11d4d7c 100644
--- a/tests/agent_orchestration/test_redis_event_validation.py
+++ b/tests/agent_orchestration/test_redis_event_validation.py
@@ -10,7 +10,6 @@ from unittest.mock import AsyncMock, Mock
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.realtime.event_publisher import EventPublisher
 from tta_ai.orchestration.realtime.event_subscriber import EventSubscriber
 from tta_ai.orchestration.realtime.models import (
diff --git a/tests/agent_orchestration/test_redis_message_coordinator.py b/tests/agent_orchestration/test_redis_message_coordinator.py
index 32d93b881..e31107b95 100644
--- a/tests/agent_orchestration/test_redis_message_coordinator.py
+++ b/tests/agent_orchestration/test_redis_message_coordinator.py
@@ -3,7 +3,6 @@ import json
 import uuid
 
 import pytest
-
 from tta_ai.orchestration import (
     AgentId,
     AgentMessage,
diff --git a/tests/agent_orchestration/test_redis_message_reliability.py b/tests/agent_orchestration/test_redis_message_reliability.py
index bba7be187..fec541fc4 100644
--- a/tests/agent_orchestration/test_redis_message_reliability.py
+++ b/tests/agent_orchestration/test_redis_message_reliability.py
@@ -2,7 +2,6 @@ import asyncio
 import uuid
 
 import pytest
-
 from tta_ai.orchestration import (
     AgentId,
     AgentMessage,
diff --git a/tests/agent_orchestration/test_redis_tool_registry.py b/tests/agent_orchestration/test_redis_tool_registry.py
index 5983b82b9..67a522ac8 100644
--- a/tests/agent_orchestration/test_redis_tool_registry.py
+++ b/tests/agent_orchestration/test_redis_tool_registry.py
@@ -1,7 +1,6 @@
 import asyncio
 
 import pytest
-
 from tta_ai.orchestration.tools.models import ToolParameter, ToolSpec
 from tta_ai.orchestration.tools.redis_tool_registry import RedisToolRegistry
 
diff --git a/tests/agent_orchestration/test_resource_manager.py b/tests/agent_orchestration/test_resource_manager.py
index f4222dc45..d9453efbd 100644
--- a/tests/agent_orchestration/test_resource_manager.py
+++ b/tests/agent_orchestration/test_resource_manager.py
@@ -1,5 +1,4 @@
 import pytest
-
 from tta_ai.orchestration.resources import ResourceManager, ResourceRequirements
 
 
diff --git a/tests/agent_orchestration/test_session_state_validation.py b/tests/agent_orchestration/test_session_state_validation.py
index 3ada77195..cb988c442 100644
--- a/tests/agent_orchestration/test_session_state_validation.py
+++ b/tests/agent_orchestration/test_session_state_validation.py
@@ -10,7 +10,6 @@ import json
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.proxies import (
     InputProcessorAgentProxy,
     NarrativeGeneratorAgentProxy,
diff --git a/tests/agent_orchestration/test_state_persistence_aggregation.py b/tests/agent_orchestration/test_state_persistence_aggregation.py
index 7a289d2c9..27306728b 100644
--- a/tests/agent_orchestration/test_state_persistence_aggregation.py
+++ b/tests/agent_orchestration/test_state_persistence_aggregation.py
@@ -12,7 +12,6 @@ from unittest.mock import patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration import (
     AgentStep,
     AgentType,
@@ -25,6 +24,7 @@ from tta_ai.orchestration import (
     WorkflowType,
     WorldBuilderAgentProxy,
 )
+
 from tests.agent_orchestration.test_multi_agent_workflow_integration import (
     IntegrationTestHelper,
     WorkflowStateVerifier,
diff --git a/tests/agent_orchestration/test_state_validator_repairs.py b/tests/agent_orchestration/test_state_validator_repairs.py
index 707135bd4..8103c5988 100644
--- a/tests/agent_orchestration/test_state_validator_repairs.py
+++ b/tests/agent_orchestration/test_state_validator_repairs.py
@@ -1,9 +1,9 @@
 import os
 
 import pytest
-
 from tta_ai.orchestration.coordinators import RedisMessageCoordinator
 from tta_ai.orchestration.models import AgentId, AgentMessage, AgentType, MessageType
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/test_therapeutic_content_validation.py b/tests/agent_orchestration/test_therapeutic_content_validation.py
index a16415426..a0ab8ee45 100644
--- a/tests/agent_orchestration/test_therapeutic_content_validation.py
+++ b/tests/agent_orchestration/test_therapeutic_content_validation.py
@@ -9,7 +9,6 @@ import json
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.proxies import (
     InputProcessorAgentProxy,
     NarrativeGeneratorAgentProxy,
diff --git a/tests/agent_orchestration/test_therapeutic_safety.py b/tests/agent_orchestration/test_therapeutic_safety.py
index 29c4143e7..db564cf68 100644
--- a/tests/agent_orchestration/test_therapeutic_safety.py
+++ b/tests/agent_orchestration/test_therapeutic_safety.py
@@ -2,7 +2,6 @@ import asyncio
 import json
 
 import pytest
-
 from tta_ai.orchestration.therapeutic_safety import (
     SafetyLevel,
     SafetyRulesProvider,
diff --git a/tests/agent_orchestration/test_therapeutic_safety_integration.py b/tests/agent_orchestration/test_therapeutic_safety_integration.py
index e625bd48a..f681d4e47 100644
--- a/tests/agent_orchestration/test_therapeutic_safety_integration.py
+++ b/tests/agent_orchestration/test_therapeutic_safety_integration.py
@@ -8,7 +8,6 @@ AgentOrchestrationService, ensuring seamless operation and proper error handling
 from unittest.mock import Mock, patch
 
 import pytest
-
 from tta_ai.orchestration.agents import AgentRegistry
 from tta_ai.orchestration.interfaces import MessageCoordinator
 from tta_ai.orchestration.service import (
diff --git a/tests/agent_orchestration/test_therapeutic_validator_performance.py b/tests/agent_orchestration/test_therapeutic_validator_performance.py
index 8a091840a..f6b41fb38 100644
--- a/tests/agent_orchestration/test_therapeutic_validator_performance.py
+++ b/tests/agent_orchestration/test_therapeutic_validator_performance.py
@@ -10,7 +10,6 @@ import time
 from concurrent.futures import ThreadPoolExecutor, as_completed
 
 import pytest
-
 from tta_ai.orchestration.therapeutic_safety import (
     CrisisType,
     TherapeuticValidator,
diff --git a/tests/agent_orchestration/test_tool_coordinator.py b/tests/agent_orchestration/test_tool_coordinator.py
index 6d161d5b3..93bffc99d 100644
--- a/tests/agent_orchestration/test_tool_coordinator.py
+++ b/tests/agent_orchestration/test_tool_coordinator.py
@@ -1,5 +1,4 @@
 import pytest
-
 from tta_ai.orchestration.tools.coordinator import ToolCoordinator
 from tta_ai.orchestration.tools.models import ToolParameter, ToolPolicy, ToolSpec
 from tta_ai.orchestration.tools.redis_tool_registry import RedisToolRegistry
diff --git a/tests/agent_orchestration/test_tool_invocation_service.py b/tests/agent_orchestration/test_tool_invocation_service.py
index 0733c0ca2..91453cac2 100644
--- a/tests/agent_orchestration/test_tool_invocation_service.py
+++ b/tests/agent_orchestration/test_tool_invocation_service.py
@@ -1,7 +1,6 @@
 import asyncio
 
 import pytest
-
 from tta_ai.orchestration.tools.coordinator import ToolCoordinator
 from tta_ai.orchestration.tools.invocation_service import ToolInvocationService
 from tta_ai.orchestration.tools.metrics import get_tool_metrics
diff --git a/tests/agent_orchestration/test_tool_models.py b/tests/agent_orchestration/test_tool_models.py
index 075051e67..e3c5da0d8 100644
--- a/tests/agent_orchestration/test_tool_models.py
+++ b/tests/agent_orchestration/test_tool_models.py
@@ -1,5 +1,4 @@
 import pytest
-
 from tta_ai.orchestration.tools.models import ToolParameter, ToolPolicy, ToolSpec
 
 
diff --git a/tests/agent_orchestration/test_tools_diagnostics_and_concurrency.py b/tests/agent_orchestration/test_tools_diagnostics_and_concurrency.py
index fa25322ab..cd39fd3a3 100644
--- a/tests/agent_orchestration/test_tools_diagnostics_and_concurrency.py
+++ b/tests/agent_orchestration/test_tools_diagnostics_and_concurrency.py
@@ -2,10 +2,10 @@ import asyncio
 import os
 
 import pytest
-
 from tta_ai.orchestration.tools.coordinator import ToolCoordinator
 from tta_ai.orchestration.tools.models import ToolParameter, ToolPolicy, ToolSpec
 from tta_ai.orchestration.tools.redis_tool_registry import RedisToolRegistry
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/test_tools_execute_auth.py b/tests/agent_orchestration/test_tools_execute_auth.py
index b022d1ca7..c9bf40079 100644
--- a/tests/agent_orchestration/test_tools_execute_auth.py
+++ b/tests/agent_orchestration/test_tools_execute_auth.py
@@ -3,8 +3,8 @@ import os
 
 import pytest
 from starlette.testclient import TestClient
-
 from tta_ai.orchestration.tools.models import ToolSpec
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/test_tools_summary_and_prometheus.py b/tests/agent_orchestration/test_tools_summary_and_prometheus.py
index ec132db53..319c2ebee 100644
--- a/tests/agent_orchestration/test_tools_summary_and_prometheus.py
+++ b/tests/agent_orchestration/test_tools_summary_and_prometheus.py
@@ -1,8 +1,8 @@
 import os
 
 import pytest
-
 from tta_ai.orchestration.tools.metrics import get_tool_metrics
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/test_unified_orchestrator.py b/tests/agent_orchestration/test_unified_orchestrator.py
index e7e16efa7..0602fd127 100644
--- a/tests/agent_orchestration/test_unified_orchestrator.py
+++ b/tests/agent_orchestration/test_unified_orchestrator.py
@@ -10,7 +10,6 @@ from unittest.mock import AsyncMock, patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.unified_orchestrator import (
     OrchestrationPhase,
     OrchestrationState,
diff --git a/tests/agent_orchestration/test_websocket_error_recovery.py b/tests/agent_orchestration/test_websocket_error_recovery.py
index 6c4ad49bc..a8256f3db 100644
--- a/tests/agent_orchestration/test_websocket_error_recovery.py
+++ b/tests/agent_orchestration/test_websocket_error_recovery.py
@@ -11,7 +11,6 @@ from unittest.mock import AsyncMock, Mock
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.realtime.error_reporting import (
     ErrorReportingManager,
     ErrorSeverity,
diff --git a/tests/agent_orchestration/test_websocket_integration.py b/tests/agent_orchestration/test_websocket_integration.py
index 6748cbdf3..c05983363 100644
--- a/tests/agent_orchestration/test_websocket_integration.py
+++ b/tests/agent_orchestration/test_websocket_integration.py
@@ -9,7 +9,6 @@ import json
 from unittest.mock import AsyncMock, Mock
 
 import pytest
-
 from tta_ai.orchestration.realtime.models import (
     AgentStatus,
     AgentStatusEvent,
diff --git a/tests/agent_orchestration/test_websocket_performance.py b/tests/agent_orchestration/test_websocket_performance.py
index 5486f6f4e..561ed8df9 100644
--- a/tests/agent_orchestration/test_websocket_performance.py
+++ b/tests/agent_orchestration/test_websocket_performance.py
@@ -14,7 +14,6 @@ from unittest.mock import AsyncMock, Mock
 import psutil
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.realtime.event_publisher import EventPublisher
 from tta_ai.orchestration.realtime.models import AgentStatus, AgentStatusEvent
 from tta_ai.orchestration.realtime.websocket_manager import (
diff --git a/tests/agent_orchestration/test_websocket_real_agent_integration.py b/tests/agent_orchestration/test_websocket_real_agent_integration.py
index c8a8f7617..5c03ce609 100644
--- a/tests/agent_orchestration/test_websocket_real_agent_integration.py
+++ b/tests/agent_orchestration/test_websocket_real_agent_integration.py
@@ -12,7 +12,6 @@ from unittest.mock import AsyncMock, Mock
 import pytest
 import pytest_asyncio
 from fastapi.websockets import WebSocket
-
 from tta_ai.orchestration.proxies import (
     InputProcessorAgentProxy,
     NarrativeGeneratorAgentProxy,
diff --git a/tests/agent_orchestration/test_websocket_validation.py b/tests/agent_orchestration/test_websocket_validation.py
index 4ff8039e7..1d4b1cca9 100644
--- a/tests/agent_orchestration/test_websocket_validation.py
+++ b/tests/agent_orchestration/test_websocket_validation.py
@@ -12,7 +12,6 @@ from unittest.mock import AsyncMock, Mock, patch
 import pytest
 import pytest_asyncio
 from fastapi.websockets import WebSocket
-
 from tta_ai.orchestration.realtime.config_manager import (
     RealtimeEnvironment,
     get_realtime_config_manager,
diff --git a/tests/agent_orchestration/test_workflow_chain_validation.py b/tests/agent_orchestration/test_workflow_chain_validation.py
index ee7dec4fd..abd813992 100644
--- a/tests/agent_orchestration/test_workflow_chain_validation.py
+++ b/tests/agent_orchestration/test_workflow_chain_validation.py
@@ -11,7 +11,6 @@ import time
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.orchestration.performance.response_time_monitor import (
     OperationType,
     ResponseTimeMonitor,
diff --git a/tests/agent_orchestration/test_workflow_safety_integration.py b/tests/agent_orchestration/test_workflow_safety_integration.py
index 15b303eb4..549ee360f 100644
--- a/tests/agent_orchestration/test_workflow_safety_integration.py
+++ b/tests/agent_orchestration/test_workflow_safety_integration.py
@@ -1,5 +1,4 @@
 import pytest
-
 from tta_ai.orchestration.models import AgentType, OrchestrationRequest
 from tta_ai.orchestration.workflow import AgentStep, WorkflowDefinition, WorkflowType
 from tta_ai.orchestration.workflow_manager import WorkflowManager
diff --git a/tests/agent_orchestration/test_workflow_transaction_rollback.py b/tests/agent_orchestration/test_workflow_transaction_rollback.py
index 00426feb8..b92ec7ee3 100644
--- a/tests/agent_orchestration/test_workflow_transaction_rollback.py
+++ b/tests/agent_orchestration/test_workflow_transaction_rollback.py
@@ -1,8 +1,8 @@
 import os
 
 import pytest
-
 from tta_ai.orchestration.workflow_transaction import WorkflowTransaction
+
 from src.components.agent_orchestration_component import AgentOrchestrationComponent
 
 
diff --git a/tests/agent_orchestration/tools/test_policy_config_and_enforcement.py b/tests/agent_orchestration/tools/test_policy_config_and_enforcement.py
index 1b3ff50f2..97ceb0389 100644
--- a/tests/agent_orchestration/tools/test_policy_config_and_enforcement.py
+++ b/tests/agent_orchestration/tools/test_policy_config_and_enforcement.py
@@ -1,7 +1,6 @@
 import asyncio
 
 import pytest
-
 from tta_ai.orchestration.tools.coordinator import ToolCoordinator
 from tta_ai.orchestration.tools.invocation_service import ToolInvocationService
 from tta_ai.orchestration.tools.models import ToolPolicy, ToolSpec
diff --git a/tests/agent_orchestration/tools/test_registry_idempotency.py b/tests/agent_orchestration/tools/test_registry_idempotency.py
index 8520ad840..e0b96b1cf 100644
--- a/tests/agent_orchestration/tools/test_registry_idempotency.py
+++ b/tests/agent_orchestration/tools/test_registry_idempotency.py
@@ -1,7 +1,6 @@
 import asyncio
 
 import pytest
-
 from tta_ai.orchestration.tools.coordinator import ToolCoordinator
 from tta_ai.orchestration.tools.models import ToolPolicy, ToolSpec
 from tta_ai.orchestration.tools.policy_config import ToolPolicyConfig
diff --git a/tests/agent_orchestration/tools/test_timeouts_and_metrics.py b/tests/agent_orchestration/tools/test_timeouts_and_metrics.py
index 82a221b92..9207ffd90 100644
--- a/tests/agent_orchestration/tools/test_timeouts_and_metrics.py
+++ b/tests/agent_orchestration/tools/test_timeouts_and_metrics.py
@@ -2,7 +2,6 @@ import asyncio
 import time
 
 import pytest
-
 from tta_ai.orchestration.tools.coordinator import ToolCoordinator
 from tta_ai.orchestration.tools.invocation_service import ToolInvocationService
 from tta_ai.orchestration.tools.metrics import get_tool_metrics
diff --git a/tests/test_impact_analysis_coverage.py b/tests/test_impact_analysis_coverage.py
index 0a9e5121c..d6588242c 100644
--- a/tests/test_impact_analysis_coverage.py
+++ b/tests/test_impact_analysis_coverage.py
@@ -7,7 +7,6 @@ and edge cases.
 """
 
 import pytest
-
 from tta_narrative.orchestration.impact_analysis import (
     assess_therapeutic_alignment,
     calculate_base_magnitude,
diff --git a/tests/test_model_management.py b/tests/test_model_management.py
index 5555db5d7..c336f7b61 100644
--- a/tests/test_model_management.py
+++ b/tests/test_model_management.py
@@ -9,7 +9,6 @@ from unittest.mock import AsyncMock, Mock, patch
 
 import pytest
 import pytest_asyncio
-
 from tta_ai.models import (
     GenerationRequest,
     ModelInfo,
diff --git a/tests/test_models_compatibility.py b/tests/test_models_compatibility.py
index f94fcb9e9..6edd96f70 100644
--- a/tests/test_models_compatibility.py
+++ b/tests/test_models_compatibility.py
@@ -1,12 +1,12 @@
-from tta_narrative.orchestration.models import (
-    NarrativeEvent,
-    NarrativeScale,
-)
 from tta_narrative.coherence import (
     ConsistencyIssue,
     ConsistencyIssueType,
     ValidationSeverity,
 )
+from tta_narrative.orchestration.models import (
+    NarrativeEvent,
+    NarrativeScale,
+)
 
 
 def test_narrative_coherence_models_importable():
diff --git a/tests/test_narrative_arc_orchestrator_component.py b/tests/test_narrative_arc_orchestrator_component.py
index 76da8bbb2..762a644a2 100644
--- a/tests/test_narrative_arc_orchestrator_component.py
+++ b/tests/test_narrative_arc_orchestrator_component.py
@@ -16,6 +16,7 @@ from tta_narrative.orchestration_component import (
     NarrativeStatus,
     PlayerChoice,
 )
+
 from src.orchestration import TTAConfig
 
 
diff --git a/tests/test_narrative_coherence_validators.py b/tests/test_narrative_coherence_validators.py
index c26d8ac89..2f0bc2853 100644
--- a/tests/test_narrative_coherence_validators.py
+++ b/tests/test_narrative_coherence_validators.py
@@ -11,7 +11,6 @@ import uuid
 from datetime import datetime
 
 import pytest
-
 from tta_narrative.coherence.coherence_validator import CoherenceValidator
 from tta_narrative.coherence.contradiction_detector import (
     ContradictionDetector,
diff --git a/tests/test_scale_manager_coverage.py b/tests/test_scale_manager_coverage.py
index 2e876c413..8897f4c6b 100644
--- a/tests/test_scale_manager_coverage.py
+++ b/tests/test_scale_manager_coverage.py
@@ -6,7 +6,6 @@ untested code paths in scale_manager.py.
 """
 
 import pytest
-
 from tta_narrative.orchestration.conflict_detection import (
     ScaleConflict,
 )
diff --git a/tests/test_wave3_facades.py b/tests/test_wave3_facades.py
index 2201692e3..5a045c4ca 100644
--- a/tests/test_wave3_facades.py
+++ b/tests/test_wave3_facades.py
@@ -1,10 +1,3 @@
-from tta_narrative.orchestration.causal_graph import *  # just ensure module loads
-from tta_narrative.orchestration.conflict_detection import ScaleConflict
-from tta_narrative.orchestration.impact_analysis import ImpactAssessment
-from tta_narrative.orchestration.resolution_engine import Resolution
-
-# Verify narrative_arc_orchestrator facades import and names exist
-from tta_narrative.orchestration.scale_manager import ScaleManager
 from tta_narrative.coherence.causal_validator import CausalValidator
 from tta_narrative.coherence.coherence_validator import CoherenceValidator
 
@@ -16,6 +9,13 @@ from tta_narrative.coherence.rules import (
     OVERALL_WEIGHTS,
     SEVERITY_WEIGHTS_LORE,
 )
+from tta_narrative.orchestration.causal_graph import *  # just ensure module loads
+from tta_narrative.orchestration.conflict_detection import ScaleConflict
+from tta_narrative.orchestration.impact_analysis import ImpactAssessment
+from tta_narrative.orchestration.resolution_engine import Resolution
+
+# Verify narrative_arc_orchestrator facades import and names exist
+from tta_narrative.orchestration.scale_manager import ScaleManager
 
 
 def test_facades_importable():
diff --git a/tests/tta_prod/test_dynamic_tools_invocation_service_integration.py b/tests/tta_prod/test_dynamic_tools_invocation_service_integration.py
index b63bb6acb..0146bd61c 100644
--- a/tests/tta_prod/test_dynamic_tools_invocation_service_integration.py
+++ b/tests/tta_prod/test_dynamic_tools_invocation_service_integration.py
@@ -1,6 +1,6 @@
 import pytest
-
 from tta_ai.orchestration.tools.metrics import get_tool_metrics
+
 from tta.prod.src.tools.dynamic_tools import DynamicTool
 
 
diff --git a/tests/tta_prod/test_dynamic_tools_policy_and_metrics_integration.py b/tests/tta_prod/test_dynamic_tools_policy_and_metrics_integration.py
index e021e80a5..ef5bcd74d 100644
--- a/tests/tta_prod/test_dynamic_tools_policy_and_metrics_integration.py
+++ b/tests/tta_prod/test_dynamic_tools_policy_and_metrics_integration.py
@@ -1,6 +1,6 @@
 import pytest
-
 from tta_ai.orchestration.tools.metrics import get_tool_metrics
+
 from tta.prod.src.tools.dynamic_tools import DynamicTool
 
 
